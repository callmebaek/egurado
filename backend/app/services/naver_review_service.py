"""
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë¦¬ë·° ì¡°íšŒ ì„œë¹„ìŠ¤
- ë°©ë¬¸ì ë¦¬ë·° (GraphQL API)
- ë¸”ë¡œê·¸ ë¦¬ë·° (GraphQL API + HTML íŒŒì‹±)
"""
import asyncio
import httpx
import logging
import pytz
import base64
import json
import re
from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta
from playwright.async_api import Page
from app.core.proxy import get_proxy, report_proxy_success, report_proxy_failure

logger = logging.getLogger(__name__)


class NaverReviewService:
    """ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë¦¬ë·° ì¡°íšŒ ì„œë¹„ìŠ¤"""
    
    GRAPHQL_URL = "https://api.place.naver.com/graphql"
    PCMAP_GRAPHQL_URL = "https://pcmap-api.place.naver.com/graphql"
    TIMEOUT = 30.0
    
    def __init__(self):
        # ëª¨ë°”ì¼ APIìš© í—¤ë” (ë°©ë¬¸ì ë¦¬ë·°)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15",
            "Accept": "application/json",
            "Content-Type": "application/json",
            "Origin": "https://m.place.naver.com",
            "Referer": "https://m.place.naver.com/",
        }
        
        # PC APIìš© í—¤ë” (ë¸”ë¡œê·¸ ë¦¬ë·°)
        self.pc_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
            "Accept": "*/*",
            "Content-Type": "application/json",
            "Origin": "https://pcmap.place.naver.com",
            "Referer": "https://pcmap.place.naver.com/",
        }
        
        # í”„ë¡ì‹œëŠ” ìš”ì²­ ì‹œì ì— ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜´ (ìƒíƒœ ê¸°ë°˜ ìë™ í´ë°±)
    
    async def get_visitor_reviews(
        self, 
        place_id: str, 
        size: int = 20,
        sort: str = "recent",  # recent, popular, high_rating, low_rating
        after: str = None  # Cursor for pagination
    ) -> Dict[str, Any]:
        """
        ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ (GraphQL - Cursor ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜)
        
        Args:
            place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID
            size: ê°€ì ¸ì˜¬ ë¦¬ë·° ìˆ˜
            sort: ì •ë ¬ ê¸°ì¤€
            after: í˜ì´ì§€ë„¤ì´ì…˜ ì»¤ì„œ (ì´ì „ ì‘ë‹µì˜ ë§ˆì§€ë§‰ ë¦¬ë·° cursor)
        
        Returns:
            {
                "total": ì „ì²´ ë¦¬ë·° ìˆ˜,
                "items": [ë¦¬ë·° ëª©ë¡] (ê° ë¦¬ë·°ì— cursor í¬í•¨),
                "has_more": ë‹¤ìŒ í˜ì´ì§€ ì—¬ë¶€,
                "last_cursor": ë§ˆì§€ë§‰ ë¦¬ë·°ì˜ cursor
            }
        """
        query = """
        query getVisitorReviews($input: VisitorReviewsInput) {
            visitorReviews(input: $input) {
                items {
                    id
                    cursor
                    reviewId
                    rating
                    author {
                        id
                        nickname
                        imageUrl
                    }
                    body
                    thumbnail
                    media {
                        type
                        thumbnail
                    }
                    tags
                    status
                    visited
                    created
                    reply {
                        editedBy
                        body
                        created
                    }
                    businessName
                }
                total
            }
        }
        """
        
        variables = {
            "input": {
                "businessId": place_id,
                "size": size,
                "sort": sort.upper(),
                "includeContent": True
            }
        }
        
        # Cursor ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜
        if after:
            variables["input"]["after"] = after
        
        print(f"[DEBUG] GraphQL Request - place_id={place_id}, size={size}, after={after[:20] if after else 'None'}...", flush=True)
        
        try:
            logger.info(f"ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ ì‹œì‘: place_id={place_id}, size={size}, after={after[:20] if after else 'None'}...")
            
            # í”„ë¡ì‹œ ì¡°ê±´ë¶€ ì„¤ì •
            client_kwargs = {"timeout": self.TIMEOUT}
            proxy_url = get_proxy()
            if proxy_url:
                client_kwargs["proxy"] = proxy_url
            
            async with httpx.AsyncClient(**client_kwargs) as client:
                payload = {"query": query, "variables": variables}
                logger.debug(f"GraphQL ìš”ì²­: {payload}")
                print(f"[DEBUG] GraphQL variables: {variables}", flush=True)
                
                response = await client.post(
                    self.GRAPHQL_URL,
                    json=payload,
                    headers=self.headers
                )
                
                if response.status_code != 200:
                    logger.error(f"ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ ì‹¤íŒ¨: status={response.status_code}, body={response.text}")
                    return {"total": 0, "items": [], "has_more": False, "last_cursor": None}
                
                data = response.json()
                
                # ì‘ë‹µ êµ¬ì¡° ë””ë²„ê¹…
                print(f"[DEBUG] Response keys: {list(data.keys())}", flush=True)
                if 'data' in data:
                    print(f"[DEBUG] data keys: {list(data.get('data', {}).keys()) if data.get('data') else 'data is None'}", flush=True)
                
                if "errors" in data:
                    logger.error(f"GraphQL ì—ëŸ¬: {data['errors']}")
                    print(f"[DEBUG] GraphQL errors: {data['errors']}", flush=True)
                    return {"total": 0, "items": [], "has_more": False, "last_cursor": None}
                
                # visitor_reviewsê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                visitor_reviews = data.get("data", {})
                if visitor_reviews is None:
                    logger.error(f"data is None in response")
                    print(f"[DEBUG] data is None in response", flush=True)
                    return {"total": 0, "items": [], "has_more": False, "last_cursor": None}
                
                visitor_reviews = visitor_reviews.get("visitorReviews")
                if visitor_reviews is None:
                    logger.error(f"visitorReviews is None in response")
                    print(f"[DEBUG] visitorReviews is None in response, full response: {data}", flush=True)
                    return {"total": 0, "items": [], "has_more": False, "last_cursor": None}
                
                items = visitor_reviews.get("items", [])
                total = visitor_reviews.get("total", 0)
                
                # ë§ˆì§€ë§‰ ë¦¬ë·°ì˜ cursor ì¶”ì¶œ
                last_cursor = None
                if items:
                    last_cursor = items[-1].get('cursor')
                    first_id = items[0].get('id', 'N/A')
                    last_id = items[-1].get('id', 'N/A')
                    first_cursor = items[0].get('cursor', 'N/A')
                    print(f"[DEBUG] GraphQL Response - items={len(items)}, first_id={first_id[:16]}, last_id={last_id[:16]}", flush=True)
                    print(f"[DEBUG] Cursors - first={first_cursor[:20] if first_cursor != 'N/A' else 'N/A'}..., last={last_cursor[:20] if last_cursor else 'None'}...", flush=True)
                
                # has_more: cursorê°€ ìˆê³ , sizeë§Œí¼ ê°€ì ¸ì™”ë‹¤ë©´ ë‹¤ìŒì´ ìˆì„ ê°€ëŠ¥ì„±
                has_more = len(items) == size and last_cursor is not None
                
                logger.info(f"ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ ì„±ê³µ: place_id={place_id}, total={total}, items_count={len(items)}, has_more={has_more}, last_cursor={last_cursor[:20] if last_cursor else 'None'}...")
                
                return {
                    "total": total,
                    "items": items,
                    "has_more": has_more,
                    "last_cursor": last_cursor
                }
                
        except Exception as e:
            logger.error(f"ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ ì˜ˆì™¸: {type(e).__name__} - {str(e)}")
            return {"total": 0, "items": [], "has_more": False, "last_cursor": None}
    
    async def get_blog_reviews(
        self, 
        place_id: str,
        page: int = 1,
        size: int = 20,
        use_pc_api: bool = False
    ) -> Dict[str, Any]:
        """
        ë¸”ë¡œê·¸ ë¦¬ë·° ì¡°íšŒ
        
        Args:
            place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            size: í˜ì´ì§€ë‹¹ ë¦¬ë·° ìˆ˜
            use_pc_api: PC API ì‚¬ìš© ì—¬ë¶€ (True: pcmap-api ì‚¬ìš©, False: ê¸°ì¡´ api ì‚¬ìš©)
        
        Returns:
            {
                "total": ì „ì²´ ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜,
                "items": [ë¸”ë¡œê·¸ ë¦¬ë·° ëª©ë¡],
                "page": í˜„ì¬ í˜ì´ì§€,
                "has_more": ë‹¤ìŒ í˜ì´ì§€ ì—¬ë¶€
            }
        """
        # getFsasReviews GraphQL API ì‚¬ìš©
        query = """
        query getFsasReviews($input: FsasReviewsInput) {
            fsasReviews(input: $input) {
                total
                maxItemCount
                items {
                    id
                    date
                    createdString
                    type
                    typeName
                    title
                    contents
                    url
                    home
                    authorName
                    thumbnailUrl
                    thumbnailCount
                    reviewId
                }
            }
        }
        """
        
        variables = {
            "input": {
                "businessId": place_id,
                "page": page
                # FsasReviewsInputì€ sizeì™€ type í•„ë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
            }
        }
        
        # API URLê³¼ í—¤ë” ì„ íƒ
        if use_pc_api:
            # PC API ì‚¬ìš© (í™œì„±í™” ê¸°ëŠ¥ ì „ìš©)
            api_url = self.PCMAP_GRAPHQL_URL
            
            # x-wtm-graphql í—¤ë” ìƒì„± (Base64 ì¸ì½”ë”©)
            wtm_data = {
                "arg": place_id,
                "type": "restaurant",
                "source": "place"
            }
            wtm_graphql_header = base64.b64encode(json.dumps(wtm_data).encode()).decode()
            
            # PC APIìš© í—¤ë” ë³µì‚¬ ë° x-wtm-graphql ì¶”ê°€
            headers = self.pc_headers.copy()
            headers["x-wtm-graphql"] = wtm_graphql_header
            headers["Referer"] = f"https://pcmap.place.naver.com/restaurant/{place_id}/review/ugc"
            logger.info(f"[ë¸”ë¡œê·¸ ë¦¬ë·°] PC API ì‚¬ìš©: place_id={place_id}, page={page}")
        else:
            # ê¸°ì¡´ ëª¨ë°”ì¼ API ì‚¬ìš© (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
            api_url = self.GRAPHQL_URL
            headers = self.headers
            logger.info(f"[ë¸”ë¡œê·¸ ë¦¬ë·°] ëª¨ë°”ì¼ API ì‚¬ìš©: place_id={place_id}, page={page}")
        
        try:
            # í”„ë¡ì‹œ ì¡°ê±´ë¶€ ì„¤ì •
            client_kwargs = {"timeout": self.TIMEOUT}
            proxy_url = get_proxy()
            if proxy_url:
                client_kwargs["proxy"] = proxy_url
            
            async with httpx.AsyncClient(**client_kwargs) as client:
                response = await client.post(
                    api_url,
                    json=[{
                        "operationName": "getFsasReviews",
                        "variables": variables,
                        "query": query
                    }],
                    headers=headers
                )
                response.raise_for_status()
                
                data = response.json()
                logger.info(f"[ë¸”ë¡œê·¸ ë¦¬ë·°] API í˜¸ì¶œ ì„±ê³µ: place_id={place_id}, page={page}")
                logger.info(f"[ë¸”ë¡œê·¸ ë¦¬ë·°] âœ… RAW Response: {data}")
                
                # ì‘ë‹µì€ ë°°ì—´ë¡œ ì˜¤ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì‚¬ìš©
                if isinstance(data, list) and len(data) > 0:
                    data = data[0]
                
                logger.info(f"[ë¸”ë¡œê·¸ ë¦¬ë·°] ì‘ë‹µ êµ¬ì¡° í™•ì¸: has_data={data.get('data') is not None}, has_errors={data.get('errors') is not None}")
                
                # ì—ëŸ¬ ì²´í¬
                if data.get("errors"):
                    logger.error(f"[ë¸”ë¡œê·¸ ë¦¬ë·°] GraphQL ì—ëŸ¬: {data.get('errors')}")
                    return {
                        "total": 0,
                        "items": [],
                        "page": page,
                        "has_more": False
                    }
                
                # fsasReviews ë°ì´í„° ì¶”ì¶œ
                fsas_reviews = data.get("data", {}).get("fsasReviews")
                logger.info(f"[ë¸”ë¡œê·¸ ë¦¬ë·°] ğŸ” fsasReviews ê°ì²´: {fsas_reviews}")
                if not fsas_reviews:
                    logger.warning(f"[ë¸”ë¡œê·¸ ë¦¬ë·°] fsasReviews ì—†ìŒ: place_id={place_id}, data keys={list(data.keys())}")
                    return {
                        "total": 0,
                        "items": [],
                        "page": page,
                        "has_more": False
                    }
                
                total = fsas_reviews.get("total", 0)
                max_item_count = fsas_reviews.get("maxItemCount", 0)
                items = fsas_reviews.get("items", [])
                
                # has_more: itemsê°€ ìˆê³  ì•„ì§ ì „ì²´ totalì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
                # ê° í˜ì´ì§€ê°€ ëª‡ ê°œë¥¼ ë°˜í™˜í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ items ê°œìˆ˜ë¡œ íŒë‹¨
                has_more = len(items) > 0 and total > (page * len(items))
                
                logger.info(f"ë¸”ë¡œê·¸ ë¦¬ë·° ì¡°íšŒ ì„±ê³µ: place_id={place_id}, total={total}, max_item_count={max_item_count}, items_count={len(items)}, page={page}, has_more={has_more}")
                
                return {
                    "total": total,
                    "items": items,
                    "page": page,
                    "has_more": has_more
                }
                
        except Exception as e:
            logger.error(f"ë¸”ë¡œê·¸ ë¦¬ë·° ì¡°íšŒ ì˜ˆì™¸: {type(e).__name__} - {str(e)}")
            return {
                "total": 0,
                "items": [],
                "page": page,
                "has_more": False
            }
    
    async def get_all_today_visitor_reviews(
        self,
        place_id: str,
        target_date: Optional[datetime] = None
    ) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ë‚ ì§œì— ì‘ì„±ëœ ëª¨ë“  ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ
        
        Args:
            place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID
            target_date: ì¡°íšŒí•  ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
        
        Returns:
            í•´ë‹¹ ë‚ ì§œì— ì‘ì„±ëœ ëª¨ë“  ë¦¬ë·° ëª©ë¡
        """
        if target_date is None:
            target_date = datetime.now()
        
        target_date_str = target_date.strftime("%Y-%m-%d")
        all_reviews = []
        cursor = None
        max_iterations = 10  # ìµœëŒ€ 10íšŒ ë°˜ë³µ
        
        logger.info(f"ë‚ ì§œë³„ ë¦¬ë·° ì¡°íšŒ ì‹œì‘: place_id={place_id}, date={target_date_str}")
        
        for iteration in range(1, max_iterations + 1):
            result = await self.get_visitor_reviews(place_id, size=20, after=cursor)
            items = result.get("items", [])
            
            if not items:
                break
            
            # í•´ë‹¹ ë‚ ì§œì˜ ë¦¬ë·°ë§Œ í•„í„°ë§ (IDì—ì„œ ë‚ ì§œ ì¶”ì¶œ)
            for item in items:
                review_id = item.get("id")
                if review_id:
                    # IDì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                    review_date_str = self.extract_date_from_id(review_id)
                    if review_date_str:
                        if review_date_str == target_date_str:
                            all_reviews.append(item)
                        elif review_date_str < target_date_str:
                            # ë” ì˜¤ë˜ëœ ë¦¬ë·°ê°€ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
                            logger.info(f"ê³¼ê±° ë¦¬ë·° ë°œê²¬, ì¡°íšŒ ì¤‘ë‹¨: {review_date_str}")
                            return all_reviews
            
            if not result.get("has_more"):
                break
            
            cursor = result.get("last_cursor")
            if not cursor:
                break
        
        logger.info(f"ë‚ ì§œë³„ ë¦¬ë·° ì¡°íšŒ ì™„ë£Œ: {len(all_reviews)}ê°œ")
        return all_reviews
    
    async def get_reviews_by_date_range(
        self,
        place_id: str,
        start_date: datetime,
        end_date: datetime
    ) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ê¸°ê°„ ë‚´ì— ì‘ì„±ëœ ëª¨ë“  ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ
        
        Args:
            place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
        
        Returns:
            í•´ë‹¹ ê¸°ê°„ ë‚´ì— ì‘ì„±ëœ ëª¨ë“  ë¦¬ë·° ëª©ë¡
        """
        start_date_str = start_date.strftime("%Y-%m-%d")
        end_date_str = end_date.strftime("%Y-%m-%d")
        all_reviews = []
        
        # ê¸°ê°„ ì¼ìˆ˜ ê³„ì‚°
        date_diff = (end_date - start_date).days + 1  # +1ì€ ì‹œì‘ì¼ í¬í•¨
        
        # ê¸°ê°„ì— ë”°ë¼ ëª©í‘œ ê°œìˆ˜ì™€ í˜ì´ì§€ ìˆ˜ ê²°ì •
        # âš ï¸ Naver APIëŠ” size > 20ì´ë©´ visitorReviewsë¥¼ Noneìœ¼ë¡œ ë°˜í™˜í•¨!
        page_size = 20  # Naver API ìµœëŒ€ í—ˆìš©ì¹˜
        
        if date_diff <= 2:  # ì˜¤ëŠ˜ ë˜ëŠ” ì–´ì œ (1~2ì¼)
            target_reviews = 100
            max_pages = 5  # 20ê°œì”© 5í˜ì´ì§€
        elif date_diff <= 7:  # 7ì¼ê°„
            target_reviews = 400
            max_pages = 20  # 20ê°œì”© 20í˜ì´ì§€
        elif date_diff <= 30:  # 30ì¼ê°„
            target_reviews = 1000
            max_pages = 50  # 20ê°œì”© 50í˜ì´ì§€
        else:  # 30ì¼ ì´ˆê³¼
            target_reviews = 1000
            max_pages = 50
        
        print(f"[DEBUG] get_reviews_by_date_range START: place_id={place_id}, period={start_date_str} ~ {end_date_str}", flush=True)
        try:
            print(f"[DEBUG] ğŸ“Š ê¸°ê°„: {date_diff}ì¼ â†’ ëª©í‘œ={target_reviews}ê°œ, max_pages={max_pages}, page_size={page_size}", flush=True)
        except UnicodeEncodeError:
            print(f"[DEBUG] [STATS] Period: {date_diff}days -> target={target_reviews}, max_pages={max_pages}, page_size={page_size}", flush=True)
        
        page_dates = []  # ê° í˜ì´ì§€ì˜ ë‚ ì§œ ë²”ìœ„ ì¶”ì 
        seen_review_ids = set()  # ì´ë¯¸ ë³¸ ë¦¬ë·° ID ì¶”ì  (ì¤‘ë³µ ë°©ì§€)
        
        cursor = None  # Cursor ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜
        iteration = 1
        
        while iteration <= max_pages:
            print(f"[DEBUG] Iteration {iteration}/{max_pages} requesting (size={page_size}, cursor={cursor[:20] if cursor else 'None'}...)...", flush=True)
            result = await self.get_visitor_reviews(place_id, size=page_size, after=cursor)
            items = result.get("items", [])
            last_cursor = result.get("last_cursor")
            
            print(f"[DEBUG] Iteration {iteration}: items={len(items)}, total={result.get('total', 0)}", flush=True)
            
            if not items:
                print(f"[DEBUG] Iteration {iteration}: No items, stopping", flush=True)
                break
            
            # í•´ë‹¹ ê¸°ê°„ ë‚´ì˜ ë¦¬ë·°ë§Œ í•„í„°ë§ (ë‚ ì§œ ê°ì§€ ë¡œì§ ì‚¬ìš©)
            found_older_review = False
            page_review_dates = []
            page_included = 0
            page_excluded_future = 0
            page_excluded_past = 0
            
            # ìƒì„¸ ë¡œê¹… (ë””ë²„ê¹…ìš©, 100ê°œ ì´í•˜ì¼ ë•Œë§Œ)
            if len(items) <= 100:
                all_ids = [item.get('id') for item in items]
                unique_ids = len(set(all_ids))
                print(f"\n{'='*100}", flush=True)
                try:
                    print(f"[DEBUG] ğŸ“‹ Iteration {iteration} ìˆ˜ì‹  (Total: {len(items)}ê°œ, Unique: {unique_ids})", flush=True)
                except UnicodeEncodeError:
                    print(f"[DEBUG] [LIST] Iteration {iteration} received (Total: {len(items)}, Unique: {unique_ids})", flush=True)
                
                if unique_ids < len(all_ids):
                    try:
                        print(f"[DEBUG] âš ï¸ WARNING: Duplicate IDs found within response!", flush=True)
                    except UnicodeEncodeError:
                        print(f"[DEBUG] [WARNING] Duplicate IDs found within response!", flush=True)
                print(f"{'='*100}\n", flush=True)
            
            # ì²« 2ë²ˆì˜ iterationì—ì„œ ëª¨ë“  ë¦¬ë·° ID ì¶œë ¥ (í˜ì´ì§€ë„¤ì´ì…˜ ê²€ì¦)
            if iteration <= 2:
                print(f"\n[VERIFY] ===== Iteration {iteration} - ì „ì²´ ë¦¬ë·° ID ëª©ë¡ =====", flush=True)
                for idx, item in enumerate(items, 1):
                    review_id = item.get('id', 'N/A')
                    visited = item.get('visited', 'N/A')
                    cursor_preview = item.get('cursor', 'N/A')
                    cursor_preview = cursor_preview[:20] + '...' if cursor_preview != 'N/A' else 'N/A'
                    print(f"  [{idx:2d}] ID: {review_id}, ë°©ë¬¸ì¼: {visited}, cursor: {cursor_preview}", flush=True)
                print(f"[VERIFY] ===== Iteration {iteration} ë =====\n", flush=True)
            
            new_reviews_in_page = 0  # ì´ í˜ì´ì§€ì—ì„œ ìƒˆë¡œ ë°œê²¬í•œ ë¦¬ë·° ìˆ˜
            duplicates_in_page = 0  # ì´ í˜ì´ì§€ì—ì„œ ë°œê²¬í•œ ì¤‘ë³µ ìˆ˜
            
            for item in items:
                review_id = item.get("id")
                if not review_id:
                    continue
                
                # ì¤‘ë³µ ë¦¬ë·° ì²´í¬
                if review_id in seen_review_ids:
                    duplicates_in_page += 1
                    continue  # ì´ë¯¸ ë³¸ ë¦¬ë·°ëŠ” ê±´ë„ˆë›°ê¸°
                
                seen_review_ids.add(review_id)
                new_reviews_in_page += 1
                    
                # visited í•„ë“œì—ì„œ ë‚ ì§œ ì¶”ì¶œ ("1.10.ê¸ˆ" í˜•ì‹)
                visited_str = item.get("visited", "")
                review_date_str = self.parse_naver_date(visited_str)
                
                # visited ì‹¤íŒ¨ì‹œ IDì—ì„œ ì¶”ì¶œ ì‹œë„ (fallback)
                if not review_date_str:
                    review_date_str = self.extract_date_from_id(review_id)
                
                # ì²« iteration ì²« ë¦¬ë·° ë””ë²„ê¹…
                if iteration == 1 and len(page_review_dates) == 0:
                    print(f"[DEBUG] ========== FIRST REVIEW DATA ==========", flush=True)
                    print(f"[DEBUG] ID: {review_id}", flush=True)
                    print(f"[DEBUG] visited (raw): {visited_str}", flush=True)
                    print(f"[DEBUG] Parsed date: {review_date_str}", flush=True)
                    print(f"[DEBUG] =========================================", flush=True)
                
                if not review_date_str:
                    print(f"[DEBUG] Iteration {iteration}: Failed to extract date: visited={visited_str}, id={review_id}", flush=True)
                    continue
                    
                page_review_dates.append(review_date_str)
                
                # ì²« iteration ì²« 3ê°œ ë¦¬ë·°ì˜ ë‚ ì§œ ë¹„êµ ë¡œê·¸
                if iteration == 1 and len(page_review_dates) <= 3:
                    print(f"[DEBUG] Review #{len(page_review_dates)}: date={review_date_str}, range={start_date_str}~{end_date_str}", flush=True)
                    print(f"[DEBUG] Comparison: {start_date_str} <= {review_date_str} <= {end_date_str} = {start_date_str <= review_date_str <= end_date_str}", flush=True)
                
                if start_date_str <= review_date_str <= end_date_str:
                    all_reviews.append(item)
                    page_included += 1
                elif review_date_str > end_date_str:
                    # ì¢…ë£Œì¼ë³´ë‹¤ ìµœì‹  ë¦¬ë·° (ë¯¸ë˜)
                    page_excluded_future += 1
                elif review_date_str < start_date_str:
                    # ì‹œì‘ì¼ë³´ë‹¤ ì˜¤ë˜ëœ ë¦¬ë·° (ê³¼ê±°)
                    page_excluded_past += 1
                    # âš ï¸ ë²”ìœ„ ë‚´ ë¦¬ë·°ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì¤‘ë‹¨, ì•„ë‹ˆë©´ ê³„ì† (ìµœì‹  ë¦¬ë·°ë¼ë„ ë³´ì—¬ì£¼ê¸° ìœ„í•´)
                    if len(all_reviews) > 0:
                        # ì´ë¯¸ ë²”ìœ„ ë‚´ ë¦¬ë·°ë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                        print(f"[DEBUG] Found older review: {review_date_str} < {start_date_str}, stopping", flush=True)
                        found_older_review = True
                        break
                    # ì•„ì§ ë²”ìœ„ ë‚´ ë¦¬ë·°ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ ìµœì‹  ë¦¬ë·°ë¼ë„ í¬í•¨
                    all_reviews.append(item)
                    page_included += 1
        
            # Iteration ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…
            if duplicates_in_page > 0:
                print(f"[DEBUG] Iteration {iteration}: DUPLICATES = {duplicates_in_page}/{len(items)}", flush=True)
            
            if page_review_dates:
                min_date = min(page_review_dates)
                max_date = max(page_review_dates)
                print(f"[DEBUG] Iteration {iteration}: ë‚ ì§œë²”ìœ„={min_date}~{max_date}, ì¶”ì¶œ={new_reviews_in_page}, ì¤‘ë³µ={duplicates_in_page}, í¬í•¨={page_included}, ë¯¸ë˜ì œì™¸={page_excluded_future}, ê³¼ê±°ì œì™¸={page_excluded_past}", flush=True)
                page_dates.append((iteration, min_date, max_date, page_included))
            else:
                print(f"[DEBUG] Iteration {iteration}: ì¶”ì¶œ={new_reviews_in_page}, ì¤‘ë³µ={duplicates_in_page}, í¬í•¨={page_included}", flush=True)
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
            # 1. ê³¼ê±° ë¦¬ë·° ë°œê²¬ - ì˜¤ë˜ëœ ë¦¬ë·°ê°€ ë‚˜ì™”ìœ¼ë¯€ë¡œ ì¤‘ë‹¨
            if found_older_review:
                print(f"[DEBUG] STOP: Iteration {iteration} found older review", flush=True)
                break
            
            # 2. ì´ë²ˆ iterationì—ì„œ ìƒˆë¡œìš´ ë¦¬ë·°ê°€ 0ê°œ - ëª¨ë‘ ì¤‘ë³µì´ë¯€ë¡œ ì¤‘ë‹¨
            if new_reviews_in_page == 0 and iteration > 1:
                print(f"[DEBUG] STOP: Iteration {iteration} has no new reviews (all duplicates)", flush=True)
                break
            
            # 3. ì´ë²ˆ iterationì—ì„œ í¬í•¨ëœ ë¦¬ë·°ê°€ 0ê°œ - ë²”ìœ„ ë°– ë¦¬ë·°ë§Œ ìˆì„ ë•Œ
            # ì²« í˜ì´ì§€ê°€ ì•„ë‹ˆê³ , ì´ë¯¸ ì¼ë¶€ ë¦¬ë·°ë¥¼ ì°¾ì•˜ë‹¤ë©´ ì¤‘ë‹¨
            if page_included == 0 and iteration > 1 and len(all_reviews) > 0:
                print(f"[DEBUG] STOP: Iteration {iteration} has 0 included reviews (already found some)", flush=True)
                break
            # ì²« í˜ì´ì§€ì—ì„œ í¬í•¨ëœ ë¦¬ë·°ê°€ 0ê°œë©´ ê³„ì† ì§„í–‰ (ìµœì‹  ë¦¬ë·°ë¼ë„ ë³´ì—¬ì£¼ê¸° ìœ„í•´)
            
            # 4. ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±
            if len(all_reviews) >= target_reviews:
                print(f"[DEBUG] STOP: Target reached ({len(all_reviews)}/{target_reviews})", flush=True)
                break
            
            # 5. ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ (cursorê°€ ì—†ê±°ë‚˜ has_moreê°€ False)
            if not result.get("has_more") or not last_cursor:
                print(f"[DEBUG] STOP: No more items (has_more={result.get('has_more')}, cursor={last_cursor is not None})", flush=True)
                break
            
            # ë‹¤ìŒ iterationì„ ìœ„í•´ cursor ì—…ë°ì´íŠ¸
            cursor = last_cursor
            iteration += 1
        
        # ìµœì¢… ìš”ì•½
        iterations_processed = len(page_dates)
        print(f"\n{'='*100}", flush=True)
        print(f"[ê²°ê³¼] ê¸°ê°„ë³„ ë¦¬ë·° ì¶”ì¶œ ì™„ë£Œ (Cursor ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜)", flush=True)
        print(f"  ìš”ì²­ ê¸°ê°„: {start_date_str} ~ {end_date_str} ({date_diff}ì¼)", flush=True)
        print(f"  ëª©í‘œ ê°œìˆ˜: {target_reviews}ê°œ", flush=True)
        print(f"  ì²˜ë¦¬ Iterations: {iterations_processed}íšŒ (size={page_size})", flush=True)
        print(f"  ìµœì¢… ê²°ê³¼: {len(all_reviews)}ê°œ", flush=True)
        
        if page_dates:
            print(f"  Iterationë³„ ìš”ì•½:", flush=True)
            for it, min_d, max_d, included in page_dates:
                print(f"    Iteration {it}: {min_d}~{max_d}, í¬í•¨={included}", flush=True)
        
        print(f"{'='*100}\n", flush=True)
        
        if all_reviews:
            logger.info(f"[OK] ê¸°ê°„ë³„ ë¦¬ë·° ì¡°íšŒ ì™„ë£Œ: {len(all_reviews)}ê°œ (ìš”ì²­: {start_date_str}~{end_date_str})")
        else:
            logger.info(f"[WARN] ê¸°ê°„ë³„ ë¦¬ë·° ì¡°íšŒ ì™„ë£Œ: 0ê°œ (ê¸°ê°„: {start_date_str} ~ {end_date_str})")
        
        return all_reviews
    
    def parse_naver_date(self, date_str: str) -> Optional[str]:
        """
        ë„¤ì´ë²„ ë‚ ì§œ í˜•ì‹ íŒŒì‹±: "1.10.ê¸ˆ" â†’ "2026-01-10"
        
        Args:
            date_str: "ì›”.ì¼.ìš”ì¼" í˜•ì‹ (ì˜ˆ: "1.10.ê¸ˆ", "12.25.ìˆ˜")
        
        Returns:
            ISO í˜•ì‹ ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD)
        """
        try:
            if not date_str or date_str == "":
                return None
            
            # "1.10.ê¸ˆ" â†’ ["1", "10", "ê¸ˆ"]
            parts = date_str.split(".")
            if len(parts) < 2:
                return None
            
            month = int(parts[0])
            day = int(parts[1])
            
            # í˜„ì¬ ë…„ë„ ê°€ì • (KST ê¸°ì¤€)
            KST = pytz.timezone('Asia/Seoul')
            now = datetime.now(KST)
            current_year = now.year
            
            # ë‚ ì§œ ìƒì„±
            review_date = datetime(current_year, month, day)
            
            # ë¯¸ë˜ ë‚ ì§œë¼ë©´ ì‘ë…„ìœ¼ë¡œ ë³€ê²½
            if review_date.replace(tzinfo=None) > now.replace(tzinfo=None):
                review_date = datetime(current_year - 1, month, day)
            
            return review_date.strftime("%Y-%m-%d")
        except Exception as e:
            logger.debug(f"ë„¤ì´ë²„ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}, {str(e)}")
            return None
    
    def extract_date_from_id(self, review_id: str) -> str:
        """
        ë¦¬ë·° ID(MongoDB ObjectId)ì—ì„œ ì‘ì„± ë‚ ì§œ ì¶”ì¶œ
        
        Args:
            review_id: ë„¤ì´ë²„ ë¦¬ë·° ID (24ì hex string)
        
        Returns:
            ISO í˜•ì‹ ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD, KST ê¸°ì¤€)
        """
        try:
            # ObjectIdì˜ ì²« 8ìëŠ” Unix timestamp (hex)
            timestamp_hex = review_id[:8]
            timestamp = int(timestamp_hex, 16)
            
            # UTC ì‹œê°„ìœ¼ë¡œ ë³€í™˜ í›„ KSTë¡œ ë³€ê²½
            dt_utc = datetime.fromtimestamp(timestamp, tz=pytz.utc)
            KST = pytz.timezone('Asia/Seoul')
            dt_kst = dt_utc.astimezone(KST)
            return dt_kst.strftime("%Y-%m-%d")
        except Exception as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
            logger.debug(f"ë¦¬ë·° ID ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {review_id}, {str(e)}")
            return None
    
    def parse_review_data(self, review: Dict[str, Any], review_type: str) -> Dict[str, Any]:
        """
        ë„¤ì´ë²„ ë¦¬ë·° ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì—¬ í‘œì¤€ í¬ë§·ìœ¼ë¡œ ë³€í™˜
        
        Args:
            review: ë„¤ì´ë²„ API ë¦¬ë·° ì›ë³¸ ë°ì´í„°
            review_type: 'visitor' ë˜ëŠ” 'blog'
        
        Returns:
            íŒŒì‹±ëœ ë¦¬ë·° ë°ì´í„°
        """
        if review_type == "visitor":
            author = review.get("author", {})
            media = review.get("media", [])
            images = [m.get("thumbnail") for m in media if m.get("type") == "image"]
            
            review_id = str(review.get("id", ""))
            # visited í•„ë“œì—ì„œ ë‚ ì§œ ì¶”ì¶œ ("1.10.ê¸ˆ" í˜•ì‹)
            visited_str = review.get("visited", "")
            review_date = self.parse_naver_date(visited_str)
            
            # visited ì‹¤íŒ¨ì‹œ IDì—ì„œ ì¶”ì¶œ ì‹œë„
            if not review_date:
                review_date = self.extract_date_from_id(review_id)
            
            # ì‘ì„±ì ë¦¬ë·° ìˆ˜ëŠ” Naver APIì—ì„œ ì œê³µí•˜ì§€ ì•ŠìŒ (reviewCount í•„ë“œ ì—†ìŒ)
            # í–¥í›„ ê°œì„ : ì‘ì„±ì í”„ë¡œí•„ í˜ì´ì§€ í¬ë¡¤ë§ ë˜ëŠ” ë‹¤ë¥¸ ë°©ë²• í•„ìš”
            author_review_count = 0
            is_power_reviewer = False
            
            return {
                "naver_review_id": review_id,
                "review_type": "visitor",
                "author_name": author.get("nickname", ""),
                "author_id": str(author.get("id", "")),
                "author_review_count": author_review_count,
                "is_power_reviewer": is_power_reviewer,
                "is_receipt_review": False,  # tags í•„ë“œê°€ Noneì´ë¯€ë¡œ íŒë‹¨ ë¶ˆê°€
                "is_reservation_review": False,  # tags í•„ë“œê°€ Noneì´ë¯€ë¡œ íŒë‹¨ ë¶ˆê°€
                "rating": float(review.get("rating")) if review.get("rating") is not None else None,
                "content": review.get("body", ""),
                "images": images,
                "review_date": review_date,  # IDì—ì„œ ì¶”ì¶œí•œ ë‚ ì§œ
                "like_count": 0,  # heart í•„ë“œ ì œê±°ë¨
                "comment_count": 1 if review.get("reply") else 0
            }
        
        elif review_type == "blog":
            return {
                "naver_review_id": str(review.get("id", "")),
                "review_type": "blog",
                "author_name": review.get("author", ""),
                "author_id": "",
                "author_review_count": 0,
                "is_power_reviewer": False,
                "is_receipt_review": False,
                "is_reservation_review": False,
                "rating": None,
                "content": review.get("summary", "") or review.get("title", ""),
                "images": [review.get("thumbnail")] if review.get("thumbnail") else [],
                "review_date": review.get("created"),
                "like_count": 0,
                "comment_count": 0
            }
        
        return {}
    
    async def get_place_info(
        self, 
        place_id: str, 
        store_name: str = None,
        x: str = None,
        y: str = None
    ) -> Dict[str, Any]:
        """
        ë§¤ì¥ ìƒì„¸ ì •ë³´ ì¡°íšŒ (ë¦¬ë·° ìˆ˜, í‰ì  ë“±)
        
        places ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ë§¤ì¥ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        ë§¤ì¥ëª…ìœ¼ë¡œ ê²€ìƒ‰ í›„ place_idë¥¼ ë§¤ì¹­í•©ë‹ˆë‹¤.
        
        Args:
            place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID
            store_name: ë§¤ì¥ëª… (ê²€ìƒ‰ì— ì‚¬ìš©)
            x: ê²½ë„ (ì„ íƒ)
            y: ìœ„ë„ (ì„ íƒ)
        
        Returns:
            {
                "place_id": ë§¤ì¥ ID,
                "name": ë§¤ì¥ëª…,
                "visitor_review_count": ë°©ë¬¸ì ë¦¬ë·° ìˆ˜,
                "blog_review_count": ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜,
                "rating": í‰ì ,
                "description": í•œì¤„í‰ (ë¹ˆ ë¬¸ìì—´, APIì—ì„œ ì œê³µí•˜ì§€ ì•ŠìŒ)
            }
        """
        query = """
        query getPlacesList($input: PlacesInput) {
            places(input: $input) {
                items {
                    id
                    name
                    visitorReviewCount
                    blogCafeReviewCount
                    visitorReviewScore
                    category
                    address
                    roadAddress
                    imageUrl
                }
            }
        }
        """
        
        # ë§¤ì¥ëª…ìœ¼ë¡œ ê²€ìƒ‰ (place_idë¡œëŠ” ê²€ìƒ‰ ì•ˆ ë¨)
        # store_nameì´ ì—†ìœ¼ë©´ place_idë¡œ ì‹œë„
        search_query = store_name if store_name else place_id
        
        # ì¢Œí‘œ ì„¤ì • (stores í…Œì´ë¸” ê°’ ìš°ì„ , ì—†ìœ¼ë©´ ì„œìš¸ ê¸°ì¤€)
        coord_x = x if x else "127.0276"
        coord_y = y if y else "37.4979"
        
        variables = {
            "input": {
                "query": search_query,
                "start": 1,
                "display": 10,  # ì¤‘ë³µ ê²°ê³¼ ëŒ€ë¹„
                "deviceType": "mobile",
                "x": coord_x,
                "y": coord_y
            }
        }
        
        try:
            logger.info(f"ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹œì‘: place_id={place_id}, store_name='{store_name}', x={coord_x}, y={coord_y}")
            
            # ë§¤ì¥ëª…ì´ ì—†ìœ¼ë©´ ë¦¬ë·°ì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
            if not store_name or store_name.strip() == "":
                logger.warning(f"[WARN] ë§¤ì¥ëª… ì—†ìŒ. ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì‹œë„")
                try:
                    visitor_result = await self.get_visitor_reviews(place_id, size=1)
                    if visitor_result and visitor_result.get("items"):
                        store_name = visitor_result["items"][0].get("businessName", "")
                        logger.info(f"[OK] ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì„±ê³µ: '{store_name}'")
                        search_query = store_name
                except Exception as e:
                    logger.error(f"ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    # ì—¬ì „íˆ ë§¤ì¥ëª…ì´ ì—†ìœ¼ë©´ place_id ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì‹¤íŒ¨í•  í™•ë¥  ë†’ìŒ)
                    search_query = place_id
            
            # í”„ë¡ì‹œ ì¡°ê±´ë¶€ ì„¤ì •
            client_kwargs = {"timeout": self.TIMEOUT}
            proxy_url = get_proxy()
            if proxy_url:
                client_kwargs["proxy"] = proxy_url
            
            async with httpx.AsyncClient(**client_kwargs) as client:
                response = await client.post(
                    self.GRAPHQL_URL,
                    json={
                        "operationName": "getPlacesList",
                        "variables": variables,
                        "query": query
                    },
                    headers=self.headers
                )
                
                if response.status_code != 200:
                    logger.error(f"ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: status={response.status_code}")
                    return None
                
                data = response.json()
                
                if "errors" in data:
                    logger.error(f"GraphQL ì—ëŸ¬: {data['errors']}")
                    return None
                
                items = data.get("data", {}).get("places", {}).get("items", [])
                
                logger.info(f"ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(items)}")
                if items:
                    for idx, item in enumerate(items):
                        logger.info(f"  [{idx+1}] {item.get('name')} (ID: {item.get('id')})")
                
                if not items:
                    logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: query={search_query}, place_id={place_id}")
                    return None
                
                # place_idê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í•­ëª© ì°¾ê¸°
                place = None
                for item in items:
                    if str(item.get("id")) == str(place_id):
                        place = item
                        logger.info(f"[OK] place_id ì¼ì¹˜: {item.get('name')} (ID: {item.get('id')})")
                        break
                
                # ì¼ì¹˜í•˜ëŠ” í•­ëª©ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš© (ë§¤ì¥ëª…ì´ ìœ ì‚¬í•˜ë‹¤ê³  ê°€ì •)
                if not place and items:
                    place = items[0]
                    logger.warning(f"[WARN] place_id ë¶ˆì¼ì¹˜. ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©: {place.get('name')} (ID: {place.get('id')}) - ìš”ì²­í•œ ID: {place_id}")
                
                if not place:
                    logger.error(f"[ERROR] ë§¤ì¥ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: place_id={place_id}")
                    return None
                
                # ìˆ«ì íŒŒì‹± í—¬í¼ í•¨ìˆ˜
                def parse_int(value):
                    """ì‰¼í‘œê°€ í¬í•¨ëœ ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜"""
                    if value is None:
                        return 0
                    if isinstance(value, int):
                        return value
                    try:
                        return int(str(value).replace(',', ''))
                    except (ValueError, AttributeError):
                        return 0
                
                def parse_float(value):
                    """ë¬¸ìì—´ì„ floatë¡œ ë³€í™˜"""
                    if value is None:
                        return None
                    if isinstance(value, (int, float)):
                        return float(value)
                    try:
                        return float(str(value).replace(',', ''))
                    except (ValueError, AttributeError):
                        return None
                
                result = {
                    "place_id": str(place.get("id", place_id)),
                    "name": place.get("name", ""),
                    "category": place.get("category", ""),
                    "address": place.get("address", ""),
                    "roadAddress": place.get("roadAddress", ""),
                    "visitor_review_count": parse_int(place.get("visitorReviewCount")),
                    "blog_review_count": parse_int(place.get("blogCafeReviewCount")),
                    "visitorReviewScore": parse_float(place.get("visitorReviewScore")),
                    "rating": parse_float(place.get("visitorReviewScore")),  # í˜¸í™˜ì„±
                    "description": "",
                    "image_url": place.get("imageUrl", ""),
                    "thumbnail": place.get("imageUrl", ""),  # í˜¸í™˜ì„±
                }
                
                logger.info(f"ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì„±ê³µ: {result}")
                return result
                
        except Exception as e:
            logger.error(f"ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì˜ˆì™¸: {type(e).__name__} - {str(e)}")
            return None
    
    async def get_blog_reviews_html(
        self,
        place_id: str,
        store_name: str,
        road_address: str = None,
        max_pages: int = 15
    ) -> List[Dict[str, Any]]:
        """
        ë„¤ì´ë²„ í†µí•© ê²€ìƒ‰ ë¸”ë¡œê·¸ íƒ­ì—ì„œ HTML íŒŒì‹± (í™œì„±í™” ê¸°ëŠ¥ ì „ìš©)
        
        Args:
            place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID (ë¡œê¹…ìš©)
            store_name: ë§¤ì¥ëª… (ê²€ìƒ‰ ì¿¼ë¦¬)
            road_address: ë„ë¡œëª…ì£¼ì†Œ (í•„í„°ë§ìš©)
            max_pages: ì‚¬ìš© ì•ˆ í•¨ (í˜¸í™˜ì„± ìœ ì§€)
        
        Returns:
            List[Dict]: ë¸”ë¡œê·¸ ë¦¬ë·° ëª©ë¡ (date, title, author ë“±)
        """
        try:
            # ë„¤ì´ë²„ í†µí•© ê²€ìƒ‰ URL ìƒì„± (ë¸”ë¡œê·¸ íƒ­, ìµœì‹ ìˆœ)
            from urllib.parse import quote
            
            # ì§€ì—­êµ¬ ì¶”ì¶œ
            district = self._extract_district_from_address(road_address) if road_address else None
            
            # ê²€ìƒ‰ì–´: "ë§¤ì¥ëª… + ì§€ì—­êµ¬"
            if district:
                search_query = f"{store_name} {district}"
            else:
                search_query = store_name
            
            query = quote(search_query)
            
            # ë„¤ì´ë²„ ê²€ìƒ‰ í˜ì´ì§€ìš© í—¤ë”
            search_headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://www.naver.com/",
            }
            
            # ì—¬ëŸ¬ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (60ì¼ ì´ì „ ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ë•Œê¹Œì§€)
            all_reviews = []
            max_pages = 10  # ìµœëŒ€ 10í˜ì´ì§€ (ì•½ 300ê°œ)
            target_days = 60  # 60ì¼ ì¼í‰ê·  ê³„ì‚°ì„ ìœ„í•´
            
            from datetime import datetime, timezone, timedelta
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=target_days)
            found_old_enough = False
            
            # í”„ë¡ì‹œ ì¡°ê±´ë¶€ ì„¤ì •
            client_kwargs = {"timeout": 10.0}
            proxy_url = get_proxy()
            if proxy_url:
                client_kwargs["proxy"] = proxy_url
            
            async with httpx.AsyncClient(**client_kwargs) as client:
                for page in range(max_pages):
                    start = page * 30 + 1  # ë„¤ì´ë²„ëŠ” 1, 31, 61, 91... í˜•íƒœë¡œ í˜ì´ì§•
                    url = f"https://search.naver.com/search.naver?ssc=tab.blog.all&query={query}&sm=tab_opt&nso=so:dd,p:all&start={start}"
                    
                    if page == 0:
                        logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] HTTP ìš”ì²­ ì‹œì‘: {url}")
                        logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] ê²€ìƒ‰ì–´: '{search_query}', Place ID: {place_id}, ëª©í‘œ: {target_days}ì¼ ì´ì „ê¹Œì§€")
                    
                    try:
                        response = await client.get(url, headers=search_headers, follow_redirects=True)
                        
                        if response.status_code != 200:
                            logger.warning(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] í˜ì´ì§€ {page+1} HTTP {response.status_code}")
                            continue
                        
                        html = response.text
                        
                        if page == 0:
                            logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] HTML ê¸¸ì´: {len(html)} bytes")
                        
                        # HTML íŒŒì‹± (í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”)
                        from bs4 import BeautifulSoup
                        import re
                        
                        soup = BeautifulSoup(html, 'html.parser')
                        blog_links_in_page = soup.find_all('a', href=re.compile(r'blog\.naver\.com/[^/]+/\d+'))
                        
                        # HTMLì— ë¸”ë¡œê·¸ ë§í¬ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ì§„ì§œ í˜ì´ì§€ ë
                        if len(blog_links_in_page) == 0:
                            logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] í˜ì´ì§€ {page+1}: HTMLì— ë¸”ë¡œê·¸ ë§í¬ ì—†ìŒ (í˜ì´ì§€ ì¢…ë£Œ)")
                            break
                        
                        # ì´ í˜ì´ì§€ì˜ ëª¨ë“  ë¸”ë¡œê·¸ ì¤‘ ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œ ì°¾ê¸° (ë§¤ì¹­ ì—¬ë¶€ ë¬´ê´€)
                        oldest_date_in_page = None
                        checked_dates = 0
                        
                        # í˜ì´ì§€ ë‚´ ì¼ë¶€ ë¸”ë¡œê·¸ ë§í¬ì˜ ë‚ ì§œë§Œ ìƒ˜í”Œë§ í™•ì¸ (ì†ë„ ìµœì í™”)
                        # ìµœëŒ€ 3ê°œë§Œ í™•ì¸í•˜ì—¬ ë¹ ë¥´ê²Œ 60ì¼ ê¸°ì¤€ í™•ì¸
                        for link_idx, link in enumerate(blog_links_in_page):
                            if link_idx >= 3:  # ìµœëŒ€ 3ê°œê¹Œì§€ ìƒ˜í”Œë§ (ì†ë„ ìµœì í™”)
                                break
                            
                            try:
                                parent = link.parent
                                date_found_for_link = False  # ì´ ë§í¬ì˜ ë‚ ì§œë¥¼ ì°¾ì•˜ëŠ”ì§€ ì—¬ë¶€
                                
                                for level in range(5):
                                    if not parent or date_found_for_link:
                                        break
                                    
                                    date_candidates = parent.find_all(['span', 'time', 'div'], limit=10)
                                    for candidate in date_candidates:
                                        text = candidate.get_text(strip=True)
                                        
                                        if not text or len(text) > 20:
                                            continue
                                        
                                        # ë‚ ì§œ íŒ¨í„´ í™•ì¸
                                        is_date = False
                                        if re.match(r'^\d+\s*(ì¼|ì£¼|ì‹œê°„|ë¶„)\s*ì „', text):
                                            is_date = True
                                        elif re.match(r'^\d{2,4}\.\d{1,2}\.\d{1,2}\.?', text):
                                            is_date = True
                                        
                                        if is_date:
                                            parsed_date = self._parse_naver_search_date(text)
                                            if parsed_date:
                                                checked_dates += 1
                                                if oldest_date_in_page is None or parsed_date < oldest_date_in_page:
                                                    oldest_date_in_page = parsed_date
                                                date_found_for_link = True  # ì´ ë§í¬ì˜ ë‚ ì§œë¥¼ ì°¾ì•˜ìŒ
                                                
                                                # ì¡°ê¸° ì¢…ë£Œ: 60ì¼ ì´ì „ ë‚ ì§œ ë°œê²¬ ì‹œ ë” ì´ìƒ í™•ì¸ ì•ˆ í•¨
                                                if parsed_date <= cutoff_date:
                                                    logger.debug(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] ì¡°ê¸° ë°œê²¬: {(datetime.now(timezone.utc) - parsed_date).days}ì¼ ì „ ë¸”ë¡œê·¸ (ìƒ˜í”Œ {link_idx+1}ë²ˆì§¸)")
                                                    break  # ì´ ë§í¬ì˜ ë‹¤ë¥¸ ë‚ ì§œëŠ” í™•ì¸í•˜ì§€ ì•ŠìŒ
                                                break  # ì´ ë§í¬ì˜ ë‹¤ë¥¸ ë‚ ì§œëŠ” í™•ì¸í•˜ì§€ ì•ŠìŒ
                                    
                                    if date_found_for_link:
                                        break  # ì´ ë§í¬ì˜ ë¶€ëª¨ë¥¼ ë” íƒìƒ‰í•˜ì§€ ì•ŠìŒ
                                    parent = parent.parent
                                
                                # 60ì¼ ì´ì „ ë‚ ì§œë¥¼ ì´ë¯¸ ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ ë§í¬ í™•ì¸ ë¶ˆí•„ìš”
                                if oldest_date_in_page and oldest_date_in_page <= cutoff_date:
                                    break
                                    
                            except:
                                continue
                        
                        # ë¸”ë¡œê·¸ ë¦¬ë·° ì¶”ì¶œ (ë§¤ì¥ëª… í•„í„°ë§ ì ìš©, ì´ë¯¸ íŒŒì‹±ëœ soup ì¬ì‚¬ìš©)
                        page_reviews = self._parse_naver_blog_search_html(soup, store_name)
                        all_reviews.extend(page_reviews)
                        
                        # Early stopping íŒë‹¨: ì´ í˜ì´ì§€ì˜ ê°€ì¥ ì˜¤ë˜ëœ ë¸”ë¡œê·¸ê°€ 60ì¼ ì´ì „ì¸ì§€ í™•ì¸
                        if oldest_date_in_page:
                            days_old = (datetime.now(timezone.utc) - oldest_date_in_page).days
                            logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] í˜ì´ì§€ {page+1}/{max_pages}: ë§¤ì¹­ {len(page_reviews)}ê°œ (ëˆ„ì : {len(all_reviews)}ê°œ), í˜ì´ì§€ ê°€ì¥ ì˜¤ë˜ëœ: {days_old}ì¼ ì „ (í™•ì¸: {checked_dates}ê°œ)")
                            
                            # 60ì¼ ì´ì „ ë¸”ë¡œê·¸ ë°œê²¬ â†’ ì¡°ê¸° ì¢…ë£Œ
                            if oldest_date_in_page <= cutoff_date:
                                found_old_enough = True
                                logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] âœ“ {target_days}ì¼ ì´ì „ ë¸”ë¡œê·¸ ë°œê²¬ (ì¡°ê¸° ì¢…ë£Œ)")
                                break
                        else:
                            logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] í˜ì´ì§€ {page+1}/{max_pages}: ë§¤ì¹­ {len(page_reviews)}ê°œ (ëˆ„ì : {len(all_reviews)}ê°œ), ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨")
                        
                        
                        # í˜ì´ì§€ ê°„ ë”œë ˆì´ (bot ê°ì§€ ë°©ì§€, ìµœì†Œí™”)
                        if page < max_pages - 1:
                            await asyncio.sleep(0.3)
                    
                    except Exception as e:
                        logger.warning(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] í˜ì´ì§€ {page+1} ì˜¤ë¥˜: {str(e)}")
                        continue
                
                if not found_old_enough and len(all_reviews) > 0:
                    logger.warning(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] âš  {target_days}ì¼ ì´ì „ ë¸”ë¡œê·¸ë¥¼ ì°¾ì§€ ëª»í•¨ ({max_pages}í˜ì´ì§€ ë„ë‹¬)")
                
                logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] íŒŒì‹± ì™„ë£Œ: ì´ {len(all_reviews)}ê°œ")
                
                return all_reviews
            
        except Exception as e:
            logger.error(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] ì˜ˆì™¸ ë°œìƒ: {type(e).__name__} - {str(e)}", exc_info=True)
            return []
    
    def _extract_district_from_address(self, address: str) -> Optional[str]:
        """
        ì£¼ì†Œì—ì„œ ì§€ì—­êµ¬ ì´ë¦„ ì¶”ì¶œ
        
        Args:
            address: ë„ë¡œëª…ì£¼ì†Œ (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘ë‘êµ¬ ë©´ëª©ì²œë¡œ6ê¸¸ 22")
        
        Returns:
            Optional[str]: ì§€ì—­êµ¬ ì´ë¦„ (ì˜ˆ: "ì¤‘ë‘êµ¬") ë˜ëŠ” None
        """
        if not address:
            return None
        
        try:
            # "êµ¬"ë¡œ ëë‚˜ëŠ” ë‹¨ì–´ ì°¾ê¸° (ì˜ˆ: ì¤‘ë‘êµ¬, ê°•ë‚¨êµ¬, ë¶„ë‹¹êµ¬)
            match = re.search(r'(\S+êµ¬)', address)
            if match:
                district = match.group(1)
                logger.debug(f"[ì£¼ì†Œ íŒŒì‹±] ì§€ì—­êµ¬ ì¶”ì¶œ ì„±ê³µ: '{address}' â†’ '{district}'")
                return district
            
            # "êµ¬"ê°€ ì—†ìœ¼ë©´ "êµ°" ì°¾ê¸° (ì˜ˆ: ì–‘í‰êµ°, ê°€í‰êµ°)
            match = re.search(r'(\S+êµ°)', address)
            if match:
                district = match.group(1)
                logger.debug(f"[ì£¼ì†Œ íŒŒì‹±] ì§€ì—­êµ¬ ì¶”ì¶œ ì„±ê³µ (êµ°): '{address}' â†’ '{district}'")
                return district
            
            logger.debug(f"[ì£¼ì†Œ íŒŒì‹±] ì§€ì—­êµ¬ ì¶”ì¶œ ì‹¤íŒ¨: '{address}'")
            return None
        
        except Exception as e:
            logger.warning(f"[ì£¼ì†Œ íŒŒì‹±] ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            return None
    
    def _parse_naver_blog_search_html(
        self, 
        soup,  # BeautifulSoup ê°ì²´ë¥¼ ì§ì ‘ ë°›ì•„ì„œ ì¬íŒŒì‹± ë°©ì§€ (ì„±ëŠ¥ ìµœì í™”)
        store_name: str
    ) -> List[Dict[str, Any]]:
        """
        ë„¤ì´ë²„ í†µí•© ê²€ìƒ‰ ë¸”ë¡œê·¸ íƒ­ HTML íŒŒì‹± (ë§¤ì¥ëª… í•„í„°ë§ ì ìš©)
        
        Args:
            soup: BeautifulSoup íŒŒì‹±ëœ ê°ì²´ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì¬ì‚¬ìš©)
            store_name: ë§¤ì¥ëª… (í•„í„°ë§ì— ì‚¬ìš©)
        
        Returns:
            List[Dict]: íŒŒì‹±ëœ ë¸”ë¡œê·¸ ë¦¬ë·° ëª©ë¡ (ë§¤ì¥ëª… í•„í„°ë§ ì ìš©)
        """
        from datetime import datetime, timedelta
        
        reviews = []
        
        # ë§¤ì¥ëª… í•„í„°ë§ ì¤€ë¹„ (ë‹¨ìˆœí™”)
        # 1. ì •í™•í•œ ë§¤ì¥ëª…
        exact_store_name = store_name.strip()
        
        # 2. ë§¤ì¥ëª… ì²« ë‹¨ì–´ (ë„ì–´ì“°ê¸°ê°€ ìˆì„ ë•Œë§Œ)
        has_space = ' ' in exact_store_name
        first_word_store_name = exact_store_name.split()[0] if has_space else ""
        
        # ë¹„êµë¥¼ ìœ„í•´ ì†Œë¬¸ì ë³€í™˜ ë° ê³µë°± ì œê±° (ë¸”ë¡œê·¸ ì œëª©/ë¯¸ë¦¬ë³´ê¸°ì˜ ë„ì–´ì“°ê¸° ë¬´ì‹œ)
        exact_store_lower = exact_store_name.lower().replace(" ", "")
        first_word_lower = first_word_store_name.lower().replace(" ", "") if first_word_store_name else ""
        
        if has_space:
            logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] ë§¤ì¥ëª…: '{exact_store_name}' (ì •ê·œí™”: '{exact_store_lower}'), ì²« ë‹¨ì–´: '{first_word_store_name}' (ì •ê·œí™”: '{first_word_lower}')")
        else:
            logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] ë§¤ì¥ëª…: '{exact_store_name}' (ì •ê·œí™”: '{exact_store_lower}', ë„ì–´ì“°ê¸° ì—†ìŒ)")
        
        # ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì§ì ‘ ì°¾ê¸°
        blog_links = soup.find_all('a', href=re.compile(r'blog\.naver\.com/[^/]+/\d+'))
        logger.info(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] ë°œê²¬ëœ ë¸”ë¡œê·¸ ë§í¬: {len(blog_links)}ê°œ")
        
        processed_urls = set()  # ì¤‘ë³µ ë°©ì§€
        filtered_count = 0  # í•„í„°ë§ìœ¼ë¡œ ì œì™¸ëœ ê°œìˆ˜
        
        for idx, link in enumerate(blog_links):
            try:
                url = link.get('href', '')
                if not url or url in processed_urls:
                    continue
                
                processed_urls.add(url)
                
                # ì œëª© ì¶”ì¶œ
                title = link.get_text(strip=True)
                if not title or len(title) < 5:  # ë„ˆë¬´ ì§§ì€ ì œëª©ì€ ë¬´ì‹œ
                    continue
                
                # ë¯¸ë¦¬ë³´ê¸°(description) ì¶”ì¶œ: ë¶€ëª¨ ìš”ì†Œì—ì„œ ì°¾ê¸°
                description = ""
                parent_for_desc = link.parent
                for level in range(3):  # ìµœëŒ€ 3ë‹¨ê³„ê¹Œì§€ íƒìƒ‰
                    if not parent_for_desc:
                        break
                    # ë¶€ëª¨ ìš”ì†Œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ ì œëª©ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ê°€ ë¯¸ë¦¬ë³´ê¸°
                    parent_text = parent_for_desc.get_text(strip=True)
                    # ì œëª©ë³´ë‹¤ ê¸´ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì´ ë¯¸ë¦¬ë³´ê¸°ë¥¼ í¬í•¨
                    if len(parent_text) > len(title) + 10:  # ì œëª©ë³´ë‹¤ ì¶©ë¶„íˆ ê¸¸ë©´
                        description = parent_text
                        break
                    parent_for_desc = parent_for_desc.parent
                
                # ë§¤ì¥ëª… í•„í„°ë§: ì œëª© OR ë¯¸ë¦¬ë³´ê¸°ì— ì •í™•í•œ ë§¤ì¥ëª… OR ì²« ë‹¨ì–´(ë„ì–´ì“°ê¸° ìˆì„ ë•Œë§Œ) í¬í•¨ë˜ì–´ì•¼ í•¨
                # ë¸”ë¡œê·¸ ì œëª©/ë¯¸ë¦¬ë³´ê¸°ì˜ ë„ì–´ì“°ê¸°ë„ ì œê±°í•˜ì—¬ ë¹„êµ (ë„ì–´ì“°ê¸° ë¬´ì‹œ)
                title_lower = title.lower().replace(" ", "")
                description_lower = description.lower().replace(" ", "")
                combined_text_lower = title_lower + description_lower
                
                # ë§¤ì¹­ ì¡°ê±´: ë§¤ì¥ëª… ì „ì²´ OR ì²« ë‹¨ì–´ (2ê°€ì§€ë§Œ)
                is_match = exact_store_lower in combined_text_lower
                if first_word_lower:  # ë„ì–´ì“°ê¸°ê°€ ìˆì„ ë•Œë§Œ ì²« ë‹¨ì–´ë„ í™•ì¸
                    is_match = is_match or (first_word_lower in combined_text_lower)
                
                if not is_match:
                    filtered_count += 1
                    if idx < 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
                        logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] ì œì™¸ #{idx+1}: '{title[:60]}' â†’ ì •ê·œí™”: '{combined_text_lower[:80]}'")
                    continue
                
                if idx < 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸
                    # ì–´ë–¤ ì¡°ê±´ìœ¼ë¡œ ë§¤ì¹­ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if exact_store_lower in combined_text_lower:
                        match_type = "ë§¤ì¥ëª…"
                    else:
                        match_type = "ì²«ë‹¨ì–´"
                    
                    # ì œëª© ë˜ëŠ” ë¯¸ë¦¬ë³´ê¸°ì—ì„œ ë§¤ì¹­ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    match_in_title = (exact_store_lower in title_lower) or (first_word_lower and first_word_lower in title_lower)
                    match_location = "ì œëª©" if match_in_title else "ë¯¸ë¦¬ë³´ê¸°"
                    
                    logger.info(f"[ë¸”ë¡œê·¸ íŒŒì‹±] âœ“ #{idx+1} ({match_type}/{match_location}): '{title[:60]}' â†’ ì •ê·œí™”: '{combined_text_lower[:80]}'")
                
                # ë§í¬ì˜ ë¶€ëª¨ ìš”ì†Œì—ì„œ ë‚ ì§œì™€ ì‘ì„±ì ì°¾ê¸°
                # ë¶€ëª¨ë¥¼ ì—¬ëŸ¬ ë‹¨ê³„ ì˜¬ë¼ê°€ë©´ì„œ ì°¾ê¸°
                date_str = None
                author = ""
                review_date = None
                
                # ë¶€ëª¨ ìš”ì†Œ íƒìƒ‰ (ìµœëŒ€ 5ë‹¨ê³„)
                parent = link.parent
                for level in range(5):
                    if not parent:
                        break
                    
                    # ë‚ ì§œ ì°¾ê¸°
                    if not date_str:
                        # ë‹¤ì–‘í•œ íŒ¨í„´ì˜ ë‚ ì§œ ìš”ì†Œ ì°¾ê¸°
                        date_candidates = parent.find_all(['span', 'time', 'div'], limit=20)
                        for candidate in date_candidates:
                            text = candidate.get_text(strip=True)
                            
                            # ë‚ ì§œëŠ” ë³´í†µ ì§§ìŒ (20ì ì´ë‚´)
                            if not text or len(text) > 20:
                                continue
                            
                            # ì—„ê²©í•œ ë‚ ì§œ íŒ¨í„´ í™•ì¸
                            is_date = False
                            
                            # "Nì¼ ì „", "Nì£¼ ì „", "Nì‹œê°„ ì „", "Në¶„ ì „" (ìˆ«ìê°€ ì•ì— ì™€ì•¼ í•¨)
                            if re.match(r'^\d+\s*(ì¼|ì£¼|ì‹œê°„|ë¶„)\s*ì „', text):
                                is_date = True
                            # "YYYY.MM.DD." ë˜ëŠ” "YY.MM.DD." í˜•ì‹
                            elif re.match(r'^\d{2,4}\.\d{1,2}\.\d{1,2}\.?', text):
                                is_date = True
                            
                            if is_date:
                                date_str = text
                                review_date = self._parse_naver_search_date(date_str)
                                if review_date:  # íŒŒì‹± ì„±ê³µí•œ ê²½ìš°ë§Œ
                                    break
                    
                    parent = parent.parent
                
                reviews.append({
                    "date": review_date.isoformat() if review_date else None,
                    "dateString": date_str,
                    "title": title,
                    "author": author,
                    "url": url
                })
            
            except Exception as e:
                if idx < 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸
                    logger.warning(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] ì•„ì´í…œ {idx} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        logger.info(f"[ë¸”ë¡œê·¸ íŒŒì‹±] ì™„ë£Œ: {len(reviews)}ê°œ (ì „ì²´: {len(blog_links)}ê°œ, í•„í„°ë§ ì œì™¸: {filtered_count}ê°œ)")
        
        return reviews
    
    def _parse_naver_search_date(self, date_str: str) -> Optional[datetime]:
        """
        ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚ ì§œ íŒŒì‹±
        
        í˜•ì‹:
        - "1ì¼ ì „", "2ì¼ ì „" â†’ ìƒëŒ€ ë‚ ì§œ
        - "1ì£¼ ì „", "2ì£¼ ì „" â†’ ìƒëŒ€ ì£¼
        - "1ì‹œê°„ ì „", "2ì‹œê°„ ì „" â†’ ìƒëŒ€ ì‹œê°„
        - "2025.12.05.", "2025.12.5." â†’ ì ˆëŒ€ ë‚ ì§œ
        - "25.12.05.", "25.12.5." â†’ ì ˆëŒ€ ë‚ ì§œ (20xxë…„ ê°€ì •)
        """
        if not date_str:
            return None
        
        KST = pytz.timezone('Asia/Seoul')
        now = datetime.now(KST)
        
        try:
            # "Nì¼ ì „"
            if "ì¼ ì „" in date_str or "ì¼ì „" in date_str:
                match = re.search(r'(\d+)ì¼\s*ì „', date_str)
                if match:
                    days = int(match.group(1))
                    return now - timedelta(days=days)
            
            # "Nì£¼ ì „"
            if "ì£¼ ì „" in date_str or "ì£¼ì „" in date_str:
                match = re.search(r'(\d+)ì£¼\s*ì „', date_str)
                if match:
                    weeks = int(match.group(1))
                    return now - timedelta(weeks=weeks)
            
            # "Nì‹œê°„ ì „"
            if "ì‹œê°„ ì „" in date_str or "ì‹œê°„ì „" in date_str:
                match = re.search(r'(\d+)ì‹œê°„\s*ì „', date_str)
                if match:
                    hours = int(match.group(1))
                    return now - timedelta(hours=hours)
            
            # "Në¶„ ì „"
            if "ë¶„ ì „" in date_str or "ë¶„ì „" in date_str:
                match = re.search(r'(\d+)ë¶„\s*ì „', date_str)
                if match:
                    minutes = int(match.group(1))
                    return now - timedelta(minutes=minutes)
            
            # "YYYY.MM.DD." ë˜ëŠ” "YYYY.M.D."
            match = re.match(r'(\d{4})\.(\d{1,2})\.(\d{1,2})\.?', date_str)
            if match:
                year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))
                return datetime(year, month, day, tzinfo=KST)
            
            # "YY.MM.DD." ë˜ëŠ” "YY.M.D." (20xxë…„ ê°€ì •)
            match = re.match(r'(\d{2})\.(\d{1,2})\.(\d{1,2})\.?', date_str)
            if match:
                year_short = int(match.group(1))
                year = 2000 + year_short if year_short < 50 else 1900 + year_short
                month, day = int(match.group(2)), int(match.group(3))
                return datetime(year, month, day, tzinfo=KST)
            
            logger.warning(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] ì•Œ ìˆ˜ ì—†ëŠ” ë‚ ì§œ í˜•ì‹: {date_str}")
            return None
        
        except Exception as e:
            logger.error(f"[ë¸”ë¡œê·¸ ê²€ìƒ‰] ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_str}, {str(e)}")
            return None
    
    async def _extract_place_id_from_blog_post(self, blog_url: str, place_id: str) -> bool:
        """
        ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì—ì„œ placeId ì¶”ì¶œí•˜ì—¬ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
        
        Args:
            blog_url: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ URL
            place_id: í™•ì¸í•  placeId
        
        Returns:
            bool: placeIdê°€ ì¼ì¹˜í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        try:
            import json
            from bs4 import BeautifulSoup
            
            # URLì„ PostView.naver í˜•ì‹ìœ¼ë¡œ ì§ì ‘ ë³€í™˜
            # https://blog.naver.com/username/postid â†’ https://blog.naver.com/PostView.naver?blogId=username&logNo=postid
            match = re.match(r'https://blog\.naver\.com/([^/]+)/(\d+)', blog_url)
            if not match:
                logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] URL íŒ¨í„´ ë¶ˆì¼ì¹˜: {blog_url}")
                return False
            
            username, post_id = match.groups()
            postview_url = f"https://blog.naver.com/PostView.naver?blogId={username}&logNo={post_id}"
            
            logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] PostView URL ë³€í™˜: {blog_url} â†’ {postview_url}")
            
            # í”„ë¡ì‹œ ì¡°ê±´ë¶€ ì„¤ì •
            client_kwargs = {"timeout": 5.0}
            proxy_url = get_proxy()
            if proxy_url:
                client_kwargs["proxy"] = proxy_url
            
            async with httpx.AsyncClient(**client_kwargs) as client:
                # PostView URLë¡œ ì§ì ‘ ì‹¤ì œ ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸°
                response = await client.get(postview_url, headers=self.headers, follow_redirects=True)
                
                if response.status_code != 200:
                    logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] HTTP {response.status_code}: {postview_url}")
                    return False
                
                frame_html = response.text
                frame_soup = BeautifulSoup(frame_html, 'html.parser')
                
                logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] HTML ê¸¸ì´: {len(frame_html)} bytes")
                
                # placeId ê²€ìƒ‰
                found_place_ids = []
                
                # ë°©ë²• 1: data-linkdata ì†ì„±ì—ì„œ placeId ì¶”ì¶œ
                map_links = frame_soup.find_all('a', attrs={'data-linkdata': True})
                logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] data-linkdata ë§í¬ {len(map_links)}ê°œ ë°œê²¬: {blog_url}")
                
                for link in map_links:
                    try:
                        link_data_str = link['data-linkdata'].replace('&quot;', '"')
                        link_data = json.loads(link_data_str)
                        post_place_id = str(link_data.get('placeId', ''))
                        if post_place_id:
                            found_place_ids.append(post_place_id)
                        if post_place_id == place_id:
                            logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] âœ… placeId ì¼ì¹˜ (data-linkdata): {blog_url}")
                            return True
                    except (json.JSONDecodeError, KeyError):
                        continue
                
                # ë°©ë²• 2: iframe srcì—ì„œ placeId ì¶”ì¶œ
                iframes = frame_soup.find_all('iframe', src=re.compile(r'place\.naver\.com'))
                logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] iframe {len(iframes)}ê°œ ë°œê²¬: {blog_url}")
                
                for iframe in iframes:
                    src = iframe.get('src', '')
                    if place_id in src:
                        logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] âœ… placeId ì¼ì¹˜ (iframe): {blog_url}")
                        return True
                
                # ë°©ë²• 3: ì§ì ‘ ë§í¬ì—ì„œ placeId í™•ì¸
                place_links = frame_soup.find_all('a', href=re.compile(rf'place\.naver\.com.*/place/{place_id}'))
                logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] place ë§í¬ {len(place_links)}ê°œ ë°œê²¬: {blog_url}")
                
                if place_links:
                    logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] âœ… placeId ì¼ì¹˜ (ë§í¬): {blog_url}")
                    return True
                
                logger.info(f"[ë¸”ë¡œê·¸ í•„í„°ë§] âŒ placeId ë¶ˆì¼ì¹˜ (ì°¾ì€ placeId: {found_place_ids[:3]}...): {blog_url}")
                return False
                
        except asyncio.TimeoutError:
            logger.debug(f"[ë¸”ë¡œê·¸ í•„í„°ë§] Timeout: {blog_url}")
            return False
        except Exception as e:
            logger.debug(f"[ë¸”ë¡œê·¸ í•„í„°ë§] ì˜ˆì™¸ {type(e).__name__}: {blog_url}")
            return False
    
    def _parse_blog_reviews_from_html(self, html: str) -> List[Dict[str, Any]]:
        """
        HTMLì—ì„œ ë¸”ë¡œê·¸ ë¦¬ë·° íŒŒì‹± (êµ¬ë²„ì „, í˜¸í™˜ì„± ìœ ì§€)
        
        Args:
            html: HTML ë¬¸ìì—´
        
        Returns:
            List[Dict]: íŒŒì‹±ëœ ë¦¬ë·° ëª©ë¡
        """
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(html, 'html.parser')
        reviews = []
        
        # listitem ìš”ì†Œ ì°¾ê¸° (ë¸”ë¡œê·¸ ë¦¬ë·° ì•„ì´í…œ)
        list_items = soup.find_all('li', role='listitem')
        logger.info(f"[ë¸”ë¡œê·¸ HTML] listitem ê°œìˆ˜: {len(list_items)}")
        
        for idx, item in enumerate(list_items):
            try:
                # time íƒœê·¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: "24.7.17.ìˆ˜")
                time_tag = item.find('time')
                if not time_tag:
                    if idx < 3:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê·¸
                        logger.debug(f"[ë¸”ë¡œê·¸ HTML] ì•„ì´í…œ {idx}: time íƒœê·¸ ì—†ìŒ")
                    continue
                
                date_str = time_tag.get_text(strip=True)
                if idx < 3:
                    logger.debug(f"[ë¸”ë¡œê·¸ HTML] ì•„ì´í…œ {idx}: date_str={date_str}")
                
                # ë‚ ì§œ íŒŒì‹± (YY.M.D.ìš”ì¼ í˜•ì‹)
                review_date = self._parse_blog_review_date_from_text(date_str)
                if not review_date:
                    if idx < 3:
                        logger.debug(f"[ë¸”ë¡œê·¸ HTML] ì•„ì´í…œ {idx}: ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨")
                    continue
                
                # ì œëª© ì¶”ì¶œ (ì²« ë²ˆì§¸ generic íƒœê·¸)
                title_elem = item.select_one('div > div')
                title = title_elem.get_text(strip=True) if title_elem else ""
                
                # ì‘ì„±ì ì¶”ì¶œ
                author_elem = item.select_one('img[alt="í”„ë¡œí•„"] + div > div:first-child')
                author = author_elem.get_text(strip=True) if author_elem else ""
                
                if idx < 3:
                    logger.debug(f"[ë¸”ë¡œê·¸ HTML] ì•„ì´í…œ {idx}: title={title[:30]}..., author={author}")
                
                reviews.append({
                    "date": review_date.isoformat(),
                    "dateString": date_str,
                    "title": title,
                    "author": author
                })
                
            except Exception as e:
                if idx < 3:
                    logger.warning(f"[ë¸”ë¡œê·¸ HTML] ì•„ì´í…œ {idx} íŒŒì‹± ì˜ˆì™¸: {e}")
                continue
        
        logger.info(f"[ë¸”ë¡œê·¸ HTML] ìµœì¢… íŒŒì‹± ê²°ê³¼: {len(reviews)}ê°œ")
        return reviews
    
    def _extract_blog_reviews_from_apollo_state(self, html: str, place_id: str) -> List[Dict[str, Any]]:
        """
        HTMLì—ì„œ __APOLLO_STATE__ ì¶”ì¶œí•˜ì—¬ ë¸”ë¡œê·¸ ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
        
        Args:
            html: HTML ë¬¸ìì—´
            place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID
        
        Returns:
            List[Dict]: ë¸”ë¡œê·¸ ë¦¬ë·° ëª©ë¡
        """
        try:
            # __APOLLO_STATE__ ì°¾ê¸°
            match = re.search(r'window\.__APOLLO_STATE__\s*=\s*({.+?});', html, re.DOTALL)
            if not match:
                logger.debug(f"[ë¸”ë¡œê·¸ HTML-Fast] __APOLLO_STATE__ ì—†ìŒ")
                return []
            
            apollo_state = json.loads(match.group(1))
            reviews = []
            
            # fsasReviews í‚¤ ì°¾ê¸°
            for key, value in apollo_state.items():
                if 'fsasReviews' in key and isinstance(value, dict):
                    items = value.get('items', [])
                    if isinstance(items, list):
                        for item in items:
                            if isinstance(item, dict):
                                # ë‚ ì§œ íŒŒì‹±
                                date_str = item.get('date') or item.get('createdString', '')
                                if date_str:
                                    review_date = self._parse_blog_review_date_from_text(date_str)
                                    if review_date:
                                        reviews.append({
                                            'date': review_date.isoformat(),
                                            'dateString': date_str,
                                            'title': item.get('title', ''),
                                            'author': item.get('authorName', '')
                                        })
            
            logger.info(f"[ë¸”ë¡œê·¸ HTML-Fast] Apollo Stateì—ì„œ {len(reviews)}ê°œ ì¶”ì¶œ")
            return reviews
            
        except Exception as e:
            logger.warning(f"[ë¸”ë¡œê·¸ HTML-Fast] Apollo State íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_blog_review_date_from_text(self, date_str: str) -> Optional[datetime]:
        """
        ë¸”ë¡œê·¸ ë¦¬ë·° ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±
        
        ì§€ì› í˜•ì‹:
        - "24.7.17.ìˆ˜", "26.1.1.ëª©" (YY.M.D.ìš”ì¼)
        - "2025.09.30." (YYYY.MM.DD.)
        - ISO í˜•ì‹
        
        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´
        
        Returns:
            datetime ê°ì²´ ë˜ëŠ” None
        """
        try:
            # 1. ISO í˜•ì‹ ì‹œë„
            if 'T' in date_str or '-' in date_str:
                try:
                    dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                    return dt.replace(tzinfo=None)  # naive datetimeìœ¼ë¡œ ë³€í™˜
                except:
                    pass
            
            # 2. YYYY.MM.DD. í˜•ì‹ (ì˜ˆ: "2025.09.30.")
            match = re.match(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_str)
            if match:
                year, month, day = match.groups()
                return datetime(int(year), int(month), int(day))
            
            # 3. YY.M.D.ìš”ì¼ í˜•ì‹ (ì˜ˆ: "24.7.17.ìˆ˜")
            match = re.match(r'(\d{2})\.(\d{1,2})\.(\d{1,2})', date_str)
            if match:
                year_short, month, day = match.groups()
                year = 2000 + int(year_short)
                return datetime(year, int(month), int(day))
            
            return None
            
        except Exception as e:
            logger.debug(f"[ë¸”ë¡œê·¸ HTML-Fast] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}, {e}")
            return None


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
naver_review_service = NaverReviewService()
