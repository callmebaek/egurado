"""
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ê´€ë ¨ API ë¼ìš°í„°
(ê²½ìŸë§¤ì¥ ë¶„ì„ í¬í•¨)
"""
from fastapi import APIRouter, HTTPException, status, Query, Depends, Header
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
from uuid import UUID
import logging

from app.services.naver_auth import store_naver_cookies
from app.services.naver_search_new import search_service_new as search_service
from app.services.naver_search_api import api_search_service
from app.services.naver_rank_service import rank_service
# ë¹„ê³µì‹ API ì„œë¹„ìŠ¤ (ìƒˆë¡œ ì¶”ê°€)
from app.services.naver_search_api_unofficial import search_service_api_unofficial
from app.services.naver_rank_api_unofficial import rank_service_api_unofficial
from app.services.naver_keywords_analyzer import keywords_analyzer_service
from app.services.naver_competitor_analysis_service import competitor_analysis_service
from app.core.database import get_supabase_client
from app.routers.auth import get_current_user
from datetime import datetime, date
from app.services.credit_service import credit_service
from app.core.config import settings

security = HTTPBearer(auto_error=False)

router = APIRouter()
logger = logging.getLogger(__name__)

# êµ¬ë… tierë³„ í‚¤ì›Œë“œ ë“±ë¡ ì œí•œ
KEYWORD_LIMITS = {
    "free": 1,
    "basic": 10,
    "pro": 50,
    "god": 9999  # God tier: ë¬´ì œí•œ (ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥)
}

def check_keyword_limit(supabase, user_id: str) -> tuple[bool, int, int]:
    """
    ì‚¬ìš©ìì˜ í‚¤ì›Œë“œ ë“±ë¡ ì œí•œì„ í™•ì¸í•©ë‹ˆë‹¤.
    
    Returns:
        tuple: (ì œí•œ ì´ˆê³¼ ì—¬ë¶€, í˜„ì¬ í‚¤ì›Œë“œ ìˆ˜, ìµœëŒ€ í—ˆìš© ìˆ˜)
    """
    try:
        # ì‚¬ìš©ìì˜ êµ¬ë… tier í™•ì¸
        user_result = supabase.table("profiles").select("subscription_tier").eq("id", user_id).single().execute()
        
        if not user_result.data:
            logger.warning(f"User not found: {user_id}")
            return False, 0, KEYWORD_LIMITS["free"]
        
        subscription_tier = user_result.data.get("subscription_tier", "free").lower()
        max_keywords = KEYWORD_LIMITS.get(subscription_tier, KEYWORD_LIMITS["free"])
        
        # í•´ë‹¹ ì‚¬ìš©ìì˜ ëª¨ë“  ë§¤ì¥ì— ë“±ë¡ëœ í‚¤ì›Œë“œ ìˆ˜ í™•ì¸
        stores_result = supabase.table("stores").select("id").eq("user_id", user_id).execute()
        
        if not stores_result.data:
            return False, 0, max_keywords
        
        store_ids = [store["id"] for store in stores_result.data]
        
        # ëª¨ë“  ë§¤ì¥ì˜ í‚¤ì›Œë“œ ìˆ˜ í•©ì‚°
        keywords_result = supabase.table("keywords").select("id", count="exact").in_("store_id", store_ids).execute()
        current_keywords = keywords_result.count if keywords_result.count else 0
        
        logger.info(
            f"[Keyword Limit Check] User: {user_id}, Tier: {subscription_tier}, "
            f"Current: {current_keywords}, Max: {max_keywords}"
        )
        
        return current_keywords >= max_keywords, current_keywords, max_keywords
        
    except Exception as e:
        logger.error(f"Error checking keyword limit: {str(e)}")
        return False, 0, KEYWORD_LIMITS["free"]


class StoreSearchResult(BaseModel):
    """ë§¤ì¥ ê²€ìƒ‰ ê²°ê³¼"""
    place_id: str
    name: str
    category: str
    address: str
    road_address: Optional[str] = ""
    thumbnail: Optional[str] = ""


class StoreSearchResponse(BaseModel):
    """ë§¤ì¥ ê²€ìƒ‰ ì‘ë‹µ"""
    status: str
    query: str
    results: List[StoreSearchResult]
    total_count: int


class NaverConnectionRequest(BaseModel):
    """ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì—°ê²° ìš”ì²­"""
    user_id: UUID
    store_id: UUID
    cookies: List[Dict]


class NaverConnectionResponse(BaseModel):
    """ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì—°ê²° ì‘ë‹µ"""
    status: str
    message: str
    store_id: UUID


class RankCheckRequest(BaseModel):
    """ìˆœìœ„ ì¡°íšŒ ìš”ì²­"""
    store_id: UUID
    keyword: str


class RankCheckResponse(BaseModel):
    """ìˆœìœ„ ì¡°íšŒ ì‘ë‹µ"""
    status: str
    keyword: str
    place_id: str
    store_name: str
    rank: Optional[int] = None
    found: bool
    total_results: int
    total_count: Optional[str] = None  # ì „ì²´ ì—…ì²´ ìˆ˜ (ì˜ˆ: "1,234")
    previous_rank: Optional[int] = None
    rank_change: Optional[int] = None  # ìˆœìœ„ ë³€ë™ (ì–‘ìˆ˜: ìƒìŠ¹, ìŒìˆ˜: í•˜ë½)
    last_checked_at: datetime
    search_results: List[Dict]  # ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ (ìˆœìœ„ ë‚´ ë§¤ì¥ë“¤)


class KeywordListResponse(BaseModel):
    """ë§¤ì¥ì˜ í‚¤ì›Œë“œ ëª©ë¡ ì‘ë‹µ"""
    status: str
    store_id: UUID
    keywords: List[Dict]
    total_count: int


@router.get("/search-stores-test")
async def search_naver_stores_test(query: str = "ì¹´í˜"):
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        results = await search_service.search_stores(query)
        return {"status": "success", "count": len(results), "results": results}
    except Exception as e:
        import traceback
        return {"status": "error", "error": str(e), "type": type(e).__name__, "trace": traceback.format_exc()}


@router.get("/search-stores", response_model=StoreSearchResponse)
async def search_naver_stores(
    query: str = Query(..., min_length=1, description="ê²€ìƒ‰í•  ë§¤ì¥ëª…")
):
    """
    ë„¤ì´ë²„ ëª¨ë°”ì¼ ì§€ë„ì—ì„œ ë§¤ì¥ ê²€ìƒ‰ (í¬ë¡¤ë§ ë°©ì‹)
    
    Args:
        query: ê²€ìƒ‰í•  ë§¤ì¥ëª… (í•„ìˆ˜)
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 10ê°œ)
        
    Raises:
        HTTPException: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
    """
    try:
        logger.info(f"[Crawling] Searching for stores: {query}")
        
        # ë„¤ì´ë²„ ëª¨ë°”ì¼ ì§€ë„ì—ì„œ ê²€ìƒ‰
        results = await search_service.search_stores(query)
        
        # ì‘ë‹µ ë³€í™˜
        search_results = [
            StoreSearchResult(
                place_id=store["place_id"],
                name=store["name"],
                category=store["category"],
                address=store["address"],
                road_address=store.get("road_address", ""),
                thumbnail=store.get("thumbnail", "")
            )
            for store in results
        ]
        
        logger.info(f"[Crawling] Found {len(search_results)} stores for query: {query}")
        
        return StoreSearchResponse(
            status="success",
            query=query,
            results=search_results,
            total_count=len(search_results)
        )
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[Crawling] Error searching stores: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë§¤ì¥ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}: {str(e)}"
        )


@router.get("/search-stores-api", response_model=StoreSearchResponse)
async def search_naver_stores_api(
    query: str = Query(..., min_length=1, description="ê²€ìƒ‰í•  ë§¤ì¥ëª…")
):
    """
    ë„¤ì´ë²„ ë¡œì»¬ ê²€ìƒ‰ APIë¡œ ë§¤ì¥ ê²€ìƒ‰ (API ë°©ì‹ - í…ŒìŠ¤íŠ¸)
    
    - í¬ë¡¤ë§ë³´ë‹¤ ë¹ ë¥´ê³  ì•ˆì •ì  (ì•½ 2-6ë°° ë¹ ë¦„)
    - API í‚¤ í•„ìš” (í™˜ê²½ ë³€ìˆ˜: NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)
    - ì¼ì¼ 25,000ê±´ ì œí•œ
    - ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì—†ìŒ
    
    Args:
        query: ê²€ìƒ‰í•  ë§¤ì¥ëª… (í•„ìˆ˜)
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ)
        
    Raises:
        HTTPException: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
    """
    try:
        logger.info(f"[API] Searching for stores: {query}")
        results = await api_search_service.search_stores(query)
        
        search_results = [
            StoreSearchResult(
                place_id=store["place_id"],
                name=store["name"],
                category=store["category"],
                address=store["address"],
                road_address=store.get("road_address", ""),
                thumbnail=store.get("thumbnail", "")
            )
            for store in results
        ]
        
        logger.info(f"[API] Found {len(search_results)} stores for query: {query}")
        
        return StoreSearchResponse(
            status="success",
            query=query,
            results=search_results,
            total_count=len(search_results)
        )
    except ValueError as e:
        logger.error(f"[API] Configuration error: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë„¤ì´ë²„ API ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[API] Error searching stores: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë§¤ì¥ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}: {str(e)}"
        )


@router.post("/connect", response_model=NaverConnectionResponse)
async def connect_naver_store(request: NaverConnectionRequest):
    """
    ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì—°ê²° (ì„¸ì…˜ ì¿ í‚¤ ì €ì¥)
    
    ì‚¬ìš©ìê°€ ë¡œì»¬ ë¸Œë¼ìš°ì €ì—ì„œ ë„¤ì´ë²„ ë¡œê·¸ì¸ í›„ ì¶”ì¶œí•œ ì¿ í‚¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    try:
        # ë§¤ì¥ ì¡´ì¬ í™•ì¸
        supabase = get_supabase_client()
        store_check = supabase.table("stores").select("id, platform").eq(
            "id", str(request.store_id)
        ).eq("user_id", str(request.user_id)).single().execute()
        
        if not store_check.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."
            )
        
        if store_check.data["platform"] != "naver":
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë§¤ì¥ì´ ì•„ë‹™ë‹ˆë‹¤."
            )
        
        # ì¿ í‚¤ ì €ì¥
        success = await store_naver_cookies(
            str(request.user_id),
            str(request.store_id),
            request.cookies
        )
        
        if not success:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="ë„¤ì´ë²„ ì„¸ì…˜ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            )
        
        return NaverConnectionResponse(
            status="success",
            message="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.",
            store_id=request.store_id
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/stores/{store_id}/status")
async def check_naver_store_status(store_id: UUID):
    """
    ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸
    """
    try:
        supabase = get_supabase_client()
        result = supabase.table("stores").select("status, last_synced_at, platform").eq(
            "id", str(store_id)
        ).single().execute()
        
        if not result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        return {
            "store_id": store_id,
            "status": result.data["status"],
            "platform": result.data["platform"],
            "last_synced_at": result.data["last_synced_at"]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )


@router.post("/stores/{store_id}/sync-reviews")
async def sync_naver_reviews(store_id: UUID):
    """
    ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë¦¬ë·° ìˆ˜ì§‘
    
    ë„¤íŠ¸ì›Œí¬ ì¸í„°ì…‰ì…˜ì„ í†µí•´ ë¦¬ë·° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    try:
        from app.services.naver_crawler import crawl_naver_reviews
        
        # ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        supabase = get_supabase_client()
        store = supabase.table("stores").select("place_id, platform").eq(
            "id", str(store_id)
        ).single().execute()
        
        if not store.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        if store.data["platform"] != "naver":
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë§¤ì¥ì´ ì•„ë‹™ë‹ˆë‹¤."
            )
        
        place_id = store.data["place_id"]
        
        # ë¦¬ë·° í¬ë¡¤ë§
        reviews = await crawl_naver_reviews(str(store_id), place_id)
        
        return {
            "status": "success",
            "message": f"{len(reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.",
            "store_id": store_id,
            "review_count": len(reviews)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¦¬ë·° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


@router.post("/check-rank", response_model=RankCheckResponse)
async def check_place_rank(request: RankCheckRequest):
    """
    ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ í‚¤ì›Œë“œ ìˆœìœ„ ì¡°íšŒ
    
    íŠ¹ì • í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ ë§¤ì¥ì˜ ìˆœìœ„ë¥¼ í™•ì¸í•˜ê³  DBì— ì €ì¥í•©ë‹ˆë‹¤.
    - ìµœì´ˆ ì¡°íšŒ ì‹œ: keywords í…Œì´ë¸”ì— INSERT
    - ì¬ì¡°íšŒ ì‹œ: keywords í…Œì´ë¸” UPDATE, rank_historyì— ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„° UPSERT
    
    ì†ë„ ìµœì í™”:
    - ìµœëŒ€ 40ê°œê¹Œì§€ë§Œ í™•ì¸ (ìŠ¤í¬ë¡¤ 2íšŒ)
    - íƒ€ê²Ÿ ë§¤ì¥ ë°œê²¬ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
    """
    try:
        supabase = get_supabase_client()
        
        # ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        store_result = supabase.table("stores").select(
            "id, place_id, store_name, platform, user_id"
        ).eq("id", str(request.store_id)).single().execute()
        
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        if store_result.data["platform"] != "naver":
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë§¤ì¥ì´ ì•„ë‹™ë‹ˆë‹¤."
            )
        
        store_data = store_result.data
        place_id = store_data["place_id"]
        store_name = store_data["store_name"]
        
        logger.info(
            f"[Rank Check] Store: {store_name} (ID: {place_id}), "
            f"Keyword: {request.keyword}"
        )
        
        # ìˆœìœ„ ì²´í¬ (í¬ë¡¤ë§) - ìµœëŒ€ 300ê°œê¹Œì§€ í™•ì¸
        rank_result = await rank_service.check_rank(
            keyword=request.keyword,
            target_place_id=place_id,
            max_results=300
        )
        
        # ê¸°ì¡´ í‚¤ì›Œë“œ í™•ì¸
        keyword_check = supabase.table("keywords").select(
            "id, current_rank, previous_rank"
        ).eq("store_id", str(request.store_id)).eq(
            "keyword", request.keyword
        ).execute()
        
        now = datetime.utcnow()
        today = date.today()
        
        keyword_id = None
        previous_rank = None
        
        if keyword_check.data and len(keyword_check.data) > 0:
            # ê¸°ì¡´ í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸
            existing_keyword = keyword_check.data[0]
            keyword_id = existing_keyword["id"]
            previous_rank = existing_keyword["current_rank"]
            
            # keywords í…Œì´ë¸” ì—…ë°ì´íŠ¸
            # total_count ì²˜ë¦¬: ì •ìˆ˜ ë˜ëŠ” ë¬¸ìì—´ "1,234" â†’ 1234 ë³€í™˜
            total_count_value = rank_result.get("total_count")
            total_results = 0
            
            if total_count_value is not None:
                if isinstance(total_count_value, str):
                    # ë¬¸ìì—´ "1,234" â†’ 1234
                    total_results = int(total_count_value.replace(",", "")) if total_count_value.strip() else 0
                elif isinstance(total_count_value, int):
                    # ì •ìˆ˜ ê·¸ëŒ€ë¡œ
                    total_results = total_count_value
            
            logger.info(f"[Rank Check] UPDATE - total_count: {total_count_value}, total_results: {total_results}")
            
            supabase.table("keywords").update({
                "previous_rank": previous_rank,
                "current_rank": rank_result["rank"],
                "total_results": total_results,
                "last_checked_at": now.isoformat()
            }).eq("id", keyword_id).execute()
            
            logger.info(
                f"[Rank Check] Updated existing keyword (ID: {keyword_id}), "
                f"Rank: {previous_rank} â†’ {rank_result['rank']}"
            )
            
        else:
            # ìƒˆ í‚¤ì›Œë“œ ë“±ë¡ ì „ì— ì œí•œ í™•ì¸
            # storeë¥¼ í†µí•´ user_id ê°€ì ¸ì˜¤ê¸°
            user_id = store_data.get("user_id")
            if not user_id:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="ë§¤ì¥ì— ì—°ê²°ëœ ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            is_limit_exceeded, current_count, max_count = check_keyword_limit(supabase, user_id)
            
            if is_limit_exceeded:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail=f"í‚¤ì›Œë“œ ë“±ë¡ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. (í˜„ì¬: {current_count}/{max_count}ê°œ) êµ¬ë… í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”."
                )
            
            # ìƒˆ í‚¤ì›Œë“œ ë“±ë¡
            # total_count ì²˜ë¦¬: ì •ìˆ˜ ë˜ëŠ” ë¬¸ìì—´ "1,234" â†’ 1234 ë³€í™˜
            total_count_value = rank_result.get("total_count")
            total_results = 0
            
            if total_count_value is not None:
                if isinstance(total_count_value, str):
                    # ë¬¸ìì—´ "1,234" â†’ 1234
                    total_results = int(total_count_value.replace(",", "")) if total_count_value.strip() else 0
                elif isinstance(total_count_value, int):
                    # ì •ìˆ˜ ê·¸ëŒ€ë¡œ
                    total_results = total_count_value
            
            logger.info(f"[Rank Check] NEW KEYWORD - total_count: {total_count_value}, total_results: {total_results}")
            
            keyword_insert = supabase.table("keywords").insert({
                "store_id": str(request.store_id),
                "keyword": request.keyword,
                "current_rank": rank_result["rank"],
                "previous_rank": None,
                "total_results": total_results,
                "last_checked_at": now.isoformat()
            }).execute()
            
            keyword_id = keyword_insert.data[0]["id"]
            
            logger.info(
                f"[Rank Check] Created new keyword (ID: {keyword_id}), "
                f"Rank: {rank_result['rank']}, User keywords: {current_count + 1}/{max_count}"
            )
        
        # rank_history ì²˜ë¦¬ (ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„°ë§Œ ìœ ì§€)
        # 1. ì˜¤ëŠ˜ ë‚ ì§œì˜ ê¸°ì¡´ ê¸°ë¡ ì‚­ì œ
        supabase.table("rank_history").delete().eq(
            "keyword_id", keyword_id
        ).gte(
            "checked_at", today.isoformat()
        ).lt(
            "checked_at", (today.replace(day=today.day + 1)).isoformat() if today.day < 28 else today.isoformat()
        ).execute()
        
        # 2. ìƒˆë¡œìš´ ê¸°ë¡ ì¶”ê°€
        supabase.table("rank_history").insert({
            "keyword_id": keyword_id,
            "rank": rank_result["rank"],
            "checked_at": now.isoformat()
        }).execute()
        
        logger.info(f"[Rank Check] Saved rank history for today")
        
        # ìˆœìœ„ ë³€ë™ ê³„ì‚°
        rank_change = None
        if previous_rank and rank_result["rank"]:
            # ìˆœìœ„ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (1ìœ„ê°€ ìµœê³ )
            rank_change = previous_rank - rank_result["rank"]  # ì–‘ìˆ˜: ìˆœìœ„ ìƒìŠ¹
        
        return RankCheckResponse(
            status="success",
            keyword=request.keyword,
            place_id=place_id,
            store_name=store_name,
            rank=rank_result["rank"],
            found=rank_result["found"],
            total_results=rank_result["total_results"],
            total_count=rank_result.get("total_count"),  # ì „ì²´ ì—…ì²´ ìˆ˜
            previous_rank=previous_rank,
            rank_change=rank_change,
            last_checked_at=now,
            search_results=rank_result["search_results"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[Rank Check] Error: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìˆœìœ„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/stores/{store_id}/keywords", response_model=KeywordListResponse)
async def get_store_keywords(store_id: UUID):
    """
    ë§¤ì¥ì˜ í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ
    
    ë“±ë¡ëœ í‚¤ì›Œë“œì™€ í˜„ì¬ ìˆœìœ„, ì´ì „ ìˆœìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        supabase = get_supabase_client()
        
        # ë§¤ì¥ ì¡´ì¬ í™•ì¸
        store_check = supabase.table("stores").select("id").eq(
            "id", str(store_id)
        ).single().execute()
        
        if not store_check.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # í‚¤ì›Œë“œ ì¡°íšŒ (ìµœê·¼ 30ê°œë§Œ) with is_tracked ì •ë³´
        keywords_result = supabase.table("keywords").select(
            "id, keyword, current_rank, previous_rank, total_results, last_checked_at, created_at"
        ).eq("store_id", str(store_id)).order(
            "last_checked_at", desc=True
        ).limit(30).execute()
        
        # ì¶”ì  ì¤‘ì¸ í‚¤ì›Œë“œ ID ëª©ë¡ ì¡°íšŒ
        trackers_result = supabase.table("metric_trackers").select(
            "keyword_id"
        ).eq("store_id", str(store_id)).execute()
        
        tracked_keyword_ids = set()
        for tracker in trackers_result.data:
            if tracker.get("keyword_id"):
                tracked_keyword_ids.add(tracker["keyword_id"])
        
        keywords = []
        for kw in keywords_result.data:
            rank_change = None
            if kw["previous_rank"] and kw["current_rank"]:
                rank_change = kw["previous_rank"] - kw["current_rank"]
            
            # keywords í…Œì´ë¸”ì˜ idë¥¼ ì‚¬ìš©í•˜ì—¬ is_tracked í™•ì¸
            is_tracked = kw["id"] in tracked_keyword_ids
            
            keywords.append({
                "id": kw["id"],
                "keyword": kw["keyword"],
                "current_rank": kw["current_rank"],
                "previous_rank": kw["previous_rank"],
                "rank_change": rank_change,
                "total_results": kw.get("total_results", 0),
                "is_tracked": is_tracked,
                "last_checked_at": kw["last_checked_at"],
                "created_at": kw["created_at"]
            })
        
        return KeywordListResponse(
            status="success",
            store_id=store_id,
            keywords=keywords,
            total_count=len(keywords)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching keywords: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í‚¤ì›Œë“œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/keywords/{keyword_id}/history")
async def get_keyword_rank_history(keyword_id: UUID):
    """
    í‚¤ì›Œë“œ ìˆœìœ„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    
    ë‚ ì§œë³„ ìˆœìœ„ ë³€í™”ë¥¼ ì¡°íšŒí•˜ì—¬ ì°¨íŠ¸ë¡œ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        supabase = get_supabase_client()
        
        # í‚¤ì›Œë“œ ì •ë³´ ì¡°íšŒ
        keyword_check = supabase.table("keywords").select(
            "id, keyword, store_id"
        ).eq("id", str(keyword_id)).single().execute()
        
        if not keyword_check.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        keyword_data = keyword_check.data
        
        # ìˆœìœ„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ë‚ ì§œìˆœ ì •ë ¬)
        history_result = supabase.table("rank_history").select(
            "id, rank, checked_at"
        ).eq("keyword_id", str(keyword_id)).order(
            "checked_at", desc=False  # ì˜¤ë˜ëœ ê²ƒë¶€í„°
        ).execute()
        
        history = []
        for record in history_result.data:
            history.append({
                "date": record["checked_at"],
                "rank": record["rank"],
                "checked_at": record["checked_at"]
            })
        
        return {
            "status": "success",
            "keyword_id": str(keyword_id),
            "keyword": keyword_data["keyword"],
            "store_id": keyword_data["store_id"],
            "history": history,
            "total_records": len(history)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Get Keyword History] Error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìˆœìœ„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.delete("/keywords/{keyword_id}")
async def delete_keyword(keyword_id: UUID, current_user: dict = Depends(get_current_user)):
    """
    í‚¤ì›Œë“œ ì‚­ì œ (Stored Procedure ì‚¬ìš©, RLS ìš°íšŒ)
    
    âš ï¸ ê²½ê³ : ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    - í‚¤ì›Œë“œ ì •ë³´ê°€ ì˜êµ¬ì ìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤.
    - ê³¼ê±° ìˆœìœ„ ê¸°ë¡(rank_history)ë„ ëª¨ë‘ ì‚­ì œë©ë‹ˆë‹¤.
    - ì‚­ì œëœ ë°ì´í„°ëŠ” ë³µêµ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    """
    try:
        supabase = get_supabase_client()
        
        logger.info(f"[Delete Keyword] Attempting to delete keyword ID: {keyword_id}")
        
        # Stored procedureë¥¼ í˜¸ì¶œí•˜ì—¬ cascade delete ìˆ˜í–‰ (RLS ìš°íšŒ)
        result = supabase.rpc(
            'delete_keyword_cascade',
            {'p_keyword_id': str(keyword_id)}
        ).execute()
        
        logger.info(f"[Delete Keyword] RPC result: {result.data}")
        
        # ê²°ê³¼ íŒŒì‹±
        if result.data:
            response_data = result.data
            
            if response_data.get('status') == 'error':
                logger.error(f"[Delete Keyword] RPC returned error: {response_data.get('message')}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=response_data.get('message', 'í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            )
        
        # ì„±ê³µ
        keyword_name = response_data.get('keyword', 'Unknown')
        deleted_trackers = response_data.get('deleted_trackers', 0)
        deleted_history = response_data.get('deleted_history', 0)
        deleted_keywords = response_data.get('deleted_keywords', 0)
        
        logger.info(
            f"[Delete Keyword] Successfully deleted: "
            f"keyword='{keyword_name}', "
            f"trackers={deleted_trackers}, "
            f"history={deleted_history}, "
            f"keywords={deleted_keywords}"
        )
        
        return {
            "status": "success",
            "message": f"í‚¤ì›Œë“œ '{keyword_name}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "keyword_id": str(keyword_id),
            "keyword": keyword_name
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Delete Keyword] Error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í‚¤ì›Œë“œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ============================================
# ë¹„ê³µì‹ API ë°©ì‹ ì—”ë“œí¬ì¸íŠ¸ (ìƒˆë¡œ ì¶”ê°€)
# ============================================

@router.get("/search-stores-unofficial", response_model=StoreSearchResponse)
async def search_naver_stores_unofficial(
    query: str = Query(..., min_length=1, description="ê²€ìƒ‰í•  ë§¤ì¥ëª…")
):
    """
    ë„¤ì´ë²„ ëª¨ë°”ì¼ ì§€ë„ì—ì„œ ë§¤ì¥ ê²€ìƒ‰ (ë¹„ê³µì‹ API ë°©ì‹)
    
    âš ï¸ ê²½ê³ : ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë„¤ì´ë²„ì˜ ë¹„ê³µì‹ APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
             êµìœ¡ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    
    ì¥ì :
    - í¬ë¡¤ë§ë³´ë‹¤ 2-3ë°° ë¹ ë¦„
    - ë” ì•ˆì •ì 
    - ë¦¬ë·°ìˆ˜, ì €ì¥ìˆ˜ ë“± ì¶”ê°€ ë°ì´í„° ì œê³µ
    
    Args:
        query: ê²€ìƒ‰í•  ë§¤ì¥ëª… (í•„ìˆ˜)
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 100ê°œ)
        
    Raises:
        HTTPException: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
    """
    try:
        logger.info(f"[Unofficial API] Searching for stores: {query}")
        
        # ë¹„ê³µì‹ APIë¡œ ê²€ìƒ‰
        results = await search_service_api_unofficial.search_stores(query)
        
        # ì‘ë‹µ ë³€í™˜
        search_results = [
            StoreSearchResult(
                place_id=store["place_id"],
                name=store["name"],
                category=store["category"],
                address=store["address"],
                road_address=store.get("road_address", ""),
                thumbnail=store.get("thumbnail", "")
            )
            for store in results
        ]
        
        logger.info(f"[Unofficial API] Found {len(search_results)} stores for query: {query}")
        
        return StoreSearchResponse(
            status="success",
            query=query,
            results=search_results,
            total_count=len(search_results)
        )
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[Unofficial API] Error searching stores: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë§¤ì¥ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {type(e).__name__}: {str(e)}"
        )


class RankCheckResponseUnofficial(BaseModel):
    """ìˆœìœ„ ì¡°íšŒ ì‘ë‹µ (ë¹„ê³µì‹ API - ë¦¬ë·°ìˆ˜ í¬í•¨)"""
    status: str
    keyword: str
    place_id: str
    store_name: str
    rank: Optional[int] = None
    found: bool
    total_results: int
    total_count: Optional[int] = None  # ì „ì²´ ì—…ì²´ ìˆ˜
    previous_rank: Optional[int] = None
    rank_change: Optional[int] = None  # ìˆœìœ„ ë³€ë™
    last_checked_at: datetime
    search_results: List[Dict]
    # ë¦¬ë·°ìˆ˜ ì •ë³´ ì¶”ê°€ â­
    visitor_review_count: int  # ë°©ë¬¸ì ë¦¬ë·° ìˆ˜
    blog_review_count: int  # ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜
    save_count: int  # ì €ì¥ ìˆ˜


@router.post("/check-rank-unofficial", response_model=RankCheckResponseUnofficial)
async def check_place_rank_unofficial(request: RankCheckRequest):
    """
    ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ í‚¤ì›Œë“œ ìˆœìœ„ ì¡°íšŒ (ë¹„ê³µì‹ API ë°©ì‹)
    
    âš ï¸ ê²½ê³ : ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë„¤ì´ë²„ì˜ ë¹„ê³µì‹ APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
             êµìœ¡ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    
    ì¥ì :
    - í¬ë¡¤ë§ë³´ë‹¤ 5-10ë°° ë¹ ë¦„ (ì•½ 2-3ì´ˆ)
    - ë” ì•ˆì •ì ì¸ ì‘ë‹µ
    - ë°©ë¬¸ì ë¦¬ë·°ìˆ˜, ë¸”ë¡œê·¸ ë¦¬ë·°ìˆ˜, ì €ì¥ìˆ˜ ì¶”ê°€ ì œê³µ â­
    
    íŠ¹ì • í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ ë§¤ì¥ì˜ ìˆœìœ„ë¥¼ í™•ì¸í•˜ê³  DBì— ì €ì¥í•©ë‹ˆë‹¤.
    - ìµœì´ˆ ì¡°íšŒ ì‹œ: keywords í…Œì´ë¸”ì— INSERT
    - ì¬ì¡°íšŒ ì‹œ: keywords í…Œì´ë¸” UPDATE, rank_historyì— ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„° UPSERT
    
    ì†ë„ ìµœì í™”:
    - ìµœëŒ€ 300ê°œê¹Œì§€ í™•ì¸
    - íƒ€ê²Ÿ ë§¤ì¥ ë°œê²¬ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨ ê°€ëŠ¥
    """
    try:
        supabase = get_supabase_client()
        
        # ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        store_result = supabase.table("stores").select(
            "id, place_id, store_name, platform, user_id"
        ).eq("id", str(request.store_id)).single().execute()
        
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        if store_result.data["platform"] != "naver":
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë§¤ì¥ì´ ì•„ë‹™ë‹ˆë‹¤."
            )
        
        store_data = store_result.data
        place_id = store_data["place_id"]
        store_name = store_data["store_name"]
        
        logger.info(
            f"[Unofficial API Rank] Store: {store_name} (ID: {place_id}), "
            f"Keyword: {request.keyword}"
        )
        
        # ìˆœìœ„ ì²´í¬ (ë¹„ê³µì‹ API) - ìµœëŒ€ 300ê°œê¹Œì§€ í™•ì¸
        rank_result = await rank_service_api_unofficial.check_rank(
            keyword=request.keyword,
            target_place_id=place_id,
            max_results=300
        )
        
        # íƒ€ê²Ÿ ë§¤ì¥ì˜ ë¦¬ë·°ìˆ˜ ì •ë³´ ì¶”ì¶œ
        target_store = rank_result.get("target_store", {})
        visitor_review_count = target_store.get("visitor_review_count", 0)
        blog_review_count = target_store.get("blog_review_count", 0)
        save_count = target_store.get("save_count", 0)
        
        logger.info(
            f"[Unofficial API Rank] Target store stats: "
            f"Visitor Reviews={visitor_review_count}, "
            f"Blog Reviews={blog_review_count}, "
            f"Saves={save_count}"
        )
        
        # ê¸°ì¡´ í‚¤ì›Œë“œ í™•ì¸
        keyword_check = supabase.table("keywords").select(
            "id, current_rank, previous_rank"
        ).eq("store_id", str(request.store_id)).eq(
            "keyword", request.keyword
        ).execute()
        
        now = datetime.utcnow()
        today = date.today()
        
        keyword_id = None
        previous_rank = None
        
        # total_count íŒŒì‹± (ë¬¸ìì—´ "778" ë˜ëŠ” ì •ìˆ˜ 778 â†’ ì •ìˆ˜ 778)
        total_count_value = rank_result.get("total_count", 0)
        total_results_int = 0
        if total_count_value is not None:
            if isinstance(total_count_value, str):
                # ë¬¸ìì—´ "1,234" â†’ 1234
                total_results_int = int(total_count_value.replace(",", "")) if total_count_value.strip() else 0
            elif isinstance(total_count_value, int):
                # ì •ìˆ˜ ê·¸ëŒ€ë¡œ
                total_results_int = total_count_value
        
        logger.info(f"[Total Results] Parsed total_count: {total_count_value} â†’ {total_results_int}")
        
        if keyword_check.data and len(keyword_check.data) > 0:
            # ê¸°ì¡´ í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸
            existing_keyword = keyword_check.data[0]
            keyword_id = existing_keyword["id"]
            previous_rank = existing_keyword["current_rank"]
            
            # keywords í…Œì´ë¸” ì—…ë°ì´íŠ¸ (total_results í¬í•¨)
            supabase.table("keywords").update({
                "previous_rank": previous_rank,
                "current_rank": rank_result["rank"],
                "total_results": total_results_int,
                "last_checked_at": now.isoformat()
            }).eq("id", keyword_id).execute()
            
            logger.info(
                f"[Unofficial API Rank] Updated existing keyword (ID: {keyword_id}), "
                f"Rank: {previous_rank} â†’ {rank_result['rank']}, Total: {total_results_int}"
            )
            
        else:
            # ìƒˆ í‚¤ì›Œë“œ ë“±ë¡ ì „ì— ì œí•œ í™•ì¸
            # storeë¥¼ í†µí•´ user_id ê°€ì ¸ì˜¤ê¸°
            user_id = store_data.get("user_id")
            if not user_id:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="ë§¤ì¥ì— ì—°ê²°ëœ ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            is_limit_exceeded, current_count, max_count = check_keyword_limit(supabase, user_id)
            
            if is_limit_exceeded:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail=f"í‚¤ì›Œë“œ ë“±ë¡ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. (í˜„ì¬: {current_count}/{max_count}ê°œ) êµ¬ë… í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”."
                )
            
            # ìƒˆ í‚¤ì›Œë“œ ë“±ë¡ (total_results í¬í•¨)
            keyword_insert = supabase.table("keywords").insert({
                "store_id": str(request.store_id),
                "keyword": request.keyword,
                "current_rank": rank_result["rank"],
                "previous_rank": None,
                "total_results": total_results_int,
                "last_checked_at": now.isoformat()
            }).execute()
            
            keyword_id = keyword_insert.data[0]["id"]
            
            logger.info(
                f"[Unofficial API Rank] Created new keyword (ID: {keyword_id}), "
                f"Rank: {rank_result['rank']}, Total: {total_results_int}, User keywords: {current_count + 1}/{max_count}"
            )
        
        # rank_history ì²˜ë¦¬ (ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„°ë§Œ ìœ ì§€)
        # 1. ì˜¤ëŠ˜ ë‚ ì§œì˜ ê¸°ì¡´ ê¸°ë¡ ì‚­ì œ
        supabase.table("rank_history").delete().eq(
            "keyword_id", keyword_id
        ).gte(
            "checked_at", today.isoformat()
        ).lt(
            "checked_at", (today.replace(day=today.day + 1)).isoformat() if today.day < 28 else today.isoformat()
        ).execute()
        
        # 2. ìƒˆë¡œìš´ ê¸°ë¡ ì¶”ê°€
        supabase.table("rank_history").insert({
            "keyword_id": keyword_id,
            "rank": rank_result["rank"],
            "checked_at": now.isoformat()
        }).execute()
        
        logger.info(f"[Unofficial API Rank] Saved rank history for today")
        
        # ìˆœìœ„ ë³€ë™ ê³„ì‚°
        rank_change = None
        if previous_rank and rank_result["rank"]:
            # ìˆœìœ„ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (1ìœ„ê°€ ìµœê³ )
            rank_change = previous_rank - rank_result["rank"]  # ì–‘ìˆ˜: ìˆœìœ„ ìƒìŠ¹
        
        return RankCheckResponseUnofficial(
            status="success",
            keyword=request.keyword,
            place_id=place_id,
            store_name=store_name,
            rank=rank_result["rank"],
            found=rank_result["found"],
            total_results=rank_result["total_results"],
            total_count=rank_result.get("total_count"),
            previous_rank=previous_rank,
            rank_change=rank_change,
            last_checked_at=now,
            search_results=rank_result["search_results"],
            # ë¦¬ë·°ìˆ˜ ì •ë³´ ì¶”ê°€ â­
            visitor_review_count=visitor_review_count,
            blog_review_count=blog_review_count,
            save_count=save_count
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[Unofficial API Rank] Error: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìˆœìœ„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ============================================
# ëŒ€í‘œí‚¤ì›Œë“œ ë¶„ì„ API
# ============================================

class KeywordsAnalysisRequest(BaseModel):
    """ëŒ€í‘œí‚¤ì›Œë“œ ë¶„ì„ ìš”ì²­"""
    query: str


class StoreKeywordInfo(BaseModel):
    """ë§¤ì¥ë³„ í‚¤ì›Œë“œ ì •ë³´"""
    rank: int
    place_id: str
    name: str
    category: str
    address: str
    thumbnail: Optional[str] = ""
    rating: Optional[float] = None
    review_count: str
    keywords: List[str]


class KeywordsAnalysisResponse(BaseModel):
    """ëŒ€í‘œí‚¤ì›Œë“œ ë¶„ì„ ì‘ë‹µ"""
    status: str
    query: str
    total_stores: int
    stores_analyzed: List[StoreKeywordInfo]


@router.post("/analyze-main-keywords", response_model=KeywordsAnalysisResponse)
async def analyze_main_keywords(
    request: KeywordsAnalysisRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ëŒ€í‘œí‚¤ì›Œë“œ ë¶„ì„
    
    ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ìƒìœ„ 15ê°œ ë§¤ì¥ì˜ ëŒ€í‘œ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    í¬ë ˆë”§: 10 í¬ë ˆë”§ ì†Œëª¨
    """
    user_id = UUID(current_user["id"])
    
    try:
        # ğŸ†• í¬ë ˆë”§ ì²´í¬ (Feature Flag í™•ì¸)
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_CHECK_STRICT:
            check_result = await credit_service.check_sufficient_credits(
                user_id=user_id,
                feature="main_keyword_analysis",
                required_credits=10
            )
            
            if not check_result.sufficient:
                logger.warning(f"[Credits] User {user_id} has insufficient credits for main keyword analysis")
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail="í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í¬ë ˆë”§ì„ ì¶©ì „í•˜ê±°ë‚˜ í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”."
                )
            
            logger.info(f"[Credits] User {user_id} has sufficient credits for main keyword analysis")
        
        logger.info(f"[ëŒ€í‘œí‚¤ì›Œë“œ ë¶„ì„] ìš”ì²­: {request.query}")
        
        result = await keywords_analyzer_service.analyze_top_stores_keywords(
            query=request.query,
            top_n=15
        )
        
        # ğŸ†• í¬ë ˆë”§ ì°¨ê° (ì„±ê³µ ì‹œ)
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_AUTO_DEDUCT:
            try:
                transaction_id = await credit_service.deduct_credits(
                    user_id=user_id,
                    feature="main_keyword_analysis",
                    credits_amount=10,
                    metadata={
                        "query": request.query,
                        "stores_count": len(result.get("stores_analyzed", []))
                    }
                )
                logger.info(f"[Credits] Deducted 10 credits from user {user_id} (transaction: {transaction_id})")
            except Exception as credit_error:
                logger.error(f"[Credits] Failed to deduct credits: {credit_error}")
        
        return KeywordsAnalysisResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[ëŒ€í‘œí‚¤ì›Œë“œ ë¶„ì„] Error: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ëŒ€í‘œí‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ============================================
# í”Œë ˆì´ìŠ¤ ì§„ë‹¨ API
# ============================================

async def get_optional_current_user(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> Optional[dict]:
    """
    Optional ì¸ì¦: í† í°ì´ ìˆìœ¼ë©´ ì‚¬ìš©ì ì •ë³´ ë°˜í™˜, ì—†ìœ¼ë©´ None ë°˜í™˜
    """
    if not credentials:
        return None
    
    try:
        return await get_current_user(credentials)
    except HTTPException:
        logger.warning("[Optional Auth] í† í° ê²€ì¦ ì‹¤íŒ¨, ìµëª… ì‚¬ìš©ìë¡œ ì²˜ë¦¬")
        return None


@router.get("/place-details/{place_id}")
async def get_place_details(
    place_id: str, 
    mode: str = "complete", 
    store_name: str = None,
    store_id: Optional[UUID] = None,
    current_user: Optional[dict] = Depends(get_optional_current_user)
):
    """
    í”Œë ˆì´ìŠ¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ (ì§„ë‹¨ìš©)
    
    Args:
        place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID
        mode: ì§„ë‹¨ ëª¨ë“œ (ê¸°ë³¸ê°’: complete)
            - "quick": ë¹ ë¥¸ ì§„ë‹¨ (GraphQLë§Œ, 1-2ì´ˆ)
            - "standard": í‘œì¤€ ì§„ë‹¨ (GraphQL + HTML, 3-5ì´ˆ) - ë¯¸êµ¬í˜„
            - "complete": ì™„ì „ ì§„ë‹¨ (ëª¨ë“  ë°ì´í„°, 5-10ì´ˆ)
        store_name: ë§¤ì¥ëª… (ì„ íƒ, ì œê³µí•˜ë©´ ê²€ìƒ‰ API ì‚¬ìš©ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ)
        store_id: ë§¤ì¥ ID (ì„ íƒ, ì œê³µí•˜ë©´ ì§„ë‹¨ íˆìŠ¤í† ë¦¬ ì €ì¥, ì¸ì¦ í•„ìš”)
    """
    try:
        logger.info(f"[í”Œë ˆì´ìŠ¤ ì§„ë‹¨] ìš”ì²­: place_id={place_id}, mode={mode}, store_name={store_name}, store_id={store_id}, authenticated={current_user is not None}")
        
        # ì™„ì „ ì§„ë‹¨ ì„œë¹„ìŠ¤ ì‚¬ìš©
        from app.services.naver_complete_diagnosis_service import complete_diagnosis_service
        
        details = await complete_diagnosis_service.diagnose_place(place_id, store_name)
        
        if not details:
            raise HTTPException(
                status_code=404,
                detail="í”Œë ˆì´ìŠ¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ëª»ëœ place_idì´ê±°ë‚˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # nameì´ ë¹„ì–´ìˆì–´ë„ place_idê°€ ìˆìœ¼ë©´ ê³„ì† ì§„í–‰
        if not details.get("name"):
            logger.warning(f"[í”Œë ˆì´ìŠ¤ ì§„ë‹¨] Name í•„ë“œê°€ ë¹„ì–´ìˆì§€ë§Œ ì§„í–‰: place_id={place_id}")
        
        # ì§„ë‹¨ í‰ê°€ ì‹¤í–‰
        from app.services.naver_diagnosis_engine import diagnosis_engine
        diagnosis_result = diagnosis_engine.diagnose(details)
        
        # ì±„ì›Œì§„ í•„ë“œ í†µê³„
        filled_count = sum(1 for v in details.values() if v not in [None, "", [], {}, 0, False])
        total_count = len(details)
        fill_rate = (filled_count / total_count * 100) if total_count > 0 else 0
        
        logger.info(f"[í”Œë ˆì´ìŠ¤ ì§„ë‹¨] ì™„ë£Œ: {details['name']}")
        logger.info(f"[í”Œë ˆì´ìŠ¤ ì§„ë‹¨] ì±„ì›Œì§„ ì •ë³´: {filled_count}/{total_count} ({fill_rate:.1f}%)")
        logger.info(f"[í”Œë ˆì´ìŠ¤ ì§„ë‹¨] í‰ê°€ ì ìˆ˜: {diagnosis_result['total_score']}/{diagnosis_result['max_score']}ì  ({diagnosis_result['grade']}ë“±ê¸‰)")
        
        # ì§„ë‹¨ íˆìŠ¤í† ë¦¬ ì €ì¥ (store_idì™€ current_userê°€ ì œê³µëœ ê²½ìš°ì—ë§Œ)
        history_id = None
        if store_id and current_user:
            try:
                supabase = get_supabase_client()
                user_id = current_user.get("id")
                
                if user_id:
                    history_data = {
                        "user_id": str(user_id),
                        "store_id": str(store_id),
                        "place_id": place_id,
                        "store_name": details.get("name", "Unknown"),
                        "total_score": int(float(diagnosis_result["total_score"])),  # floatì„ intë¡œ ë³€í™˜
                        "max_score": int(float(diagnosis_result["max_score"])),      # floatì„ intë¡œ ë³€í™˜
                        "grade": diagnosis_result["grade"],
                        "diagnosis_result": diagnosis_result,
                        "place_details": details
                    }
                    result = supabase.table("diagnosis_history").insert(history_data).execute()
                    if result.data and len(result.data) > 0:
                        history_id = result.data[0].get("id")
                        logger.info(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ì €ì¥ ì™„ë£Œ: store_id={store_id}, user_id={user_id}, history_id={history_id}")
                    else:
                        logger.warning(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ì €ì¥ì€ ë˜ì—ˆìœ¼ë‚˜ IDë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
                else:
                    logger.warning(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] user_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            except Exception as e:
                # íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨í•´ë„ ì§„ë‹¨ ê²°ê³¼ëŠ” ë°˜í™˜ (ê¸°ì¡´ ê¸°ëŠ¥ì— ì˜í–¥ ì—†ìŒ)
                logger.error(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        elif store_id and not current_user:
            logger.info(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ì, íˆìŠ¤í† ë¦¬ ì €ì¥ ê±´ë„ˆëœ€")
        else:
            logger.info(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] store_id ì—†ìŒ, íˆìŠ¤í† ë¦¬ ì €ì¥ ê±´ë„ˆëœ€")
        
        return {
            "status": "success",
            "place_id": place_id,
            "mode": mode,
            "fill_rate": round(fill_rate, 1),
            "details": details,
            "diagnosis": diagnosis_result,  # ì§„ë‹¨ í‰ê°€ ê²°ê³¼ ì¶”ê°€
            "history_id": history_id  # ì €ì¥ëœ íˆìŠ¤í† ë¦¬ ID ì¶”ê°€
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[í”Œë ˆì´ìŠ¤ ì§„ë‹¨] Error: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í”Œë ˆì´ìŠ¤ ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ============================================
# ê²½ìŸë§¤ì¥ ë¶„ì„ API
# ============================================

class CompetitorSearchRequest(BaseModel):
    """ê²½ìŸë§¤ì¥ ê²€ìƒ‰ ìš”ì²­"""
    keyword: str
    limit: int = 20


class CompetitorSearchResponse(BaseModel):
    """ê²½ìŸë§¤ì¥ ê²€ìƒ‰ ì‘ë‹µ"""
    status: str
    keyword: str
    total: int
    stores: List[Dict]


class CompetitorAnalysisRequest(BaseModel):
    """ê²½ìŸë§¤ì¥ ë¶„ì„ ìš”ì²­"""
    keyword: str
    my_place_id: str
    limit: int = 20


class CompetitorCompareRequest(BaseModel):
    """ê²½ìŸë§¤ì¥ ë¹„êµ ë¶„ì„ ìš”ì²­"""
    my_store: dict
    competitors: list
    
    class Config:
        arbitrary_types_allowed = True


class CompetitorAnalysisResponse(BaseModel):
    """ê²½ìŸë§¤ì¥ ë¶„ì„ ì‘ë‹µ"""
    status: str
    keyword: str
    my_store: Dict
    competitors: List[Dict]
    comparison: Dict


@router.post("/competitor/search", response_model=CompetitorSearchResponse)
async def search_competitors(request: CompetitorSearchRequest):
    """
    í‚¤ì›Œë“œë¡œ ìƒìœ„ ë…¸ì¶œ ê²½ìŸë§¤ì¥ ê²€ìƒ‰
    
    Args:
        request: ê²€ìƒ‰ ìš”ì²­ (í‚¤ì›Œë“œ, ê°œìˆ˜)
        
    Returns:
        ìƒìœ„ ë…¸ì¶œ ë§¤ì¥ ëª©ë¡
    """
    try:
        logger.info(f"[ê²½ìŸë§¤ì¥] ê²€ìƒ‰ ì‹œì‘: keyword={request.keyword}, limit={request.limit}")
        
        # ìƒìœ„ ë§¤ì¥ ê²€ìƒ‰
        stores = await competitor_analysis_service.get_top_competitors(
            keyword=request.keyword,
            limit=request.limit
        )
        
        logger.info(f"[ê²½ìŸë§¤ì¥] ê²€ìƒ‰ ì™„ë£Œ: {len(stores)}ê°œ ë°œê²¬")
        
        return {
            "status": "success",
            "keyword": request.keyword,
            "total": len(stores),
            "stores": stores
        }
        
    except Exception as e:
        logger.error(f"[ê²½ìŸë§¤ì¥] ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê²½ìŸë§¤ì¥ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/competitor/analyze", response_model=CompetitorAnalysisResponse)
async def analyze_competitors(request: CompetitorAnalysisRequest):
    """
    ê²½ìŸë§¤ì¥ ì „ì²´ ë¶„ì„ ë° ë¹„êµ
    
    Args:
        request: ë¶„ì„ ìš”ì²­ (í‚¤ì›Œë“œ, ìš°ë¦¬ ë§¤ì¥ ID, ê°œìˆ˜)
        
    Returns:
        ê²½ìŸë§¤ì¥ ë¶„ì„ ê²°ê³¼ + ìš°ë¦¬ ë§¤ì¥ ë¹„êµ
    """
    try:
        logger.info(f"[ê²½ìŸë§¤ì¥] ì „ì²´ ë¶„ì„ ì‹œì‘: keyword={request.keyword}, my_place_id={request.my_place_id}")
        
        # 1. ìš°ë¦¬ ë§¤ì¥ ë¶„ì„
        logger.info(f"[ê²½ìŸë§¤ì¥] ìš°ë¦¬ ë§¤ì¥ ë¶„ì„ ì¤‘...")
        my_store = await competitor_analysis_service.analyze_competitor(
            place_id=request.my_place_id,
            rank=0  # ìš°ë¦¬ ë§¤ì¥ì€ ìˆœìœ„ 0
        )
        
        if not my_store:
            raise HTTPException(
                status_code=404,
                detail="ìš°ë¦¬ ë§¤ì¥ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # 2. ê²½ìŸë§¤ì¥ ë¶„ì„
        logger.info(f"[ê²½ìŸë§¤ì¥] ê²½ìŸì‚¬ ë¶„ì„ ì¤‘...")
        competitors = await competitor_analysis_service.analyze_all_competitors(
            keyword=request.keyword,
            limit=request.limit
        )
        
        # 3. ë¹„êµ ë¶„ì„ (LLM ê¸°ë°˜)
        logger.info(f"[ê²½ìŸë§¤ì¥] ë¹„êµ ë¶„ì„ ì¤‘ (LLM ì‚¬ìš©)...")
        comparison = await competitor_analysis_service.compare_with_my_store(
            my_store_data=my_store,
            competitors=competitors
        )
        
        logger.info(f"[ê²½ìŸë§¤ì¥] ì „ì²´ ë¶„ì„ ì™„ë£Œ: {len(competitors)}ê°œ ê²½ìŸì‚¬")
        
        return {
            "status": "success",
            "keyword": request.keyword,
            "my_store": my_store,
            "competitors": competitors,
            "comparison": comparison
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[ê²½ìŸë§¤ì¥] ë¶„ì„ ì‹¤íŒ¨: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê²½ìŸë§¤ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/competitor/compare")
async def compare_competitors(request: dict):
    """
    ê²½ìŸë§¤ì¥ ë¹„êµ ë¶„ì„ (LLM ê¸°ë°˜)
    
    Args:
        request: ìš°ë¦¬ ë§¤ì¥ + ê²½ìŸë§¤ì¥ ë°ì´í„°
        
    Returns:
        LLM ê¸°ë°˜ ë¹„êµ ë¶„ì„ ê²°ê³¼
    """
    try:
        print(f"[DEBUG] ë¹„êµ ë¶„ì„ ìš”ì²­ ë°›ìŒ")
        my_store = request.get("my_store", {})
        competitors = request.get("competitors", [])
        print(f"[DEBUG] my_store type: {type(my_store)}")
        print(f"[DEBUG] competitors type: {type(competitors)}")
        print(f"[DEBUG] competitors length: {len(competitors)}")
        logger.info(f"[ê²½ìŸë§¤ì¥] ë¹„êµ ë¶„ì„ ìš”ì²­: {len(competitors)}ê°œ ê²½ìŸì‚¬")
        
        comparison = await competitor_analysis_service.compare_with_my_store(
            my_store_data=my_store,
            competitors=competitors
        )
        
        logger.info(f"[ê²½ìŸë§¤ì¥] ë¹„êµ ë¶„ì„ ì™„ë£Œ: {len(comparison.get('recommendations', []))}ê°œ ê¶Œì¥ì‚¬í•­")
        
        return comparison
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"[ê²½ìŸë§¤ì¥] ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}\n{error_details}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/competitor/analyze-single/{place_id}")
async def analyze_single_competitor(place_id: str, rank: int = 0, store_name: str = None):
    """
    ë‹¨ì¼ ê²½ìŸë§¤ì¥ ë¶„ì„ (ì ì§„ì  ë¡œë”©ìš©)
    
    Args:
        place_id: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ID
        rank: ê²€ìƒ‰ ìˆœìœ„
        store_name: ë§¤ì¥ëª… (ì„ íƒ, ì œê³µí•˜ë©´ ì •í™•ë„ í–¥ìƒ)
        
    Returns:
        ë§¤ì¥ ë¶„ì„ ê²°ê³¼
    """
    try:
        logger.info(f"[ê²½ìŸë§¤ì¥] ë‹¨ì¼ ë¶„ì„ ì‹œì‘: place_id={place_id}, store_name={store_name}, rank={rank}")
        
        result = await competitor_analysis_service.analyze_competitor(
            place_id=place_id,
            rank=rank,
            store_name=store_name
        )
        
        if not result:
            logger.error(f"[ê²½ìŸë§¤ì¥] place_id={place_id}ë¡œ ë§¤ì¥ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            raise HTTPException(
                status_code=404,
                detail=f"ë§¤ì¥ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. place_id: {place_id}"
            )
        
        logger.info(f"[ê²½ìŸë§¤ì¥] ë‹¨ì¼ ë¶„ì„ ì™„ë£Œ: {result.get('name', 'Unknown')}")
        
        return {
            "status": "success",
            "result": result
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[ê²½ìŸë§¤ì¥] ë‹¨ì¼ ë¶„ì„ ì‹¤íŒ¨: {str(e)}\n{error_trace}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë§¤ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ============================================
# ì§„ë‹¨ íˆìŠ¤í† ë¦¬ API
# ============================================

@router.get("/diagnosis-history/{store_id}")
async def get_diagnosis_history(
    store_id: UUID,
    current_user: dict = Depends(get_current_user),
    limit: int = Query(30, ge=1, le=100, description="ì¡°íšŒí•  íˆìŠ¤í† ë¦¬ ê°œìˆ˜")
):
    """
    ë§¤ì¥ì˜ ì§„ë‹¨ íˆìŠ¤í† ë¦¬ ëª©ë¡ ì¡°íšŒ
    
    Args:
        store_id: ë§¤ì¥ ID
        limit: ì¡°íšŒí•  ê°œìˆ˜ (ê¸°ë³¸ 30ê°œ, ìµœëŒ€ 100ê°œ)
        
    Returns:
        ì§„ë‹¨ íˆìŠ¤í† ë¦¬ ëª©ë¡ (ìµœì‹ ìˆœ)
    """
    try:
        supabase = get_supabase_client()
        user_id = current_user["id"]
        
        logger.info(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ì¡°íšŒ ì‹œì‘: user_id={user_id}, store_id={store_id}, limit={limit}")
        
        # íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ìµœì‹ ìˆœ, user_idì™€ store_idë¡œ í•„í„°ë§)
        result = supabase.table("diagnosis_history")\
            .select("id, place_id, store_name, diagnosed_at, total_score, max_score, grade")\
            .eq("user_id", str(user_id))\
            .eq("store_id", str(store_id))\
            .order("diagnosed_at", desc=True)\
            .limit(limit)\
            .execute()
        
        history_list = result.data if result.data else []
        
        logger.info(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ì¡°íšŒ ì™„ë£Œ: {len(history_list)}ê°œ")
        
        return {
            "status": "success",
            "store_id": str(store_id),
            "total": len(history_list),
            "history": history_list
        }
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\n{error_trace}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì§„ë‹¨ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/diagnosis-history/detail/{history_id}")
async def get_diagnosis_history_detail(
    history_id: UUID,
    current_user: dict = Depends(get_current_user)
):
    """
    íŠ¹ì • ì§„ë‹¨ íˆìŠ¤í† ë¦¬ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
    
    Args:
        history_id: íˆìŠ¤í† ë¦¬ ID
        
    Returns:
        ì§„ë‹¨ íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ (ì „ì²´ ì§„ë‹¨ ê²°ê³¼ í¬í•¨)
    """
    try:
        supabase = get_supabase_client()
        user_id = current_user["id"]
        
        logger.info(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ìƒì„¸ ì¡°íšŒ: user_id={user_id}, history_id={history_id}")
        
        # íˆìŠ¤í† ë¦¬ ìƒì„¸ ì¡°íšŒ (user_id í™•ì¸)
        result = supabase.table("diagnosis_history")\
            .select("*")\
            .eq("id", str(history_id))\
            .eq("user_id", str(user_id))\
            .single()\
            .execute()
        
        if not result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ì§„ë‹¨ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ìƒì„¸ ì¡°íšŒ ì™„ë£Œ: {result.data['store_name']}")
        
        return {
            "status": "success",
            "history": result.data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[ì§„ë‹¨ íˆìŠ¤í† ë¦¬] ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\n{error_trace}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì§„ë‹¨ íˆìŠ¤í† ë¦¬ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ==================== í”Œë ˆì´ìŠ¤ í™œì„±í™” ====================

class ActivationResponse(BaseModel):
    """í”Œë ˆì´ìŠ¤ í™œì„±í™” ì •ë³´ ì‘ë‹µ"""
    status: str
    data: Dict[str, Any]


class GenerateTextRequest(BaseModel):
    """í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­"""
    store_id: str
    # ê¸°ì¡´ í•„ë“œ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
    prompt: Optional[str] = None
    # ìƒˆë¡œìš´ í•„ë“œ (ì—…ì²´ì†Œê°œê¸€ SEO ìµœì í™”ìš©)
    region_keyword: Optional[str] = None  # ì§€ì—­ í‚¤ì›Œë“œ (1ê°œ)
    landmark_keywords: Optional[List[str]] = None  # ëœë“œë§ˆí¬ í‚¤ì›Œë“œ (ìµœëŒ€ 2ê°œ)
    business_type_keyword: Optional[str] = None  # ì—…ì¢… í‚¤ì›Œë“œ (1ê°œ)
    product_keywords: Optional[List[str]] = None  # ìƒí’ˆ/ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ (ìµœëŒ€ 3ê°œ)
    store_features: Optional[str] = None  # ë§¤ì¥ íŠ¹ìƒ‰ ë° ê°•ì 
    # ì°¾ì•„ì˜¤ëŠ”ê¸¸ SEO ìµœì í™”ìš©
    directions_description: Optional[str] = None  # ì°¾ì•„ì˜¤ëŠ” ê¸¸ ììœ  ì…ë ¥ ì„¤ëª…


class GenerateTextResponse(BaseModel):
    """í…ìŠ¤íŠ¸ ìƒì„± ì‘ë‹µ"""
    status: str
    generated_text: str


@router.get("/activation/{store_id}", response_model=ActivationResponse)
async def get_activation_info(
    store_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    í”Œë ˆì´ìŠ¤ í™œì„±í™” ì •ë³´ ì¡°íšŒ
    
    Args:
        store_id: ë§¤ì¥ ID (UUID)
        current_user: í˜„ì¬ ì‚¬ìš©ì ì •ë³´
    """
    try:
        logger.info(f"[í”Œë ˆì´ìŠ¤ í™œì„±í™”] ìš”ì²­: store_id={store_id}, user_id={current_user['id']}")
        
        supabase = get_supabase_client()
        
        # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        store_result = supabase.table("stores").select("*").eq("id", store_id).execute()
        
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = store_result.data[0]
        
        # 2. ê¶Œí•œ í™•ì¸
        if store.get("user_id") != current_user["id"]:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="í•´ë‹¹ ë§¤ì¥ì— ì ‘ê·¼í•  ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤"
            )
        
        place_id = store.get("place_id")
        store_name = store.get("store_name")
        
        if not place_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ë“±ë¡ë˜ì§€ ì•Šì€ ë§¤ì¥ì…ë‹ˆë‹¤"
            )
        
        # ğŸ†• í¬ë ˆë”§ ì²´í¬ (Feature Flag í™•ì¸)
        user_id = UUID(current_user["id"])
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_CHECK_STRICT:
            check_result = await credit_service.check_sufficient_credits(
                user_id=user_id,
                feature="place_diagnosis",
                required_credits=5
            )
            
            if not check_result.sufficient:
                logger.warning(f"[Credits] User {user_id} has insufficient credits for place activation")
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail="í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í¬ë ˆë”§ì„ ì¶©ì „í•˜ê±°ë‚˜ í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”."
                )
            
            logger.info(f"[Credits] User {user_id} has sufficient credits for place activation")
        
        # 3. í™œì„±í™” ì •ë³´ ì¡°íšŒ
        from app.services.naver_activation_service_v3 import activation_service_v3
        
        activation_data = await activation_service_v3.get_activation_info(
            store_id=store_id,
            place_id=place_id,
            store_name=store_name
        )
        
        logger.info(f"[í”Œë ˆì´ìŠ¤ í™œì„±í™”] ì™„ë£Œ: {len(activation_data.get('issues', []))}ê°œ ì´ìŠˆ")
        
        # ğŸ†• í¬ë ˆë”§ ì°¨ê° (ì„±ê³µ ì‹œì—ë§Œ)
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_AUTO_DEDUCT:
            try:
                transaction_id = await credit_service.deduct_credits(
                    user_id=user_id,
                    feature="place_diagnosis",
                    credits_amount=5,
                    metadata={
                        "store_id": store_id,
                        "place_id": place_id,
                        "store_name": store_name
                    }
                )
                logger.info(f"[Credits] Deducted 5 credits from user {user_id} (transaction: {transaction_id})")
            except Exception as credit_error:
                logger.error(f"[Credits] Failed to deduct credits: {credit_error}")
                # í¬ë ˆë”§ ì°¨ê° ì‹¤íŒ¨ëŠ” ê¸°ëŠ¥ ì‚¬ìš©ì„ ë§‰ì§€ ì•ŠìŒ (ì´ë¯¸ ì¡°íšŒëŠ” ì™„ë£Œë¨)
        
        # 4. í™œì„±í™” ì´ë ¥ ì €ì¥
        try:
            history_data = {
                "user_id": current_user["id"],
                "store_id": store_id,
                "store_name": store_name,
                "place_id": place_id,
                "thumbnail": store.get("thumbnail"),
                "summary_cards": activation_data.get("summary_cards", []),
                "activation_data": activation_data,
            }
            
            supabase.table("activation_history").insert(history_data).execute()
            logger.info(f"[í”Œë ˆì´ìŠ¤ í™œì„±í™”] ì´ë ¥ ì €ì¥ ì™„ë£Œ: store_id={store_id}")
        except Exception as e:
            # ì´ë ¥ ì €ì¥ ì‹¤íŒ¨í•´ë„ ê²°ê³¼ëŠ” ë°˜í™˜
            logger.warning(f"[í”Œë ˆì´ìŠ¤ í™œì„±í™”] ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        return ActivationResponse(
            status="success",
            data=activation_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[í”Œë ˆì´ìŠ¤ í™œì„±í™”] ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í™œì„±í™” ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/activation/generate-description", response_model=GenerateTextResponse)
async def generate_description(
    request: GenerateTextRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ì—…ì²´ì†Œê°œê¸€ SEO ìµœì í™” ìƒì„±
    
    Args:
        request: ìƒì„± ìš”ì²­ (store_id, prompt)
        current_user: í˜„ì¬ ì‚¬ìš©ì ì •ë³´
    """
    try:
        logger.info(f"[ì—…ì²´ì†Œê°œê¸€ ìƒì„±] ìš”ì²­: store_id={request.store_id}")
        
        supabase = get_supabase_client()
        
        # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ ë° ê¶Œí•œ í™•ì¸
        store_result = supabase.table("stores").select("*").eq("id", request.store_id).execute()
        
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = store_result.data[0]
        
        if store.get("user_id") != current_user["id"]:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="í•´ë‹¹ ë§¤ì¥ì— ì ‘ê·¼í•  ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤"
            )
        
        # ğŸ†• í¬ë ˆë”§ ì²´í¬ (Feature Flag í™•ì¸)
        user_id = UUID(current_user["id"])
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_CHECK_STRICT:
            check_result = await credit_service.check_sufficient_credits(
                user_id=user_id,
                feature="business_description",
                required_credits=5
            )
            
            if not check_result.sufficient:
                logger.warning(f"[Credits] User {user_id} has insufficient credits for business description")
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail="í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í¬ë ˆë”§ì„ ì¶©ì „í•˜ê±°ë‚˜ í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”."
                )
            
            logger.info(f"[Credits] User {user_id} has sufficient credits for business description")
        
        # 2. LLMìœ¼ë¡œ ì—…ì²´ì†Œê°œê¸€ ìƒì„±
        import os
        from openai import AsyncOpenAI
        
        openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # ìƒˆë¡œìš´ í•„ë“œê°€ ì œê³µëœ ê²½ìš° SEO ìµœì í™” í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        if request.region_keyword or request.landmark_keywords or request.business_type_keyword:
            # í‚¤ì›Œë“œ ì •ë¦¬
            region = request.region_keyword or "ì •ë³´ ì—†ìŒ"
            landmarks = ", ".join(request.landmark_keywords) if request.landmark_keywords else "ì •ë³´ ì—†ìŒ"
            business_type = request.business_type_keyword or "ì •ë³´ ì—†ìŒ"
            products = ", ".join(request.product_keywords) if request.product_keywords else "ì •ë³´ ì—†ìŒ"
            features = request.store_features or "ì •ë³´ ì—†ìŒ"
            
            system_prompt = """ë‹¹ì‹ ì€ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ SEO ë° ë¡œì»¬ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì— íŠ¹í™”ëœ ëŒ€í•œë¯¼êµ­ ìµœê³  ìˆ˜ì¤€ì˜
ë¡œì»¬ ë§ˆì¼€íŒ… ì „ë¬¸ ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì•„ë˜ [ì…ë ¥ ì •ë³´]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë…¸ì¶œì— ìµœì í™”ëœ ì—…ì²´ ì„¤ëª…ì„ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."""
            
            user_message = f"""â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ì…ë ¥ ì •ë³´]

1. ì§€ì—­ í‚¤ì›Œë“œ:
{region}

2. ëœë“œë§ˆí¬ í‚¤ì›Œë“œ (ì—­, ìƒê¶Œ, ê±´ë¬¼, ê´€ê´‘ì§€ ë“±):
{landmarks}

3. ì—…ì¢… í‚¤ì›Œë“œ:
{business_type}

4. ìƒí’ˆ/ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ:
{products}

5. ë§¤ì¥ íŠ¹ìƒ‰ ë° ê°•ì , ìš°ë¦¬ ë§¤ì¥ì„ ê¼­ ë°©ë¬¸í•´ì•¼ í•˜ëŠ” ì´ìœ :
{features}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[SEO í•µì‹¬ ì§€ì¹¨ â€” ë°˜ë“œì‹œ ì¤€ìˆ˜]

1. ìµœì¢… ê²°ê³¼ë¬¼ì€ **1900ì ì´ìƒ 1950ì ì´í•˜ (í•œê¸€ ê¸°ì¤€)** ë¡œ ì‘ì„±í•˜ì„¸ìš”.
   - ê¸€ì ìˆ˜ë¥¼ ë°˜ë“œì‹œ ì§ì ‘ ê³„ì‚°í•˜ì—¬ ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í•˜ì„¸ìš”.

2. ì•„ë˜ í‚¤ì›Œë“œëŠ” **SEO ìµœì  ë¹ˆë„ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„ì‚° ë°°ì¹˜**í•´ì•¼ í•©ë‹ˆë‹¤.

   â–ª ì§€ì—­ í‚¤ì›Œë“œ:
     - ì´ **7~9íšŒ**
     - ë„ì…ë¶€, ì¤‘ë°˜, ê²°ë¡ ë¶€ì— ê³ ë¥´ê²Œ ë¶„í¬

   â–ª ëœë“œë§ˆí¬ í‚¤ì›Œë“œ:
     - ì´ **4~6íšŒ**
     - 'ìœ„ì¹˜ ì„¤ëª…', 'ì°¾ì•„ì˜¤ëŠ” ë°©ë²•', 'ì ‘ê·¼ì„±', 'ì£¼ë³€ í™˜ê²½' ë¬¸ë§¥ì—ì„œ ì‚¬ìš©
     - ì§€ì—­ í‚¤ì›Œë“œë¥¼ ë‹¨ìˆœ ë°˜ë³µí•˜ì§€ ë§ê³  ë³´ì¡° ìœ„ì¹˜ ì‹ í˜¸ë¡œ í™œìš©

   â–ª ì—…ì¢… í‚¤ì›Œë“œ:
     - ì´ **6~7íšŒ**
     - ë§¤ì¥ ì •ì²´ì„±ê³¼ ì „ë¬¸ì„±ì„ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì—ì„œ í™œìš©

   â–ª ìƒí’ˆ/ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ:
     - í•µì‹¬ ìƒí’ˆ ê° **2~4íšŒ**
     - ì‹¤ì œ ì œê³µ ê°€ì¹˜, ê²½í—˜, ê²°ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ 

3. **ìœ ì €ê°€ ì…ë ¥í•œ ëª¨ë“  ë¬¸ì¥ì€ ì˜ë¯¸ í›¼ì† ì—†ì´ ë°˜ì˜**í•´ì•¼ í•©ë‹ˆë‹¤.
   - ì¼ë¶€ í‘œí˜„ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì„ ìˆ˜ ìˆìœ¼ë‚˜, ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ìµœì í™” êµ¬ì¡°]

ì•„ë˜ êµ¬ì¡°ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ë“œëŸ¬ë‚˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.  
(ì†Œì œëª©ì€ ì“°ì§€ ë§ê³  íë¦„ìœ¼ë¡œë§Œ ë°˜ì˜)

1) ì§€ì—­ + ëœë“œë§ˆí¬ ê¸°ë°˜ ë§¤ì¥ ì†Œê°œ
2) ì—…ì¢… ë° ìƒí’ˆ/ì„œë¹„ìŠ¤ ì „ë¬¸ì„± ì„¤ëª…
3) ë§¤ì¥ì˜ ì°¨ë³„í™”ëœ ê°•ì ê³¼ ë¶„ìœ„ê¸°
4) ì‹¤ì œ ë°©ë¬¸ ì‹œ ê³ ê°ì´ ê²½í—˜í•˜ê²Œ ë˜ëŠ” í¬ì¸íŠ¸
5) ì´ëŸ° ê³ ê°ì—ê²Œ íŠ¹íˆ ì¶”ì²œë˜ëŠ” ì´ìœ 
6) ì¬ë°©ë¬¸ê³¼ ì¶”ì²œì„ ìœ ë„í•˜ëŠ” ë§ˆë¬´ë¦¬

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ë¬¸ì²´ ë° í‘œí˜„ ê·œì¹™]

- ê´‘ê³  ë¬¸êµ¬ì²˜ëŸ¼ ê³¼ì¥ëœ í‘œí˜„ âŒ
- ì‹¤ì œ ë§¤ì¥ ìš´ì˜ìê°€ ì •ì„±ê» ì§ì ‘ ì‘ì„±í•œ ì„¤ëª…ì²˜ëŸ¼ ì‹ ë¢°ê° ìˆê²Œ ì‘ì„±
- "ì…ë‹ˆë‹¤ / í•©ë‹ˆë‹¤" ì²´ë¡œ í†µì¼
- í‚¤ì›Œë“œ ë‚˜ì—´ì²˜ëŸ¼ ë³´ì´ì§€ ì•Šë„ë¡ ë¬¸ë§¥ ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì¼ ê²ƒ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ì¶œë ¥ ì œí•œ]

- ì œëª© âŒ
- ì†Œì œëª© âŒ
- ì´ëª¨ì§€ âŒ
- í•´ì‹œíƒœê·¸ âŒ
- ì „í™”ë²ˆí˜¸, ê°€ê²©, ì´ë²¤íŠ¸ ì–¸ê¸‰ âŒ
- ìˆœìˆ˜ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥"""
        
        else:
            # ê¸°ì¡´ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
            system_prompt = """ë‹¹ì‹ ì€ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì—…ì²´ì†Œê°œê¸€ì„ ì‘ì„±í•  ë•Œ ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¥´ì„¸ìš”:

1. í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ (ì§€ì—­ëª…, ì—…ì¢…, íŠ¹ì§•)
2. ê³ ê°ì´ ì°¾ëŠ” ì •ë³´ ìš°ì„  (ë©”ë‰´, ê°€ê²©ëŒ€, íŠ¹ìƒ‰)
3. ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì¥ (200-300ì ê¶Œì¥)
4. ì°¨ë³„í™” í¬ì¸íŠ¸ ê°•ì¡°
5. ê³¼ì¥ í‘œí˜„ ì§€ì–‘, ì‚¬ì‹¤ ê¸°ë°˜ ì‘ì„±

ì—…ì²´ì†Œê°œê¸€ë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

            user_message = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì—…ì²´ì†Œê°œê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

ë§¤ì¥ëª…: {store.get('store_name')}
ì¹´í…Œê³ ë¦¬: {store.get('category', 'ì •ë³´ ì—†ìŒ')}
ì£¼ì†Œ: {store.get('address', 'ì •ë³´ ì—†ìŒ')}

ì‚¬ìš©ì ìš”ì²­ì‚¬í•­:
{request.prompt or 'ë§¤ì¥ íŠ¹ìƒ‰ì„ ì‚´ë¦° ì—…ì²´ì†Œê°œê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.'}

ì—…ì²´ì†Œê°œê¸€:"""

        # OpenAI API ì§ì ‘ í˜¸ì¶œ
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=3000
        )
        
        generated_text = response.choices[0].message.content
        logger.info(f"[ì—…ì²´ì†Œê°œê¸€ ìƒì„±] ì™„ë£Œ: {len(generated_text)}ì")
        
        # ğŸ†• í¬ë ˆë”§ ì°¨ê° (ì„±ê³µ ì‹œì—ë§Œ)
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_AUTO_DEDUCT:
            try:
                transaction_id = await credit_service.deduct_credits(
                    user_id=user_id,
                    feature="business_description",
                    credits_amount=5,
                    metadata={
                        "store_id": request.store_id,
                        "store_name": store.get("store_name"),
                        "text_length": len(generated_text)
                    }
                )
                logger.info(f"[Credits] Deducted 5 credits from user {user_id} (transaction: {transaction_id})")
            except Exception as credit_error:
                logger.error(f"[Credits] Failed to deduct credits: {credit_error}")
                # í¬ë ˆë”§ ì°¨ê° ì‹¤íŒ¨ëŠ” ê¸°ëŠ¥ ì‚¬ìš©ì„ ë§‰ì§€ ì•ŠìŒ (ì´ë¯¸ ìƒì„±ì€ ì™„ë£Œë¨)
        
        return GenerateTextResponse(
            status="success",
            generated_text=generated_text.strip()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ì—…ì²´ì†Œê°œê¸€ ìƒì„±] ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì—…ì²´ì†Œê°œê¸€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/activation/generate-directions", response_model=GenerateTextResponse)
async def generate_directions(
    request: GenerateTextRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ì°¾ì•„ì˜¤ëŠ”ê¸¸ SEO ìµœì í™” ìƒì„±
    
    Args:
        request: ìƒì„± ìš”ì²­ (store_id, prompt)
        current_user: í˜„ì¬ ì‚¬ìš©ì ì •ë³´
    """
    try:
        logger.info(f"[ì°¾ì•„ì˜¤ëŠ”ê¸¸ ìƒì„±] ìš”ì²­: store_id={request.store_id}")
        
        supabase = get_supabase_client()
        
        # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ ë° ê¶Œí•œ í™•ì¸
        store_result = supabase.table("stores").select("*").eq("id", request.store_id).execute()
        
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = store_result.data[0]
        
        if store.get("user_id") != current_user["id"]:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="í•´ë‹¹ ë§¤ì¥ì— ì ‘ê·¼í•  ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤"
            )
        
        # ğŸ†• í¬ë ˆë”§ ì²´í¬ (Feature Flag í™•ì¸)
        user_id = UUID(current_user["id"])
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_CHECK_STRICT:
            check_result = await credit_service.check_sufficient_credits(
                user_id=user_id,
                feature="directions",
                required_credits=3
            )
            
            if not check_result.sufficient:
                logger.warning(f"[Credits] User {user_id} has insufficient credits for directions")
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail="í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í¬ë ˆë”§ì„ ì¶©ì „í•˜ê±°ë‚˜ í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”."
                )
            
            logger.info(f"[Credits] User {user_id} has sufficient credits for directions")
        
        # 2. LLMìœ¼ë¡œ ì°¾ì•„ì˜¤ëŠ”ê¸¸ ìƒì„±
        import os
        from openai import AsyncOpenAI
        
        openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # ìƒˆë¡œìš´ SEO ìµœì í™” ë°©ì‹ (region_keyword, landmark_keywords, directions_description ì‚¬ìš©)
        if request.region_keyword and request.directions_description:
            system_prompt = """ë‹¹ì‹ ì€ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ 'ì°¾ì•„ì˜¤ëŠ” ê¸¸' ì˜ì—­ì„
SEO ê´€ì ì—ì„œ ìµœì í™”í•˜ëŠ” ë¡œì»¬ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ [ì…ë ¥ ì •ë³´]ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬,
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë…¸ì¶œê³¼ ì‹¤ì œ ë°©ë¬¸ í¸ì˜ì„±ì„ ëª¨ë‘ ë†’ì´ëŠ”
'ì°¾ì•„ì˜¤ëŠ” ê¸¸' ì„¤ëª…ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ì…ë ¥ ì •ë³´]

1. ì§€ì—­ í‚¤ì›Œë“œ:
{region_keyword}

2. ëœë“œë§ˆí¬ í‚¤ì›Œë“œ (ì—­, ìƒê¶Œ, ê±´ë¬¼, ê´€ê´‘ì§€ ë“±):
{landmark_keywords}

3. ì‘ì„±ìê°€ ììœ ë¡­ê²Œ ì…ë ¥í•œ ì°¾ì•„ì˜¤ëŠ” ê¸¸ ì„¤ëª…:
{directions_description}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ì‘ì„± í•µì‹¬ ì§€ì¹¨ â€” ë°˜ë“œì‹œ ì¤€ìˆ˜]

1. ìµœì¢… ê¸€ì ìˆ˜ëŠ” **360ì ì´ìƒ 390ì ì´í•˜ (í•œê¸€ ê¸°ì¤€)** ë¡œ ì‘ì„±í•˜ì„¸ìš”.
   - ë°˜ë“œì‹œ ê¸€ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì´ ë²”ìœ„ë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ ë¶€ì¡±í•˜ì§€ ì•Šê²Œ ì‘ì„±í•˜ì„¸ìš”.

2. ì…ë ¥ëœ ëª¨ë“  ì •ë³´ëŠ” **ì˜ë¯¸ í›¼ì† ì—†ì´ ë°˜ë“œì‹œ ë°˜ì˜**í•´ì•¼ í•©ë‹ˆë‹¤.
   - ì‘ì„±ìê°€ ì…ë ¥í•œ ì„¤ëª…ì€ í•µì‹¬ ë™ì„ Â·ë°©í–¥Â·íŠ¹ì§•ì„ ìœ ì§€í•œ ì±„
     ë” ì´í•´í•˜ê¸° ì‰½ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
   - ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸¸, ê±´ë¬¼, ì¶œêµ¬, êµí†µí¸ì„ ì„ì˜ë¡œ ìƒì„± âŒ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[SEO í‚¤ì›Œë“œ ì‚¬ìš© ê°€ì´ë“œ]

ì•„ë˜ í‚¤ì›Œë“œëŠ” ë„¤ì´ë²„ ì§€ë„ ë° ì§€ì—­ ê²€ìƒ‰ ë…¸ì¶œì„ ê³ ë ¤í•´
ìì—°ìŠ¤ëŸ½ê³  ë¶„ì‚°ëœ ë¹ˆë„ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.

â–ª ì§€ì—­ í‚¤ì›Œë“œ:
  - ì´ **2~3íšŒ**
  - ë¬¸ì¥ ë„ì…ë¶€ ë˜ëŠ” ë§ˆë¬´ë¦¬ í¬í•¨

â–ª ëœë“œë§ˆí¬ í‚¤ì›Œë“œ:
  - ì´ **2~3íšŒ**
  - 'ì—­ì—ì„œ ì˜¤ëŠ” ë°©ë²•', 'ë„ë³´ ê¸°ì¤€', 'ì£¼ë³€ ê¸°ì¤€ë¬¼' ë¬¸ë§¥ì—ì„œ í™œìš©
  - ê¸¸ ì•ˆë‚´ì˜ ê¸°ì¤€ì (anchor) ì—­í• ë¡œ ì‚¬ìš©

â€» í‚¤ì›Œë“œë¥¼ ë‚˜ì—´í•˜ê±°ë‚˜ ë°˜ë³µ ì‚½ì…í•œ ëŠë‚Œ âŒ
â€» ì‹¤ì œ ê¸¸ ì•ˆë‚´ ë¬¸ì¥ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ì‚¬ìš©

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[êµ¬ì„± ë° ë‚´ìš© ê°€ì´ë“œ]

ë‹¤ìŒ ìš”ì†Œê°€ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ë˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.

1) ëŒ€í‘œì ì¸ ì ‘ê·¼ ê¸°ì¤€ (ì—­, ì£¼ìš” ê±´ë¬¼, ìƒê¶Œ ë“±)
2) ë„ë³´ ë˜ëŠ” ì´ë™ ì‹œ ì£¼ìš” ë™ì„  ì„¤ëª…
3) ì´ˆí–‰ìë„ í—·ê°ˆë¦¬ì§€ ì•Šë„ë¡ ë•ëŠ” ê¸°ì¤€ë¬¼
4) ë§ˆì§€ë§‰ ë„ì°© ì§€ì ì—ì„œì˜ ê°„ë‹¨í•œ ì•ˆë‚´

â€» ì§€ì‹œí˜• ë¬¸ì¥ë³´ë‹¤ëŠ”
   '~í•˜ì‹œë©´ ì°¾ê¸° ì‰½ìŠµë‹ˆë‹¤', '~ìª½ìœ¼ë¡œ ì˜¤ì‹œë©´ ë©ë‹ˆë‹¤'ì™€ ê°™ì€
   ì¹œì ˆí•œ ì•ˆë‚´ ë¬¸ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[í‘œí˜„ ë° ì¶œë ¥ ì œí•œ]

- ì œëª© âŒ
- ì†Œì œëª© âŒ
- ì´ëª¨ì§€ âŒ
- í™”ì‚´í‘œ(â†’), íŠ¹ìˆ˜ê¸°í˜¸ âŒ
- ê±°ë¦¬Â·ì‹œê°„ ê³¼ì¥ âŒ
- ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë³¸ë¬¸ë§Œ ì¶œë ¥"""

            user_message = f"""ë‹¤ìŒ [ì…ë ¥ ì •ë³´]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì°¾ì•„ì˜¤ëŠ” ê¸¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ì…ë ¥ ì •ë³´]
1. ì§€ì—­ í‚¤ì›Œë“œ: {request.region_keyword}
2. ëœë“œë§ˆí¬ í‚¤ì›Œë“œ: {', '.join(request.landmark_keywords) if request.landmark_keywords else 'ì—†ìŒ'}
3. ì‘ì„±ìê°€ ììœ ë¡­ê²Œ ì…ë ¥í•œ ì°¾ì•„ì˜¤ëŠ” ê¸¸ ì„¤ëª…: {request.directions_description}

ì°¾ì•„ì˜¤ëŠ”ê¸¸:"""
        
        else:
            # ê¸°ì¡´ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
            system_prompt = """ë‹¹ì‹ ì€ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì°¾ì•„ì˜¤ëŠ”ê¸¸ì„ ì‘ì„±í•  ë•Œ ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¥´ì„¸ìš”:

1. ëŒ€ì¤‘êµí†µ ì •ë³´ ìš°ì„  (ì§€í•˜ì² ì—­, ë²„ìŠ¤ ì •ë¥˜ì¥)
2. ë„ë³´ ì†Œìš” ì‹œê°„ ëª…ì‹œ
3. ì£¼ì°¨ ì •ë³´ í¬í•¨
4. ì£¼ìš” ëœë“œë§ˆí¬ í™œìš©
5. ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ë°©í–¥ ì•ˆë‚´
6. 150-250ì ê¶Œì¥

ì°¾ì•„ì˜¤ëŠ”ê¸¸ë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

            user_message = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì°¾ì•„ì˜¤ëŠ”ê¸¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

ë§¤ì¥ëª…: {store.get('store_name')}
ì£¼ì†Œ: {store.get('address', 'ì •ë³´ ì—†ìŒ')}
ë„ë¡œëª… ì£¼ì†Œ: {store.get('road_address', 'ì •ë³´ ì—†ìŒ')}

ì‚¬ìš©ì ìš”ì²­ì‚¬í•­:
{request.prompt}

ì°¾ì•„ì˜¤ëŠ”ê¸¸:"""

        # OpenAI API ì§ì ‘ í˜¸ì¶œ
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=1500
        )
        
        generated_text = response.choices[0].message.content
        logger.info(f"[ì°¾ì•„ì˜¤ëŠ”ê¸¸ ìƒì„±] ì™„ë£Œ: {len(generated_text)}ì")
        
        # ğŸ†• í¬ë ˆë”§ ì°¨ê° (ì„±ê³µ ì‹œì—ë§Œ)
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_AUTO_DEDUCT:
            try:
                transaction_id = await credit_service.deduct_credits(
                    user_id=user_id,
                    feature="directions",
                    credits_amount=3,
                    metadata={
                        "store_id": request.store_id,
                        "store_name": store.get("store_name"),
                        "text_length": len(generated_text)
                    }
                )
                logger.info(f"[Credits] Deducted 3 credits from user {user_id} (transaction: {transaction_id})")
            except Exception as credit_error:
                logger.error(f"[Credits] Failed to deduct credits: {credit_error}")
                # í¬ë ˆë”§ ì°¨ê° ì‹¤íŒ¨ëŠ” ê¸°ëŠ¥ ì‚¬ìš©ì„ ë§‰ì§€ ì•ŠìŒ (ì´ë¯¸ ìƒì„±ì€ ì™„ë£Œë¨)
        
        return GenerateTextResponse(
            status="success",
            generated_text=generated_text.strip()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ì°¾ì•„ì˜¤ëŠ”ê¸¸ ìƒì„±] ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì°¾ì•„ì˜¤ëŠ”ê¸¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/activation/history/{store_id}")
async def get_activation_history(
    store_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    í”Œë ˆì´ìŠ¤ í™œì„±í™” ê³¼ê±° ì´ë ¥ ì¡°íšŒ
    
    Args:
        store_id: ë§¤ì¥ ID (UUID)
        current_user: í˜„ì¬ ì‚¬ìš©ì ì •ë³´
        
    Returns:
        ê³¼ê±° í™œì„±í™” ì´ë ¥ ëª©ë¡ (ìµœì‹  10ê°œ)
    """
    try:
        user_id = current_user["id"]
        logger.info(f"[í™œì„±í™” ì´ë ¥] User {user_id} - Store {store_id} ì´ë ¥ ì¡°íšŒ ì‹œì‘")
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        supabase_client = get_supabase_client()
        
        # í•´ë‹¹ ë§¤ì¥ì˜ ì´ë ¥ ì¡°íšŒ (ìµœì‹ ìˆœ)
        result = supabase_client.table("activation_history")\
            .select("*")\
            .eq("user_id", user_id)\
            .eq("store_id", store_id)\
            .order("created_at", desc=True)\
            .limit(10)\
            .execute()
        
        histories = result.data if result.data else []
        logger.info(f"[í™œì„±í™” ì´ë ¥] {len(histories)}ê°œ ì´ë ¥ ì¡°íšŒ ì™„ë£Œ")
        
        return {
            "status": "success",
            "store_id": store_id,
            "histories": histories,
            "total_count": len(histories)
        }
        
    except Exception as e:
        logger.error(f"[í™œì„±í™” ì´ë ¥] ì¡°íšŒ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í™œì„±í™” ì´ë ¥ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
