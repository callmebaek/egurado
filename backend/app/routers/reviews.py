"""
ë¦¬ë·° ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
- ë¦¬ë·° ì¡°íšŒ ë° ë¶„ì„
- í†µê³„ ì €ì¥/ì¡°íšŒ
"""
import logging
import time
from datetime import datetime, date
from typing import List, Optional
from uuid import UUID
import pytz

from fastapi import APIRouter, HTTPException, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json
import asyncio

from app.services.naver_review_service import NaverReviewService
from app.services.review_sentiment_service import ReviewSentimentService
from app.core.database import get_supabase_client

# í•œêµ­ ì‹œê°„ëŒ€
KST = pytz.timezone('Asia/Seoul')

logger = logging.getLogger(__name__)

router = APIRouter()


# ============================================
# Pydantic ëª¨ë¸
# ============================================

class AnalyzeReviewsRequest(BaseModel):
    """ë¦¬ë·° ë¶„ì„ ìš”ì²­"""
    store_id: str
    start_date: Optional[str] = None  # YYYY-MM-DD, Noneì´ë©´ ì˜¤ëŠ˜
    end_date: Optional[str] = None    # YYYY-MM-DD, Noneì´ë©´ ì˜¤ëŠ˜


class ExtractReviewsRequest(BaseModel):
    """ë¦¬ë·° ì¶”ì¶œ ìš”ì²­ (ë¶„ì„ ì—†ì´)"""
    store_id: str
    start_date: Optional[str] = None  # YYYY-MM-DD, Noneì´ë©´ ì˜¤ëŠ˜
    end_date: Optional[str] = None    # YYYY-MM-DD, Noneì´ë©´ ì˜¤ëŠ˜


class ExtractedReviewResponse(BaseModel):
    """ì¶”ì¶œëœ ë¦¬ë·° ì‘ë‹µ"""
    naver_review_id: str
    review_type: str
    author_name: str
    rating: Optional[float]
    content: str
    review_date: str
    images: List[str]


class ExtractReviewsResponse(BaseModel):
    """ë¦¬ë·° ì¶”ì¶œ ì‘ë‹µ"""
    status: str
    store_id: str
    total_reviews: int
    reviews: List[ExtractedReviewResponse]
    start_date: str
    end_date: str


class ReviewStatsResponse(BaseModel):
    """ë¦¬ë·° í†µê³„ ì‘ë‹µ"""
    status: str
    store_id: str
    date: str
    checked_at: str
    
    # ë°©ë¬¸ì ë¦¬ë·° í†µê³„
    visitor_review_count: int
    visitor_positive_count: int
    visitor_neutral_count: int
    visitor_negative_count: int
    visitor_receipt_count: int
    visitor_reservation_count: int
    photo_review_count: int  # ì‚¬ì§„ í¬í•¨ ë¦¬ë·° ìˆ˜
    average_temperature: float  # í‰ê·  ë¦¬ë·° ì˜¨ë„
    
    # ë¸”ë¡œê·¸ ë¦¬ë·° í†µê³„
    blog_review_count: int
    
    # ìš”ì•½
    summary: str


class ReviewItemResponse(BaseModel):
    """ê°œë³„ ë¦¬ë·° ì‘ë‹µ"""
    id: str
    naver_review_id: str
    review_type: str
    author_name: str
    is_receipt_review: bool
    is_reservation_review: bool
    rating: Optional[float]
    content: str
    images: List[str]
    sentiment: str
    temperature_score: int
    confidence: float
    review_date: str
    like_count: int


# ============================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================

@router.post("/extract", response_model=ExtractReviewsResponse)
async def extract_reviews(request: ExtractReviewsRequest):
    """
    ë¦¬ë·° ì¶”ì¶œ (ë¶„ì„ ì—†ì´)
    
    ë¹ ë¥´ê²Œ ë¦¬ë·°ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´í›„ analyze-streamìœ¼ë¡œ ì‹¤ì‹œê°„ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.
    """
    print("=" * 80, flush=True)
    print("EXTRACT_REVIEWS FUNCTION CALLED!", flush=True)
    print("=" * 80, flush=True)
    try:
        print("1. try ë¸”ë¡ ì§„ì…", flush=True)
        store_id = request.store_id
        print(f"2. store_id = {store_id}", flush=True)
        kst_now = datetime.now(KST)
        today_str = kst_now.strftime("%Y-%m-%d")
        
        start_date_str = request.start_date or today_str
        end_date_str = request.end_date or today_str
        print(f"3. Period = {start_date_str} ~ {end_date_str}", flush=True)
        
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
        
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (ë””ë²„ê¹…ìš©)
        date_diff = (end_date - start_date).days + 1  # +1ì€ ì‹œì‘ì¼ í¬í•¨
        print(f"   -> Total days in range: {date_diff} days (including both start and end dates)", flush=True)
        print(f"   -> Today (KST): {today_str}", flush=True)
        
        print(f"4. ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹œì‘", flush=True)
        
        # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        supabase = get_supabase_client()
        print(f"5. Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ", flush=True)
        store_result = supabase.table("stores").select("*").eq("id", store_id).single().execute()
        print(f"6. ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì™„ë£Œ", flush=True)
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = store_result.data
        naver_place_id = store.get("place_id")
        print(f"7. naver_place_id = {naver_place_id}", flush=True)
        
        if not naver_place_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ë“±ë¡ë˜ì§€ ì•Šì€ ë§¤ì¥ì…ë‹ˆë‹¤"
            )
        
        # 2. ë„¤ì´ë²„ì—ì„œ ë¦¬ë·° ì¶”ì¶œ (ë¶„ì„ ì—†ì´)
        review_service = NaverReviewService()
        print(f"8. get_reviews_by_date_range í˜¸ì¶œ ì‹œì‘", flush=True)
        visitor_reviews = await review_service.get_reviews_by_date_range(
            naver_place_id,
            start_date,
            end_date
        )
        
        print(f"9. ë¦¬ë·° ì¶”ì¶œ ì™„ë£Œ: {len(visitor_reviews)}ê°œ", flush=True)
        
        # 3. ë¦¬ë·° ë°ì´í„° íŒŒì‹±
        parsed_reviews = []
        for review in visitor_reviews:
            parsed = review_service.parse_review_data(review, "visitor")
            parsed_reviews.append(ExtractedReviewResponse(
                naver_review_id=parsed["naver_review_id"],
                review_type=parsed["review_type"],
                author_name=parsed["author_name"],
                rating=parsed["rating"],
                content=parsed["content"],
                review_date=parsed["review_date"],
                images=parsed["images"]
            ))
        
        return ExtractReviewsResponse(
            status="success",
            store_id=store_id,
            total_reviews=len(parsed_reviews),
            reviews=parsed_reviews,
            start_date=start_date_str,
            end_date=end_date_str
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¦¬ë·° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¦¬ë·° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/analyze", response_model=ReviewStatsResponse)
async def analyze_store_reviews(request: AnalyzeReviewsRequest):
    """
    ë§¤ì¥ì˜ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì—¬ í†µê³„ ìƒì„±
    
    1. ë„¤ì´ë²„ì—ì„œ ë¦¬ë·° ì¡°íšŒ (ë°©ë¬¸ì + ë¸”ë¡œê·¸)
    2. OpenAIë¡œ ê°ì„± ë¶„ì„
    3. DBì— ì €ì¥ (ì¼ë³„ í†µê³„ + ê°œë³„ ë¦¬ë·°)
    4. í†µê³„ ë°˜í™˜
    """
    try:
        store_id = request.store_id
        # í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œ ê³„ì‚°
        kst_now = datetime.now(KST)
        today_str = kst_now.strftime("%Y-%m-%d")
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        start_date_str = request.start_date or today_str
        end_date_str = request.end_date or today_str
        
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
        
        logger.info(f"ë¦¬ë·° ë¶„ì„ ì‹œì‘: store_id={store_id}, ê¸°ê°„={start_date_str} ~ {end_date_str}")
        start_time = time.time()
        
        # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        supabase = get_supabase_client()
        store_result = supabase.table("stores").select("*").eq("id", store_id).single().execute()
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = store_result.data
        naver_place_id = store.get("place_id")
        category = store.get("category", "")
        
        if not naver_place_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ë“±ë¡ë˜ì§€ ì•Šì€ ë§¤ì¥ì…ë‹ˆë‹¤"
            )
        
        # 2. ë„¤ì´ë²„ ë¦¬ë·° ì¡°íšŒ
        step_start = time.time()
        review_service = NaverReviewService()
        
        # ë°©ë¬¸ì ë¦¬ë·° (ê¸°ê°„ ë‚´ ì‘ì„±ëœ ê²ƒ)
        visitor_reviews = await review_service.get_reviews_by_date_range(
            naver_place_id, 
            start_date,
            end_date
        )
        fetch_time = time.time() - step_start
        logger.info(f"â±ï¸ ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ ì™„ë£Œ: {len(visitor_reviews)}ê°œ (ì†Œìš”ì‹œê°„: {fetch_time:.2f}ì´ˆ)")
        
        # ë¸”ë¡œê·¸ ë¦¬ë·° (ì´ ê°œìˆ˜ë§Œ)
        blog_result = await review_service.get_blog_reviews(naver_place_id, page=1, size=1)
        blog_review_count = blog_result.get("total", 0)
        logger.info(f"ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜: {blog_review_count}ê°œ")
        
        # 3. ë°©ë¬¸ì ë¦¬ë·° íŒŒì‹±
        parsed_reviews = []
        for review in visitor_reviews:
            parsed = review_service.parse_review_data(review, "visitor")
            parsed_reviews.append(parsed)
        
        # 4. OpenAI ê°ì„± ë¶„ì„
        step_start = time.time()
        sentiment_service = ReviewSentimentService()
        analyzed_reviews = await sentiment_service.analyze_reviews_batch(
            parsed_reviews,
            context=category
        )
        analysis_time = time.time() - step_start
        logger.info(f"â±ï¸ ê°ì„± ë¶„ì„ ì™„ë£Œ: {len(analyzed_reviews)}ê°œ (ì†Œìš”ì‹œê°„: {analysis_time:.2f}ì´ˆ)")
        
        # 5. í†µê³„ ê³„ì‚°
        stats = {
            "positive": len([r for r in analyzed_reviews if r.get("sentiment") == "positive"]),
            "neutral": len([r for r in analyzed_reviews if r.get("sentiment") == "neutral"]),
            "negative": len([r for r in analyzed_reviews if r.get("sentiment") == "negative"]),
            "receipt": len([r for r in analyzed_reviews if r.get("is_receipt_review")]),
            "reservation": len([r for r in analyzed_reviews if r.get("is_reservation_review")])
        }
        
        # 6. ì¼ë³„ ìš”ì•½ ìƒì„±
        summary = await sentiment_service.generate_daily_summary(
            analyzed_reviews, 
            stats,
            start_date_str,
            end_date_str
        )
        
        # 7. DB ì €ì¥
        # 7-1. ê¸°ì¡´ í†µê³„ ì‚­ì œ (ê°™ì€ ì¡°íšŒì¼, ê°™ì€ ë§¤ì¥)
        today_date = datetime.now(KST).strftime("%Y-%m-%d")
        supabase.table("review_stats").delete().eq("store_id", store_id).eq("date", today_date).execute()
        
        # 7-2. í†µê³„ ì €ì¥
        stats_data = {
            "store_id": store_id,
            "date": today_date,  # ì¡°íšŒ ì‹œì  ë‚ ì§œ
            "visitor_review_count": len(analyzed_reviews),
            "visitor_positive_count": stats["positive"],
            "visitor_neutral_count": stats["neutral"],
            "visitor_negative_count": stats["negative"],
            "visitor_receipt_count": stats["receipt"],
            "visitor_reservation_count": stats["reservation"],
            "blog_review_count": blog_review_count,
            "summary": summary,
            "checked_at": datetime.now(KST).isoformat()
        }
        
        stats_insert_result = supabase.table("review_stats").insert(stats_data).execute()
        review_stats_id = stats_insert_result.data[0]["id"]
        logger.info(f"í†µê³„ ì €ì¥ ì™„ë£Œ: id={review_stats_id}")
        
        # 7-3. ê°œë³„ ë¦¬ë·° ì €ì¥
        for review in analyzed_reviews:
            review_data = {
                "store_id": store_id,
                "review_stats_id": review_stats_id,
                "naver_review_id": review.get("naver_review_id"),
                "review_type": review.get("review_type"),
                "author_name": review.get("author_name"),
                "author_id": review.get("author_id"),
                "author_review_count": review.get("author_review_count", 0),
                "is_receipt_review": review.get("is_receipt_review", False),
                "is_reservation_review": review.get("is_reservation_review", False),
                "rating": review.get("rating"),
                "content": review.get("content"),
                "images": review.get("images", []),
                "sentiment": review.get("sentiment"),
                "temperature_score": review.get("temperature_score"),
                "confidence": review.get("confidence"),
                "evidence_quotes": review.get("evidence_quotes", []),
                "aspect_sentiments": review.get("aspect_sentiments", {}),
                "review_date": review.get("review_date"),
                "like_count": review.get("like_count", 0),
                "comment_count": review.get("comment_count", 0)
            }
            
            # ì¤‘ë³µ ì²´í¬ í›„ ì‚½ì… (upsert)
            existing = supabase.table("reviews").select("id").eq(
                "naver_review_id", review_data["naver_review_id"]
            ).execute()
            
            if existing.data:
                # ì—…ë°ì´íŠ¸
                supabase.table("reviews").update(review_data).eq(
                    "id", existing.data[0]["id"]
                ).execute()
            else:
                # ì‚½ì…
                supabase.table("reviews").insert(review_data).execute()
        
        logger.info(f"ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {len(analyzed_reviews)}ê°œ")
        
        total_time = time.time() - start_time
        logger.info(f"â±ï¸ ì „ì²´ ë¶„ì„ ì™„ë£Œ: ì´ ì†Œìš”ì‹œê°„ {total_time:.2f}ì´ˆ (ë¦¬ë·° ì¡°íšŒ: {fetch_time:.2f}ì´ˆ, AI ë¶„ì„: {analysis_time:.2f}ì´ˆ)")
        
        # 8. ì‘ë‹µ ë°˜í™˜
        return ReviewStatsResponse(
            status="success",
            store_id=store_id,
            date=today_date,
            checked_at=datetime.now(KST).isoformat(),
            visitor_review_count=len(analyzed_reviews),
            visitor_positive_count=stats["positive"],
            visitor_neutral_count=stats["neutral"],
            visitor_negative_count=stats["negative"],
            visitor_receipt_count=stats["receipt"],
            visitor_reservation_count=stats["reservation"],
            blog_review_count=blog_review_count,
            summary=summary
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¦¬ë·° ë¶„ì„ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¦¬ë·° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/analyze-stream")
async def analyze_reviews_stream(store_id: str, start_date: str, end_date: str):
    """
    ë¦¬ë·° ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ (SSE)
    
    ì¶”ì¶œëœ ë¦¬ë·°ë¥¼ í•˜ë‚˜ì”© ë¶„ì„í•˜ë©´ì„œ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    async def event_generator():
        try:
            logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹œì‘: store_id={store_id}, ê¸°ê°„={start_date} ~ {end_date}")
            
            # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ
            supabase = get_supabase_client()
            store_result = supabase.table("stores").select("*").eq("id", store_id).single().execute()
            if not store_result.data:
                yield f"data: {json.dumps({'type': 'error', 'message': 'ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'})}\n\n"
                return
            
            store = store_result.data
            naver_place_id = store.get("place_id")
            category = store.get("category", "")
            
            if not naver_place_id:
                yield f"data: {json.dumps({'type': 'error', 'message': 'ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ì—†ìŠµë‹ˆë‹¤'})}\n\n"
                return
            
            # 2. ë¦¬ë·° ì¶”ì¶œ
            review_service = NaverReviewService()
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
            
            visitor_reviews = await review_service.get_reviews_by_date_range(
                naver_place_id,
                start_dt,
                end_dt
            )
            
            total_reviews = len(visitor_reviews)
            logger.info(f"ì¶”ì¶œëœ ë¦¬ë·° ìˆ˜: {total_reviews}ê°œ")
            
            # ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
            yield f"data: {json.dumps({'type': 'init', 'total': total_reviews})}\n\n"
            
            # 3. ë¦¬ë·° íŒŒì‹±
            parsed_reviews = []
            for review in visitor_reviews:
                parsed = review_service.parse_review_data(review, "visitor")
                parsed_reviews.append(parsed)
            
            # 4. í•˜ë‚˜ì”© ë¶„ì„ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
            sentiment_service = ReviewSentimentService()
            analyzed_reviews = []
            stats = {"positive": 0, "neutral": 0, "negative": 0}
            
            for idx, review in enumerate(parsed_reviews, 1):
                # ì§„í–‰ ìƒí™© ì „ì†¡
                yield f"data: {json.dumps({'type': 'progress', 'current': idx, 'total': total_reviews})}\n\n"
                
                # ë¹ˆ ë¦¬ë·° ì²˜ë¦¬
                if not review.get("content", "").strip():
                    # ë¹ˆ ë¦¬ë·°ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                    analyzed_review = {
                        **review,
                        "sentiment": "neutral",
                        "temperature_score": 50,
                        "confidence": 0.0,
                        "evidence_quotes": [],
                        "aspect_sentiments": {}
                    }
                    analyzed_reviews.append(analyzed_review)
                    stats["neutral"] = stats.get("neutral", 0) + 1
                    print(f"Empty review included as neutral (idx={idx}): naver_id={review.get('naver_review_id')}", flush=True)
                    
                    # ë¹ˆ ë¦¬ë·°ë„ ì „ì†¡
                    yield f"data: {json.dumps({
                        'type': 'review_analyzed',
                        'review': {
                            'id': review.get('naver_review_id'),
                            'author': review.get('author_name'),
                            'content': '(ë¹ˆ ë¦¬ë·°)',
                            'sentiment': 'neutral',
                            'temperature_score': 50,
                            'rating': review.get('rating')
                        }
                    })}\n\n"
                    
                    # í†µê³„ ì „ì†¡
                    yield f"data: {json.dumps({'type': 'stats_update', **stats})}\n\n"
                    
                    continue
                
                # ë¦¬ë·° ë¶„ì„
                try:
                    analysis = await sentiment_service.analyze_review(
                        review.get("content", ""),
                        review.get("rating"),
                        category
                    )
                    
                    # ë¶„ì„ ê²°ê³¼ ë³‘í•©
                    analyzed_review = {**review, **analysis}
                    analyzed_reviews.append(analyzed_review)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    sentiment = analysis.get("sentiment", "neutral")
                    stats[sentiment] = stats.get(sentiment, 0) + 1
                    
                    # ë¶„ì„ëœ ë¦¬ë·° ì „ì†¡
                    yield f"data: {json.dumps({
                        'type': 'review_analyzed',
                        'review': {
                            'id': review.get('naver_review_id'),
                            'author': review.get('author_name'),
                            'content': review.get('content')[:100] + '...' if len(review.get('content', '')) > 100 else review.get('content'),
                            'sentiment': sentiment,
                            'temperature_score': analysis.get('temperature_score'),
                            'rating': review.get('rating')
                        }
                    })}\n\n"
                    
                    # í†µê³„ ì „ì†¡
                    yield f"data: {json.dumps({'type': 'stats_update', **stats})}\n\n"
                    
                    # Rate limit íšŒí”¼ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    # ë¶„ì„ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                    print(f"Review analysis failed, included as neutral (idx={idx}): {str(e)}", flush=True)
                    analyzed_review = {
                        **review,
                        "sentiment": "neutral",
                        "temperature_score": 50,
                        "confidence": 0.0,
                        "evidence_quotes": [],
                        "aspect_sentiments": {}
                    }
                    analyzed_reviews.append(analyzed_review)
                    stats["neutral"] = stats.get("neutral", 0) + 1
                    
                    # ì‹¤íŒ¨í•œ ë¦¬ë·°ë„ ì „ì†¡
                    yield f"data: {json.dumps({
                        'type': 'review_analyzed',
                        'review': {
                            'id': review.get('naver_review_id'),
                            'author': review.get('author_name'),
                            'content': review.get('content', '')[:100] + '...' if len(review.get('content', '')) > 100 else review.get('content', ''),
                            'sentiment': 'neutral',
                            'temperature_score': 50,
                            'rating': review.get('rating')
                        }
                    })}\n\n"
                    
                    # í†µê³„ ì „ì†¡
                    yield f"data: {json.dumps({'type': 'stats_update', **stats})}\n\n"
            
            # 5. ìš”ì•½ ìƒì„±
            logger.info(f"ìš”ì•½ ìƒì„± ì‹œì‘...")
            summary = await sentiment_service.generate_daily_summary(
                analyzed_reviews,
                stats,
                start_date,
                end_date
            )
            
            # 6. DB ì €ì¥
            today_date = datetime.now(KST).strftime("%Y-%m-%d")
            supabase.table("review_stats").delete().eq("store_id", store_id).eq("date", today_date).execute()
            
            blog_result = await review_service.get_blog_reviews(naver_place_id, page=1, size=1)
            blog_review_count = blog_result.get("total", 0)
            
            # ì‚¬ì§„ í¬í•¨ ë¦¬ë·° ìˆ˜ ê³„ì‚°
            photo_review_count = sum(1 for r in analyzed_reviews if r.get("images") and len(r.get("images", [])) > 0)
            
            # í‰ê·  ë¦¬ë·° ì˜¨ë„ ê³„ì‚°
            temperature_scores = [r.get("temperature_score", 0) for r in analyzed_reviews if r.get("temperature_score") is not None]
            average_temperature = round(sum(temperature_scores) / len(temperature_scores), 1) if temperature_scores else 0.0
            
            stats_data = {
                "store_id": store_id,
                "date": today_date,
                "visitor_review_count": len(analyzed_reviews),
                "visitor_positive_count": stats["positive"],
                "visitor_neutral_count": stats["neutral"],
                "visitor_negative_count": stats["negative"],
                "visitor_receipt_count": sum(1 for r in analyzed_reviews if r.get("is_receipt_review")),
                "visitor_reservation_count": sum(1 for r in analyzed_reviews if r.get("is_reservation_review")),
                "photo_review_count": photo_review_count,
                "average_temperature": average_temperature,
                "blog_review_count": blog_review_count,
                "summary": summary,
                "checked_at": datetime.now(KST).isoformat()
            }
            
            stats_insert_result = supabase.table("review_stats").insert(stats_data).execute()
            review_stats_id = stats_insert_result.data[0]["id"]
            
            # ê°œë³„ ë¦¬ë·° ì €ì¥
            saved_count = 0
            failed_count = 0
            skipped_count = 0
            
            for idx, review in enumerate(analyzed_reviews, 1):
                try:
                    naver_review_id = review.get("naver_review_id")
                    
                    # ê¸°ì¡´ ë¦¬ë·° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    existing = supabase.table("reviews").select("id").eq("naver_review_id", naver_review_id).execute()
                    
                    if existing.data:
                        # ê¸°ì¡´ ë¦¬ë·°ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                        review_data = {
                            "store_id": store_id,
                            "review_stats_id": review_stats_id,
                            "review_type": review.get("review_type"),
                            "author_name": review.get("author_name"),
                            "author_id": review.get("author_id"),
                            "author_review_count": review.get("author_review_count", 0),
                            "is_receipt_review": review.get("is_receipt_review", False),
                            "is_reservation_review": review.get("is_reservation_review", False),
                            "rating": review.get("rating"),
                            "content": review.get("content"),
                            "images": review.get("images", []),
                            "sentiment": review.get("sentiment"),
                            "temperature_score": review.get("temperature_score"),
                            "confidence": review.get("confidence"),
                            "evidence_quotes": review.get("evidence_quotes", []),
                            "aspect_sentiments": review.get("aspect_sentiments", {}),
                            "review_date": review.get("review_date"),
                            "like_count": review.get("like_count", 0),
                            "comment_count": review.get("comment_count", 0),
                            "created_at": datetime.now(KST).isoformat()
                        }
                        supabase.table("reviews").update(review_data).eq("naver_review_id", naver_review_id).execute()
                        skipped_count += 1
                    else:
                        # ìƒˆ ë¦¬ë·° ì‚½ì…
                        review_data = {
                            "store_id": store_id,
                            "review_stats_id": review_stats_id,
                            "naver_review_id": naver_review_id,
                            "review_type": review.get("review_type"),
                            "author_name": review.get("author_name"),
                            "author_id": review.get("author_id"),
                            "author_review_count": review.get("author_review_count", 0),
                            "is_receipt_review": review.get("is_receipt_review", False),
                            "is_reservation_review": review.get("is_reservation_review", False),
                            "rating": review.get("rating"),
                            "content": review.get("content"),
                            "images": review.get("images", []),
                            "sentiment": review.get("sentiment"),
                            "temperature_score": review.get("temperature_score"),
                            "confidence": review.get("confidence"),
                            "evidence_quotes": review.get("evidence_quotes", []),
                            "aspect_sentiments": review.get("aspect_sentiments", {}),
                            "review_date": review.get("review_date"),
                            "like_count": review.get("like_count", 0),
                            "comment_count": review.get("comment_count", 0),
                            "created_at": datetime.now(KST).isoformat()
                        }
                        supabase.table("reviews").insert(review_data).execute()
                    
                    saved_count += 1
                except Exception as insert_error:
                    failed_count += 1
                    print(f"Review {idx}/{len(analyzed_reviews)} save failed - naver_id={review.get('naver_review_id')}: {str(insert_error)}", flush=True)
            
            print(f"Review save summary: {saved_count} saved ({skipped_count} updated), {failed_count} failed out of {len(analyzed_reviews)} total", flush=True)
            
            # 7. ì™„ë£Œ ì „ì†¡
            yield f"data: {json.dumps({
                'type': 'complete',
                'summary': summary,
                'total_analyzed': len(analyzed_reviews),
                'stats': stats
            })}\n\n"
            
            logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì™„ë£Œ: {len(analyzed_reviews)}ê°œ")
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        }
    )


@router.get("/stats/{store_id}", response_model=ReviewStatsResponse)
async def get_review_stats(store_id: str, date: Optional[str] = None):
    """
    ì €ì¥ëœ ë¦¬ë·° í†µê³„ ì¡°íšŒ
    
    Args:
        store_id: ë§¤ì¥ ID
        date: ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ìµœì‹ 
    """
    try:
        supabase = get_supabase_client()
        query = supabase.table("review_stats").select("*").eq("store_id", store_id)
        
        if date:
            query = query.eq("date", date)
        else:
            query = query.order("date", desc=True).limit(1)
        
        result = query.execute()
        
        if not result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë¦¬ë·° í†µê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        stats = result.data[0]
        
        return ReviewStatsResponse(
            status="success",
            store_id=stats["store_id"],
            date=stats["date"],
            checked_at=stats["checked_at"],
            visitor_review_count=stats["visitor_review_count"],
            visitor_positive_count=stats["visitor_positive_count"],
            visitor_neutral_count=stats["visitor_neutral_count"],
            visitor_negative_count=stats["visitor_negative_count"],
            visitor_receipt_count=stats["visitor_receipt_count"],
            visitor_reservation_count=stats["visitor_reservation_count"],
            photo_review_count=stats["photo_review_count"],
            average_temperature=stats.get("average_temperature", 0.0),
            blog_review_count=stats["blog_review_count"],
            summary=stats["summary"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )


@router.get("/list/{store_id}", response_model=List[ReviewItemResponse])
async def get_reviews_list(
    store_id: str,
    date: Optional[str] = None,
    sentiment: Optional[str] = None,  # positive, neutral, negative
    is_receipt: Optional[bool] = None,
    is_reservation: Optional[bool] = None
):
    """
    ê°œë³„ ë¦¬ë·° ëª©ë¡ ì¡°íšŒ (í•„í„° ì§€ì›)
    
    Args:
        store_id: ë§¤ì¥ ID
        date: ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ìµœì‹ 
        sentiment: ê°ì„± í•„í„°
        is_receipt: ì˜ìˆ˜ì¦ ë¦¬ë·° í•„í„°
        is_reservation: ì˜ˆì•½ì ë¦¬ë·° í•„í„°
    """
    try:
        supabase = get_supabase_client()
        
        # 1. í†µê³„ ID ì¡°íšŒ
        stats_query = supabase.table("review_stats").select("id").eq("store_id", store_id)
        
        if date:
            stats_query = stats_query.eq("date", date)
        else:
            stats_query = stats_query.order("date", desc=True).limit(1)
        
        stats_result = stats_query.execute()
        
        if not stats_result.data:
            return []
        
        review_stats_id = stats_result.data[0]["id"]
        
        # 2. ë¦¬ë·° ì¡°íšŒ
        query = supabase.table("reviews").select("*").eq("review_stats_id", review_stats_id)
        
        # í•„í„° ì ìš©
        if sentiment:
            query = query.eq("sentiment", sentiment)
        if is_receipt is not None:
            query = query.eq("is_receipt_review", is_receipt)
        if is_reservation is not None:
            query = query.eq("is_reservation_review", is_reservation)
        
        query = query.order("review_date", desc=True)
        
        result = query.execute()
        
        # 3. ì‘ë‹µ ë³€í™˜
        reviews = []
        for review in result.data:
            reviews.append(ReviewItemResponse(
                id=review["id"],
                naver_review_id=review["naver_review_id"],
                review_type=review["review_type"],
                author_name=review["author_name"],
                is_receipt_review=review["is_receipt_review"],
                is_reservation_review=review["is_reservation_review"],
                rating=review["rating"],
                content=review["content"],
                images=review["images"] or [],
                sentiment=review["sentiment"],
                temperature_score=review["temperature_score"],
                confidence=review["confidence"],
                review_date=review["review_date"],
                like_count=review["like_count"]
            ))
        
        logger.info(f"ë¦¬ë·° ëª©ë¡ ì¡°íšŒ ì™„ë£Œ: {len(reviews)}ê°œ")
        return reviews
        
    except Exception as e:
        logger.error(f"ë¦¬ë·° ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="ë¦¬ë·° ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )


@router.get("/place-info/{store_id}")
async def get_place_info(store_id: str):
    """
    íŠ¹ì • ë§¤ì¥ì˜ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì •ë³´ ì¡°íšŒ
    - ë§¤ì¥ëª…, ë°©ë¬¸ì ë¦¬ë·° ìˆ˜, ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜, í‰ì , í•œì¤„í‰
    """
    try:
        print(f"[DEBUG] ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹œì‘: store_id={store_id}", flush=True)
        logger.info(f"[DEBUG] ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹œì‘: store_id={store_id}")
        
        # Supabaseì—ì„œ ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        supabase = get_supabase_client()
        result = supabase.table("stores").select("*").eq("id", store_id).execute()
        
        print(f"[DEBUG] Supabase ì¡°íšŒ ê²°ê³¼: found={len(result.data) if result.data else 0} rows", flush=True)
        logger.info(f"[DEBUG] Supabase ì¡°íšŒ ê²°ê³¼: found={len(result.data) if result.data else 0} rows")
        if result.data:
            print(f"[DEBUG] ë§¤ì¥ ë°ì´í„°: {result.data[0]}", flush=True)
            logger.info(f"[DEBUG] ë§¤ì¥ ë°ì´í„°: {result.data[0]}")
        else:
            print(f"[DEBUG] Supabaseì—ì„œ í•´ë‹¹ store_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ", flush=True)
            logger.info(f"[DEBUG] Supabaseì—ì„œ í•´ë‹¹ store_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        if not result.data:
            print(f"[DEBUG] ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: store_id={store_id}", flush=True)
            logger.error(f"[DEBUG] ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: store_id={store_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = result.data[0]
        place_id = store.get("place_id")
        store_name = store.get("store_name", "") or store.get("name", "")  # store_name ë˜ëŠ” name ì»¬ëŸ¼
        business_type = store.get("business_type", "restaurant")  # ê¸°ë³¸ê°’ì€ restaurant
        
        print(f"[DEBUG] place_id={place_id}, store_name='{store_name}', business_type='{business_type}'", flush=True)
        logger.info(f"ğŸ“‹ ë§¤ì¥ ì •ë³´: id={store_id}, name='{store_name}', place_id={place_id}, business_type='{business_type}'")
        
        if not place_id:
            print(f"[DEBUG] place_idê°€ ì—†ì–´ì„œ 400 ì—ëŸ¬", flush=True)
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ë“±ë¡ë˜ì§€ ì•Šì€ ë§¤ì¥ì…ë‹ˆë‹¤"
            )
        
        # ë„¤ì´ë²„ APIì—ì„œ ë§¤ì¥ ì •ë³´ ì¡°íšŒ (ë§¤ì¥ëª…, ì¢Œí‘œ ì „ë‹¬)
        review_service = NaverReviewService()
        
        # ë§¤ì¥ëª…ì´ ì—†ìœ¼ë©´ ë¦¬ë·°ì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        if not store_name:
            print(f"[DEBUG] ë§¤ì¥ëª… ì—†ìŒ. ë¦¬ë·°ì—ì„œ ì¶”ì¶œ ì‹œë„", flush=True)
            logger.warning(f"âš ï¸ ë§¤ì¥ëª… ì—†ìŒ. ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì‹œë„: place_id={place_id}")
            try:
                visitor_result = await review_service.get_visitor_reviews(place_id, size=1, business_type=business_type)
                if visitor_result and visitor_result.get("items"):
                    store_name = visitor_result["items"][0].get("businessName", "")
                    print(f"[DEBUG] ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ: '{store_name}'", flush=True)
                    logger.info(f"âœ… ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ: '{store_name}'")
            except Exception as e:
                print(f"[DEBUG] ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}", flush=True)
                logger.error(f"ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        
        x = store.get("x")
        y = store.get("y")
        print(f"[DEBUG] get_place_info í˜¸ì¶œ ì „: place_id={place_id}, store_name='{store_name}', x={x}, y={y}", flush=True)
        place_info = await review_service.get_place_info(place_id, store_name, x, y)
        print(f"[DEBUG] get_place_info í˜¸ì¶œ í›„: place_info={place_info}", flush=True)
        
        if not place_info:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        # ë§¤ì¥ëª…ì´ ì—†ìœ¼ë©´ stores í…Œì´ë¸”ì˜ name ì‚¬ìš©
        if not place_info.get("name"):
            place_info["name"] = store.get("name", "")
        
        logger.info(f"ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {place_info}")
        return {
            "status": "success",
            "store_id": store_id,
            "place_info": place_info
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )
