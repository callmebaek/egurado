"""
ë¦¬ë·° ê´€ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
- ë¦¬ë·° ì¡°íšŒ ë° ë¶„ì„
- í†µê³„ ì €ì¥/ì¡°íšŒ
"""
import logging
import time
from datetime import datetime, date
from typing import List, Optional
from uuid import UUID
import pytz

from fastapi import APIRouter, HTTPException, status, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json
import asyncio

from app.services.naver_review_service import NaverReviewService
from app.services.review_sentiment_service import ReviewSentimentService
from app.routers.auth import get_current_user
from app.services.credit_service import credit_service
from app.core.config import settings
from app.core.database import get_supabase_client

# í•œêµ­ ì‹œê°„ëŒ€
KST = pytz.timezone('Asia/Seoul')

logger = logging.getLogger(__name__)

router = APIRouter()


# ============================================
# Pydantic ëª¨ë¸
# ============================================

class AnalyzeReviewsRequest(BaseModel):
    """ë¦¬ë·° ë¶„ì„ ìš”ì²­"""
    store_id: str
    start_date: Optional[str] = None  # YYYY-MM-DD, Noneì´ë©´ ì˜¤ëŠ˜
    end_date: Optional[str] = None    # YYYY-MM-DD, Noneì´ë©´ ì˜¤ëŠ˜


class ExtractReviewsRequest(BaseModel):
    """ë¦¬ë·° ì¶”ì¶œ ìš”ì²­ (ë¶„ì„ ì—†ì´)"""
    store_id: str
    start_date: Optional[str] = None  # YYYY-MM-DD, Noneì´ë©´ ì˜¤ëŠ˜
    end_date: Optional[str] = None    # YYYY-MM-DD, Noneì´ë©´ ì˜¤ëŠ˜


class ExtractedReviewResponse(BaseModel):
    """ì¶”ì¶œëœ ë¦¬ë·° ì‘ë‹µ"""
    naver_review_id: str
    review_type: str
    author_name: str
    rating: Optional[float]
    content: str
    review_date: str
    images: List[str]


class ExtractReviewsResponse(BaseModel):
    """ë¦¬ë·° ì¶”ì¶œ ì‘ë‹µ"""
    status: str
    store_id: str
    total_reviews: int
    reviews: List[ExtractedReviewResponse]
    start_date: str
    end_date: str


class ReviewStatsResponse(BaseModel):
    """ë¦¬ë·° í†µê³„ ì‘ë‹µ"""
    status: str
    store_id: str
    date: str
    checked_at: str
    
    # ë°©ë¬¸ì ë¦¬ë·° í†µê³„
    visitor_review_count: int
    visitor_positive_count: int
    visitor_neutral_count: int
    visitor_negative_count: int
    visitor_receipt_count: int
    visitor_reservation_count: int
    photo_review_count: int  # ì‚¬ì§„ í¬í•¨ ë¦¬ë·° ìˆ˜
    average_temperature: float  # í‰ê·  ë¦¬ë·° ì˜¨ë„
    
    # ë¸”ë¡œê·¸ ë¦¬ë·° í†µê³„
    blog_review_count: int
    
    # ìš”ì•½
    summary: str


class ReviewItemResponse(BaseModel):
    """ê°œë³„ ë¦¬ë·° ì‘ë‹µ"""
    id: str
    naver_review_id: str
    review_type: str
    author_name: str
    is_receipt_review: bool
    is_reservation_review: bool
    rating: Optional[float]
    content: str
    images: List[str]
    sentiment: str
    temperature_score: int
    confidence: float
    review_date: str
    like_count: int


# ============================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================

@router.post("/extract", response_model=ExtractReviewsResponse)
async def extract_reviews(
    request: ExtractReviewsRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ë¦¬ë·° ì¶”ì¶œ (ë¶„ì„ ì—†ì´)
    
    ë¹ ë¥´ê²Œ ë¦¬ë·°ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´í›„ analyze-streamìœ¼ë¡œ ì‹¤ì‹œê°„ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.
    
    í¬ë ˆë”§ ì²´í¬: ì´í›„ ë¶„ì„ì— í•„ìš”í•œ 10 í¬ë ˆë”§ì„ ë¯¸ë¦¬ ì²´í¬í•©ë‹ˆë‹¤.
    """
    print("=" * 80, flush=True)
    print("EXTRACT_REVIEWS FUNCTION CALLED!", flush=True)
    print("=" * 80, flush=True)
    try:
        print("1. try ë¸”ë¡ ì§„ì…", flush=True)
        store_id = request.store_id
        print(f"2. store_id = {store_id}", flush=True)
        kst_now = datetime.now(KST)
        today_str = kst_now.strftime("%Y-%m-%d")
        
        start_date_str = request.start_date or today_str
        end_date_str = request.end_date or today_str
        print(f"3. Period = {start_date_str} ~ {end_date_str}", flush=True)
        
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
        
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (ë””ë²„ê¹…ìš©)
        date_diff = (end_date - start_date).days + 1  # +1ì€ ì‹œì‘ì¼ í¬í•¨
        print(f"   -> Total days in range: {date_diff} days (including both start and end dates)", flush=True)
        print(f"   -> Today (KST): {today_str}", flush=True)
        
        print(f"4. ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹œì‘", flush=True)
        
        # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        supabase = get_supabase_client()
        print(f"5. Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ", flush=True)
        store_result = supabase.table("stores").select("*").eq("id", store_id).single().execute()
        print(f"6. ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì™„ë£Œ", flush=True)
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = store_result.data
        naver_place_id = store.get("place_id")
        print(f"7. naver_place_id = {naver_place_id}", flush=True)
        
        if not naver_place_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ë“±ë¡ë˜ì§€ ì•Šì€ ë§¤ì¥ì…ë‹ˆë‹¤"
            )
        
        # 2. ë„¤ì´ë²„ì—ì„œ ë¦¬ë·° ì¶”ì¶œ (ë¶„ì„ ì—†ì´)
        review_service = NaverReviewService()
        print(f"8. get_reviews_by_date_range í˜¸ì¶œ ì‹œì‘", flush=True)
        visitor_reviews = await review_service.get_reviews_by_date_range(
            naver_place_id,
            start_date,
            end_date
        )
        
        print(f"9. ë¦¬ë·° ì¶”ì¶œ ì™„ë£Œ: {len(visitor_reviews)}ê°œ", flush=True)
        
        # 3. ë¦¬ë·° ë°ì´í„° íŒŒì‹±
        parsed_reviews = []
        for review in visitor_reviews:
            parsed = review_service.parse_review_data(review, "visitor")
            parsed_reviews.append(ExtractedReviewResponse(
                naver_review_id=parsed["naver_review_id"],
                review_type=parsed["review_type"],
                author_name=parsed["author_name"],
                rating=parsed["rating"],
                content=parsed["content"],
                review_date=parsed["review_date"],
                images=parsed["images"]
            ))
        
        # 4. í¬ë ˆë”§ ì‚¬ì „ ì²´í¬ (ë¦¬ë·°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if len(parsed_reviews) > 0 and settings.CREDIT_SYSTEM_ENABLED:
            from app.services.credit_service import credit_service
            user_id = current_user.get("id")
            credit_check = await credit_service.check_sufficient_credits(
                user_id=user_id,
                feature="review_analysis",
                required_credits=10
            )
            if not credit_check.sufficient:
                logger.warning(f"[ë¦¬ë·° ë¶„ì„] í¬ë ˆë”§ ë¶€ì¡±: user_id={user_id}, required=10, available={credit_check.current_credits}")
                raise HTTPException(
                    status_code=402,
                    detail=f"í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. (í•„ìš”: 10 í¬ë ˆë”§, ë³´ìœ : {credit_check.current_credits} í¬ë ˆë”§)"
                )
        
        return ExtractReviewsResponse(
            status="success",
            store_id=store_id,
            total_reviews=len(parsed_reviews),
            reviews=parsed_reviews,
            start_date=start_date_str,
            end_date=end_date_str
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¦¬ë·° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¦¬ë·° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/analyze", response_model=ReviewStatsResponse)
async def analyze_store_reviews(
    request: AnalyzeReviewsRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ë§¤ì¥ì˜ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì—¬ í†µê³„ ìƒì„±
    
    1. ë„¤ì´ë²„ì—ì„œ ë¦¬ë·° ì¡°íšŒ (ë°©ë¬¸ì + ë¸”ë¡œê·¸)
    2. OpenAIë¡œ ê°ì„± ë¶„ì„
    3. DBì— ì €ì¥ (ì¼ë³„ í†µê³„ + ê°œë³„ ë¦¬ë·°)
    4. í†µê³„ ë°˜í™˜
    í¬ë ˆë”§: 30 í¬ë ˆë”§ ì†Œëª¨
    """
    user_id = UUID(current_user["id"])
    
    try:
        # ğŸ†• í¬ë ˆë”§ ì²´í¬ (Feature Flag í™•ì¸)
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_CHECK_STRICT:
            check_result = await credit_service.check_sufficient_credits(
                user_id=user_id,
                feature="review_analysis",
                required_credits=30
            )
            
            if not check_result.sufficient:
                logger.warning(f"[Credits] User {user_id} has insufficient credits for review analysis")
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail="í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í¬ë ˆë”§ì„ ì¶©ì „í•˜ê±°ë‚˜ í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”."
                )
            
            logger.info(f"[Credits] User {user_id} has sufficient credits for review analysis")
        
        store_id = request.store_id
        # í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œ ê³„ì‚°
        kst_now = datetime.now(KST)
        today_str = kst_now.strftime("%Y-%m-%d")
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        start_date_str = request.start_date or today_str
        end_date_str = request.end_date or today_str
        
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
        
        logger.info(f"ë¦¬ë·° ë¶„ì„ ì‹œì‘: store_id={store_id}, ê¸°ê°„={start_date_str} ~ {end_date_str}")
        start_time = time.time()
        
        # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        supabase = get_supabase_client()
        store_result = supabase.table("stores").select("*").eq("id", store_id).single().execute()
        if not store_result.data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = store_result.data
        naver_place_id = store.get("place_id")
        category = store.get("category", "")
        
        if not naver_place_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ë“±ë¡ë˜ì§€ ì•Šì€ ë§¤ì¥ì…ë‹ˆë‹¤"
            )
        
        # 2. ë„¤ì´ë²„ ë¦¬ë·° ì¡°íšŒ
        step_start = time.time()
        review_service = NaverReviewService()
        
        # ë°©ë¬¸ì ë¦¬ë·° (ê¸°ê°„ ë‚´ ì‘ì„±ëœ ê²ƒ)
        visitor_reviews = await review_service.get_reviews_by_date_range(
            naver_place_id, 
            start_date,
            end_date
        )
        fetch_time = time.time() - step_start
        logger.info(f"â±ï¸ ë°©ë¬¸ì ë¦¬ë·° ì¡°íšŒ ì™„ë£Œ: {len(visitor_reviews)}ê°œ (ì†Œìš”ì‹œê°„: {fetch_time:.2f}ì´ˆ)")
        
        # ë¸”ë¡œê·¸ ë¦¬ë·° (ì´ ê°œìˆ˜ë§Œ)
        blog_result = await review_service.get_blog_reviews(naver_place_id, page=1, size=1)
        blog_review_count = blog_result.get("total", 0)
        logger.info(f"ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜: {blog_review_count}ê°œ")
        
        # 3. ë°©ë¬¸ì ë¦¬ë·° íŒŒì‹±
        parsed_reviews = []
        for review in visitor_reviews:
            parsed = review_service.parse_review_data(review, "visitor")
            parsed_reviews.append(parsed)
        
        # 4. OpenAI ê°ì„± ë¶„ì„
        step_start = time.time()
        sentiment_service = ReviewSentimentService()
        analyzed_reviews = await sentiment_service.analyze_reviews_batch(
            parsed_reviews,
            context=category
        )
        analysis_time = time.time() - step_start
        logger.info(f"â±ï¸ ê°ì„± ë¶„ì„ ì™„ë£Œ: {len(analyzed_reviews)}ê°œ (ì†Œìš”ì‹œê°„: {analysis_time:.2f}ì´ˆ)")
        
        # 5. í†µê³„ ê³„ì‚°
        stats = {
            "positive": len([r for r in analyzed_reviews if r.get("sentiment") == "positive"]),
            "neutral": len([r for r in analyzed_reviews if r.get("sentiment") == "neutral"]),
            "negative": len([r for r in analyzed_reviews if r.get("sentiment") == "negative"]),
            "receipt": len([r for r in analyzed_reviews if r.get("is_receipt_review")]),
            "reservation": len([r for r in analyzed_reviews if r.get("is_reservation_review")])
        }
        
        # 6. ì¼ë³„ ìš”ì•½ ìƒì„±
        summary = await sentiment_service.generate_daily_summary(
            analyzed_reviews, 
            stats,
            start_date_str,
            end_date_str
        )
        
        # 7. DB ì €ì¥ (ë¶„ì„ ê¸°ê°„ì˜ ì¢…ë£Œì¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥)
        save_date = end_date_str  # ë¶„ì„í•œ ê¸°ê°„ì˜ ì¢…ë£Œì¼ ì‚¬ìš© (today_date ëŒ€ì‹ )
        logger.info(f"í†µê³„ ì €ì¥ ì‹œì‘: store_id={store_id}, date={save_date}")
        
        # 7-1. í†µê³„ ì €ì¥ (Optimistic Locking: INSERT ë¨¼ì € ì‹œë„, ì‹¤íŒ¨ ì‹œ UPDATE)
        normal_stats_data = {
            "store_id": store_id,
            "date": save_date,
            "visitor_review_count": len(analyzed_reviews),
            "visitor_positive_count": stats["positive"],
            "visitor_neutral_count": stats["neutral"],
            "visitor_negative_count": stats["negative"],
            "visitor_receipt_count": stats["receipt"],
            "visitor_reservation_count": stats["reservation"],
            "blog_review_count": blog_review_count,
            "summary": summary,
            "checked_at": datetime.now(KST).isoformat()
        }
        
        try:
            # ë¨¼ì € INSERT ì‹œë„
            stats_result = supabase.table("review_stats").insert(normal_stats_data).execute()
            review_stats_id = stats_result.data[0]["id"] if stats_result.data else None
            logger.info(f"âœ… ìƒˆ í†µê³„ ë°ì´í„° ì‚½ì… ì™„ë£Œ: id={review_stats_id}")
        except Exception as insert_error:
            error_msg = str(insert_error)
            # ì¤‘ë³µ í‚¤ ì—ëŸ¬ì¸ ê²½ìš°ë§Œ UPDATEë¡œ ì „í™˜
            if "duplicate key" in error_msg.lower() or "23505" in error_msg:
                logger.info(f"âš ï¸ ì¤‘ë³µ í‚¤ ê°ì§€, UPDATEë¡œ ì „í™˜")
                existing_stats = supabase.table("review_stats").select("id").eq("store_id", store_id).eq("date", save_date).execute()
                if existing_stats.data:
                    stats_result = supabase.table("review_stats").update(normal_stats_data).eq("store_id", store_id).eq("date", save_date).execute()
                    review_stats_id = existing_stats.data[0]["id"]
                    logger.info(f"âœ… í†µê³„ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: id={review_stats_id}")
                else:
                    raise Exception("ì¤‘ë³µ í‚¤ ì—ëŸ¬ ë°œìƒí–ˆì§€ë§Œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                # ë‹¤ë¥¸ ì—ëŸ¬ë©´ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
                raise
        
        logger.info(f"í†µê³„ ì €ì¥ ì™„ë£Œ: id={review_stats_id}, date={save_date}")
        
        # 7-3. ê°œë³„ ë¦¬ë·° ì €ì¥
        for review in analyzed_reviews:
            review_data = {
                "store_id": store_id,
                "review_stats_id": review_stats_id,
                "naver_review_id": review.get("naver_review_id"),
                "review_type": review.get("review_type"),
                "author_name": review.get("author_name"),
                "author_id": review.get("author_id"),
                "author_review_count": review.get("author_review_count", 0),
                "is_receipt_review": review.get("is_receipt_review", False),
                "is_reservation_review": review.get("is_reservation_review", False),
                "rating": review.get("rating"),
                "content": review.get("content"),
                "images": review.get("images", []),
                "sentiment": review.get("sentiment"),
                "temperature_score": review.get("temperature_score"),
                "confidence": review.get("confidence"),
                "evidence_quotes": review.get("evidence_quotes", []),
                "aspect_sentiments": review.get("aspect_sentiments", {}),
                "review_date": review.get("review_date"),
                "like_count": review.get("like_count", 0),
                "comment_count": review.get("comment_count", 0)
            }
            
            # ì¤‘ë³µ ì²´í¬ í›„ ì‚½ì… (upsert)
            existing = supabase.table("reviews").select("id").eq(
                "naver_review_id", review_data["naver_review_id"]
            ).execute()
            
            if existing.data:
                # ì—…ë°ì´íŠ¸
                supabase.table("reviews").update(review_data).eq(
                    "id", existing.data[0]["id"]
                ).execute()
            else:
                # ì‚½ì…
                supabase.table("reviews").insert(review_data).execute()
        
        logger.info(f"ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {len(analyzed_reviews)}ê°œ")
        
        total_time = time.time() - start_time
        logger.info(f"â±ï¸ ì „ì²´ ë¶„ì„ ì™„ë£Œ: ì´ ì†Œìš”ì‹œê°„ {total_time:.2f}ì´ˆ (ë¦¬ë·° ì¡°íšŒ: {fetch_time:.2f}ì´ˆ, AI ë¶„ì„: {analysis_time:.2f}ì´ˆ)")
        
        # ğŸ†• í¬ë ˆë”§ ì°¨ê° (ì„±ê³µ ì‹œ)
        if settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_AUTO_DEDUCT:
            try:
                transaction_id = await credit_service.deduct_credits(
                    user_id=user_id,
                    feature="review_analysis",
                    credits_amount=30,
                    metadata={
                        "store_id": store_id,
                        "start_date": start_date_str,
                        "end_date": end_date_str,
                        "review_count": len(analyzed_reviews)
                    }
                )
                logger.info(f"[Credits] Deducted 30 credits from user {user_id} (transaction: {transaction_id})")
            except Exception as credit_error:
                logger.error(f"[Credits] Failed to deduct credits: {credit_error}")
        
        # 8. ì‘ë‹µ ë°˜í™˜
        return ReviewStatsResponse(
            status="success",
            store_id=store_id,
            date=save_date,  # ì €ì¥ëœ ë‚ ì§œ ë°˜í™˜
            checked_at=datetime.now(KST).isoformat(),
            visitor_review_count=len(analyzed_reviews),
            visitor_positive_count=stats["positive"],
            visitor_neutral_count=stats["neutral"],
            visitor_negative_count=stats["negative"],
            visitor_receipt_count=stats["receipt"],
            visitor_reservation_count=stats["reservation"],
            blog_review_count=blog_review_count,
            summary=summary
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¦¬ë·° ë¶„ì„ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¦¬ë·° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/analyze-stream")
async def analyze_reviews_stream(
    store_id: str,
    start_date: str,
    end_date: str,
    token: Optional[str] = None
):
    """
    ë¦¬ë·° ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ (SSE)
    
    ì¶”ì¶œëœ ë¦¬ë·°ë¥¼ í•˜ë‚˜ì”© ë¶„ì„í•˜ë©´ì„œ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    í¬ë ˆë”§: 30 í¬ë ˆë”§ ì†Œëª¨
    
    Note: SSEëŠ” ì»¤ìŠ¤í…€ í—¤ë”ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í† í°ì„ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŠµë‹ˆë‹¤.
    """
    # í† í°ìœ¼ë¡œ ì‚¬ìš©ì ì¸ì¦
    user_id = None
    if token:
        try:
            from app.routers.auth import decode_access_token
            payload = decode_access_token(token)
            if payload:
                user_id = UUID(payload.get("user_id"))
                logger.info(f"[SSE Auth] User authenticated: {user_id}")
        except Exception as e:
            logger.warning(f"[SSE Auth] Token validation failed: {e}")
    
    # ğŸ†• í¬ë ˆë”§ ì²´í¬ (Feature Flag í™•ì¸) - user_idê°€ ìˆì„ ë•Œë§Œ
    if user_id and settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_CHECK_STRICT:
        check_result = await credit_service.check_sufficient_credits(
            user_id=user_id,
            feature="review_analysis",
            required_credits=30
        )
        
        if not check_result.sufficient:
            logger.warning(f"[Credits] User {user_id} has insufficient credits for review analysis (stream)")
            raise HTTPException(
                status_code=status.HTTP_402_PAYMENT_REQUIRED,
                detail="í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í¬ë ˆë”§ì„ ì¶©ì „í•˜ê±°ë‚˜ í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•´ì£¼ì„¸ìš”."
            )
        
        logger.info(f"[Credits] User {user_id} has sufficient credits for review analysis (stream)")
    
    async def event_generator():
        try:
            logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹œì‘: store_id={store_id}, ê¸°ê°„={start_date} ~ {end_date}")
            
            # 1. ë§¤ì¥ ì •ë³´ ì¡°íšŒ
            supabase = get_supabase_client()
            store_result = supabase.table("stores").select("*").eq("id", store_id).single().execute()
            if not store_result.data:
                yield f"data: {json.dumps({'type': 'error', 'message': 'ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'})}\n\n"
                return
            
            store = store_result.data
            naver_place_id = store.get("place_id")
            category = store.get("category", "")
            
            if not naver_place_id:
                yield f"data: {json.dumps({'type': 'error', 'message': 'ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ì—†ìŠµë‹ˆë‹¤'})}\n\n"
                return
            
            # 2. ë¦¬ë·° ì¶”ì¶œ
            review_service = NaverReviewService()
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
            
            visitor_reviews = await review_service.get_reviews_by_date_range(
                naver_place_id,
                start_dt,
                end_dt
            )
            
            total_reviews = len(visitor_reviews)
            logger.info(f"ì¶”ì¶œëœ ë¦¬ë·° ìˆ˜: {total_reviews}ê°œ")
            
            # ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
            yield f"data: {json.dumps({'type': 'init', 'total': total_reviews})}\n\n"
            
            # 3. ë¦¬ë·° íŒŒì‹±
            parsed_reviews = []
            for review in visitor_reviews:
                parsed = review_service.parse_review_data(review, "visitor")
                parsed_reviews.append(parsed)
            
            # 4. í•˜ë‚˜ì”© ë¶„ì„ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
            sentiment_service = ReviewSentimentService()
            analyzed_reviews = []
            stats = {"positive": 0, "neutral": 0, "negative": 0}
            
            for idx, review in enumerate(parsed_reviews, 1):
                # ì§„í–‰ ìƒí™© ì „ì†¡
                yield f"data: {json.dumps({'type': 'progress', 'current': idx, 'total': total_reviews})}\n\n"
                
                # ë¹ˆ ë¦¬ë·° ì²˜ë¦¬
                if not review.get("content", "").strip():
                    # ë¹ˆ ë¦¬ë·°ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                    analyzed_review = {
                        **review,
                        "sentiment": "neutral",
                        "temperature_score": 50,
                        "confidence": 0.0,
                        "evidence_quotes": [],
                        "aspect_sentiments": {}
                    }
                    analyzed_reviews.append(analyzed_review)
                    stats["neutral"] = stats.get("neutral", 0) + 1
                    print(f"Empty review included as neutral (idx={idx}): naver_id={review.get('naver_review_id')}", flush=True)
                    
                    # ë¹ˆ ë¦¬ë·°ë„ ì „ì†¡
                    empty_review_data = {
                        'type': 'review_analyzed',
                        'review': {
                            'id': review.get('naver_review_id'),
                            'author': review.get('author_name'),
                            'content': '(ë¹ˆ ë¦¬ë·°)',
                            'sentiment': 'neutral',
                            'temperature_score': 50,
                            'rating': review.get('rating')
                        }
                    }
                    yield f"data: {json.dumps(empty_review_data)}\n\n"
                    
                    # í†µê³„ ì „ì†¡
                    yield f"data: {json.dumps({'type': 'stats_update', **stats})}\n\n"
                    
                    continue
                
                # ë¦¬ë·° ë¶„ì„
                try:
                    analysis = await sentiment_service.analyze_review(
                        review.get("content", ""),
                        review.get("rating"),
                        category
                    )
                    
                    # ë¶„ì„ ê²°ê³¼ ë³‘í•©
                    analyzed_review = {**review, **analysis}
                    analyzed_reviews.append(analyzed_review)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    sentiment = analysis.get("sentiment", "neutral")
                    stats[sentiment] = stats.get(sentiment, 0) + 1
                    
                    # ë¶„ì„ëœ ë¦¬ë·° ì „ì†¡
                    review_data = {
                        'type': 'review_analyzed',
                        'review': {
                            'id': review.get('naver_review_id'),
                            'author': review.get('author_name'),
                            'content': review.get('content')[:100] + '...' if len(review.get('content', '')) > 100 else review.get('content'),
                            'sentiment': sentiment,
                            'temperature_score': analysis.get('temperature_score'),
                            'rating': review.get('rating')
                        }
                    }
                    yield f"data: {json.dumps(review_data)}\n\n"
                    
                    # í†µê³„ ì „ì†¡
                    yield f"data: {json.dumps({'type': 'stats_update', **stats})}\n\n"
                    
                    # Rate limit íšŒí”¼ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    # ë¶„ì„ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                    print(f"Review analysis failed, included as neutral (idx={idx}): {str(e)}", flush=True)
                    analyzed_review = {
                        **review,
                        "sentiment": "neutral",
                        "temperature_score": 50,
                        "confidence": 0.0,
                        "evidence_quotes": [],
                        "aspect_sentiments": {}
                    }
                    analyzed_reviews.append(analyzed_review)
                    stats["neutral"] = stats.get("neutral", 0) + 1
                    
                    # ì‹¤íŒ¨í•œ ë¦¬ë·°ë„ ì „ì†¡
                    failed_review_data = {
                        'type': 'review_analyzed',
                        'review': {
                            'id': review.get('naver_review_id'),
                            'author': review.get('author_name'),
                            'content': review.get('content', '')[:100] + '...' if len(review.get('content', '')) > 100 else review.get('content', ''),
                            'sentiment': 'neutral',
                            'temperature_score': 50,
                            'rating': review.get('rating')
                        }
                    }
                    yield f"data: {json.dumps(failed_review_data)}\n\n"
                    
                    # í†µê³„ ì „ì†¡
                    yield f"data: {json.dumps({'type': 'stats_update', **stats})}\n\n"
            
            # 5. ìš”ì•½ ìƒì„±
            logger.info(f"ìš”ì•½ ìƒì„± ì‹œì‘...")
            try:
                summary = await sentiment_service.generate_daily_summary(
                    analyzed_reviews,
                    stats,
                    start_date,
                    end_date
                )
                logger.info(f"ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(summary)}ì")
            except Exception as summary_error:
                logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(summary_error)}", exc_info=True)
                summary = f"{period_text} ë™ì•ˆ ì´ {len(analyzed_reviews)}ê°œì˜ ë¦¬ë·°ê°€ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. " \
                          f"ê¸ì • {stats['positive']}ê°œ, ì¤‘ë¦½ {stats['neutral']}ê°œ, ë¶€ì • {stats['negative']}ê°œì…ë‹ˆë‹¤."
                logger.info(f"ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©")
            
            # 6. DB ì €ì¥ (ë¶„ì„í•œ ê¸°ê°„ì˜ ì¢…ë£Œì¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥)
            save_date = end_date  # ë¶„ì„í•œ ê¸°ê°„ì˜ ì¢…ë£Œì¼ ì‚¬ìš©
            logger.info(f"ì €ì¥í•  ë‚ ì§œ: {save_date}")
            
            blog_result = await review_service.get_blog_reviews(naver_place_id, page=1, size=1)
            blog_review_count = blog_result.get("total", 0)
            
            # ì‚¬ì§„ í¬í•¨ ë¦¬ë·° ìˆ˜ ê³„ì‚°
            photo_review_count = sum(1 for r in analyzed_reviews if r.get("images") and len(r.get("images", [])) > 0)
            
            # í‰ê·  ë¦¬ë·° ì˜¨ë„ ê³„ì‚°
            temperature_scores = [r.get("temperature_score", 0) for r in analyzed_reviews if r.get("temperature_score") is not None]
            average_temperature = round(sum(temperature_scores) / len(temperature_scores), 1) if temperature_scores else 0.0
            
            # í†µê³„ ë°ì´í„° ì €ì¥ (Optimistic Locking: INSERT ë¨¼ì € ì‹œë„, ì‹¤íŒ¨ ì‹œ UPDATE)
            stats_data = {
                "store_id": store_id,
                "date": save_date,
                "visitor_review_count": len(analyzed_reviews),
                "visitor_positive_count": stats["positive"],
                "visitor_neutral_count": stats["neutral"],
                "visitor_negative_count": stats["negative"],
                "visitor_receipt_count": sum(1 for r in analyzed_reviews if r.get("is_receipt_review")),
                "visitor_reservation_count": sum(1 for r in analyzed_reviews if r.get("is_reservation_review")),
                "photo_review_count": photo_review_count,
                "average_temperature": average_temperature,
                "blog_review_count": blog_review_count,
                "summary": summary,
                "checked_at": datetime.now(KST).isoformat()
            }
            
            logger.info(f"í†µê³„ ë°ì´í„° ì €ì¥ ì‹œì‘: store_id={store_id}, date={save_date}")
            
            try:
                # ë¨¼ì € INSERT ì‹œë„ (ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ëŠ” ì´ê²ƒìœ¼ë¡œ ë)
                stats_result = supabase.table("review_stats").insert(stats_data).execute()
                review_stats_id = stats_result.data[0]["id"] if stats_result.data else None
                logger.info(f"âœ… ìƒˆ í†µê³„ ë°ì´í„° ì‚½ì… ì™„ë£Œ: id={review_stats_id}")
            except Exception as insert_error:
                error_msg = str(insert_error)
                # ì¤‘ë³µ í‚¤ ì—ëŸ¬ì¸ ê²½ìš°ë§Œ UPDATEë¡œ ì „í™˜ (Race Condition ì²˜ë¦¬)
                if "duplicate key" in error_msg.lower() or "23505" in error_msg:
                    logger.info(f"âš ï¸ ì¤‘ë³µ í‚¤ ê°ì§€, UPDATEë¡œ ì „í™˜")
                    existing_stats = supabase.table("review_stats").select("id").eq("store_id", store_id).eq("date", save_date).execute()
                    if existing_stats.data:
                        stats_result = supabase.table("review_stats").update(stats_data).eq("store_id", store_id).eq("date", save_date).execute()
                        review_stats_id = existing_stats.data[0]["id"]
                        logger.info(f"âœ… í†µê³„ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: id={review_stats_id}")
                    else:
                        raise Exception("ì¤‘ë³µ í‚¤ ì—ëŸ¬ ë°œìƒí–ˆì§€ë§Œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                else:
                    # ë‹¤ë¥¸ ì—ëŸ¬ë©´ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
                    raise
            
            logger.info(f"í†µê³„ ë°ì´í„° ì €ì¥ ì™„ë£Œ: review_stats_id={review_stats_id}")
            print(f"[DEBUG] review_stats_id={review_stats_id}, ë¦¬ë·° ì €ì¥ ì‹œì‘: {len(analyzed_reviews)}ê°œ", flush=True)
            
            # ê°œë³„ ë¦¬ë·° ì €ì¥
            saved_count = 0
            failed_count = 0
            skipped_count = 0
            
            for idx, review in enumerate(analyzed_reviews, 1):
                try:
                    naver_review_id = review.get("naver_review_id")
                    
                    # ê¸°ì¡´ ë¦¬ë·° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    existing = supabase.table("reviews").select("id").eq("naver_review_id", naver_review_id).execute()
                    
                    if existing.data:
                        # ê¸°ì¡´ ë¦¬ë·°ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                        review_data = {
                            "store_id": store_id,
                            "review_stats_id": review_stats_id,
                            "review_type": review.get("review_type"),
                            "author_name": review.get("author_name"),
                            "author_id": review.get("author_id"),
                            "author_review_count": review.get("author_review_count", 0),
                            "is_receipt_review": review.get("is_receipt_review", False),
                            "is_reservation_review": review.get("is_reservation_review", False),
                            "rating": review.get("rating"),
                            "content": review.get("content"),
                            "images": review.get("images", []),
                            "sentiment": review.get("sentiment"),
                            "temperature_score": review.get("temperature_score"),
                            "confidence": review.get("confidence"),
                            "evidence_quotes": review.get("evidence_quotes", []),
                            "aspect_sentiments": review.get("aspect_sentiments", {}),
                            "review_date": review.get("review_date"),
                            "like_count": review.get("like_count", 0),
                            "comment_count": review.get("comment_count", 0),
                            "created_at": datetime.now(KST).isoformat()
                        }
                        supabase.table("reviews").update(review_data).eq("naver_review_id", naver_review_id).execute()
                        skipped_count += 1
                    else:
                        # ìƒˆ ë¦¬ë·° ì‚½ì…
                        review_data = {
                            "store_id": store_id,
                            "review_stats_id": review_stats_id,
                            "naver_review_id": naver_review_id,
                            "review_type": review.get("review_type"),
                            "author_name": review.get("author_name"),
                            "author_id": review.get("author_id"),
                            "author_review_count": review.get("author_review_count", 0),
                            "is_receipt_review": review.get("is_receipt_review", False),
                            "is_reservation_review": review.get("is_reservation_review", False),
                            "rating": review.get("rating"),
                            "content": review.get("content"),
                            "images": review.get("images", []),
                            "sentiment": review.get("sentiment"),
                            "temperature_score": review.get("temperature_score"),
                            "confidence": review.get("confidence"),
                            "evidence_quotes": review.get("evidence_quotes", []),
                            "aspect_sentiments": review.get("aspect_sentiments", {}),
                            "review_date": review.get("review_date"),
                            "like_count": review.get("like_count", 0),
                            "comment_count": review.get("comment_count", 0),
                            "created_at": datetime.now(KST).isoformat()
                        }
                        print(f"[DEBUG] ë¦¬ë·° INSERT ì‹œë„: naver_id={naver_review_id}, review_stats_id={review_stats_id}", flush=True)
                        supabase.table("reviews").insert(review_data).execute()
                        print(f"[DEBUG] ë¦¬ë·° INSERT ì„±ê³µ: {idx}/{len(analyzed_reviews)}", flush=True)
                    
                    saved_count += 1
                except Exception as insert_error:
                    failed_count += 1
                    print(f"Review {idx}/{len(analyzed_reviews)} save failed - naver_id={review.get('naver_review_id')}: {str(insert_error)}", flush=True)
            
            print(f"Review save summary: {saved_count} saved ({skipped_count} updated), {failed_count} failed out of {len(analyzed_reviews)} total", flush=True)
            
            # ğŸ†• í¬ë ˆë”§ ì°¨ê° (ì„±ê³µ ì‹œ) - user_idê°€ ìˆì„ ë•Œë§Œ
            if user_id and settings.CREDIT_SYSTEM_ENABLED and settings.CREDIT_AUTO_DEDUCT:
                try:
                    transaction_id = await credit_service.deduct_credits(
                        user_id=user_id,
                        feature="review_analysis",
                        credits_amount=30,
                        metadata={
                            "store_id": store_id,
                            "start_date": start_date,
                            "end_date": end_date,
                            "review_count": len(analyzed_reviews)
                        }
                    )
                    logger.info(f"[Credits] Deducted 30 credits from user {user_id} (transaction: {transaction_id})")
                except Exception as credit_error:
                    logger.error(f"[Credits] Failed to deduct credits: {credit_error}")
            
            # 7. ì™„ë£Œ ì „ì†¡
            complete_data = {
                'type': 'complete',
                'summary': summary,
                'total_analyzed': len(analyzed_reviews),
                'stats': stats,
                'saved_date': save_date  # ì €ì¥ëœ ë‚ ì§œ ì „ë‹¬
            }
            yield f"data: {json.dumps(complete_data)}\n\n"
            
            logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì™„ë£Œ: {len(analyzed_reviews)}ê°œ, ì €ì¥ ë‚ ì§œ: {save_date}")
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        }
    )


@router.get("/stats/{store_id}", response_model=ReviewStatsResponse)
async def get_review_stats(store_id: str, date: Optional[str] = None):
    """
    ì €ì¥ëœ ë¦¬ë·° í†µê³„ ì¡°íšŒ
    
    Args:
        store_id: ë§¤ì¥ ID
        date: ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ìµœì‹ 
    """
    try:
        supabase = get_supabase_client()
        logger.info(f"í†µê³„ ì¡°íšŒ ìš”ì²­: store_id={store_id}, date={date}")
        
        query = supabase.table("review_stats").select("*").eq("store_id", store_id)
        
        if date:
            query = query.eq("date", date)
        else:
            query = query.order("date", desc=True).limit(1)
        
        result = query.execute()
        logger.info(f"í†µê³„ ì¡°íšŒ ê²°ê³¼: {len(result.data) if result.data else 0}ê°œ")
        
        if result.data:
            logger.info(f"ì¡°íšŒëœ ë°ì´í„° ë‚ ì§œ: {[r['date'] for r in result.data]}")
        
        if not result.data:
            # ë””ë²„ê¹…: í•´ë‹¹ ë§¤ì¥ì˜ ëª¨ë“  í†µê³„ ë‚ ì§œ ì¡°íšŒ
            all_dates = supabase.table("review_stats").select("date").eq("store_id", store_id).execute()
            available_dates = [r['date'] for r in all_dates.data] if all_dates.data else []
            logger.error(f"í†µê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ìš”ì²­ ë‚ ì§œ: {date}, ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ: {available_dates}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë¦¬ë·° í†µê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ìš”ì²­: {date}, ê°€ëŠ¥: {available_dates})"
            )
        
        stats = result.data[0]
        
        return ReviewStatsResponse(
            status="success",
            store_id=stats["store_id"],
            date=stats["date"],
            checked_at=stats["checked_at"],
            visitor_review_count=stats["visitor_review_count"],
            visitor_positive_count=stats["visitor_positive_count"],
            visitor_neutral_count=stats["visitor_neutral_count"],
            visitor_negative_count=stats["visitor_negative_count"],
            visitor_receipt_count=stats["visitor_receipt_count"],
            visitor_reservation_count=stats["visitor_reservation_count"],
            photo_review_count=stats["photo_review_count"],
            average_temperature=stats.get("average_temperature", 0.0),
            blog_review_count=stats["blog_review_count"],
            summary=stats["summary"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )


@router.get("/list/{store_id}", response_model=List[ReviewItemResponse])
async def get_reviews_list(
    store_id: str,
    date: Optional[str] = None,
    sentiment: Optional[str] = None,  # positive, neutral, negative
    is_receipt: Optional[bool] = None,
    is_reservation: Optional[bool] = None
):
    """
    ê°œë³„ ë¦¬ë·° ëª©ë¡ ì¡°íšŒ (í•„í„° ì§€ì›)
    
    Args:
        store_id: ë§¤ì¥ ID
        date: ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ìµœì‹ 
        sentiment: ê°ì„± í•„í„°
        is_receipt: ì˜ìˆ˜ì¦ ë¦¬ë·° í•„í„°
        is_reservation: ì˜ˆì•½ì ë¦¬ë·° í•„í„°
    """
    try:
        supabase = get_supabase_client()
        
        # 1. í†µê³„ ID ì¡°íšŒ
        stats_query = supabase.table("review_stats").select("id").eq("store_id", store_id)
        
        if date:
            stats_query = stats_query.eq("date", date)
        else:
            stats_query = stats_query.order("date", desc=True).limit(1)
        
        stats_result = stats_query.execute()
        
        if not stats_result.data:
            return []
        
        review_stats_id = stats_result.data[0]["id"]
        
        # 2. ë¦¬ë·° ì¡°íšŒ
        query = supabase.table("reviews").select("*").eq("review_stats_id", review_stats_id)
        
        # í•„í„° ì ìš©
        if sentiment:
            query = query.eq("sentiment", sentiment)
        if is_receipt is not None:
            query = query.eq("is_receipt_review", is_receipt)
        if is_reservation is not None:
            query = query.eq("is_reservation_review", is_reservation)
        
        query = query.order("review_date", desc=True)
        
        result = query.execute()
        
        # 3. ì‘ë‹µ ë³€í™˜
        reviews = []
        for review in result.data:
            reviews.append(ReviewItemResponse(
                id=review["id"],
                naver_review_id=review["naver_review_id"],
                review_type=review["review_type"],
                author_name=review["author_name"],
                is_receipt_review=review["is_receipt_review"],
                is_reservation_review=review["is_reservation_review"],
                rating=review["rating"],
                content=review["content"],
                images=review["images"] or [],
                sentiment=review["sentiment"],
                temperature_score=review["temperature_score"],
                confidence=review["confidence"],
                review_date=review["review_date"],
                like_count=review["like_count"]
            ))
        
        logger.info(f"ë¦¬ë·° ëª©ë¡ ì¡°íšŒ ì™„ë£Œ: {len(reviews)}ê°œ")
        return reviews
        
    except Exception as e:
        logger.error(f"ë¦¬ë·° ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="ë¦¬ë·° ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )


@router.get("/place-info/{store_id}")
async def get_place_info(store_id: str):
    """
    íŠ¹ì • ë§¤ì¥ì˜ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì •ë³´ ì¡°íšŒ
    - ë§¤ì¥ëª…, ë°©ë¬¸ì ë¦¬ë·° ìˆ˜, ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜, í‰ì , í•œì¤„í‰
    """
    try:
        print(f"[DEBUG] ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹œì‘: store_id={store_id}", flush=True)
        logger.info(f"[DEBUG] ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹œì‘: store_id={store_id}")
        
        # Supabaseì—ì„œ ë§¤ì¥ ì •ë³´ ì¡°íšŒ
        supabase = get_supabase_client()
        result = supabase.table("stores").select("*").eq("id", store_id).execute()
        
        print(f"[DEBUG] Supabase ì¡°íšŒ ê²°ê³¼: found={len(result.data) if result.data else 0} rows", flush=True)
        logger.info(f"[DEBUG] Supabase ì¡°íšŒ ê²°ê³¼: found={len(result.data) if result.data else 0} rows")
        if result.data:
            print(f"[DEBUG] ë§¤ì¥ ë°ì´í„°: {result.data[0]}", flush=True)
            logger.info(f"[DEBUG] ë§¤ì¥ ë°ì´í„°: {result.data[0]}")
        else:
            print(f"[DEBUG] Supabaseì—ì„œ í•´ë‹¹ store_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ", flush=True)
            logger.info(f"[DEBUG] Supabaseì—ì„œ í•´ë‹¹ store_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        if not result.data:
            print(f"[DEBUG] ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: store_id={store_id}", flush=True)
            logger.error(f"[DEBUG] ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: store_id={store_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë§¤ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        store = result.data[0]
        place_id = store.get("place_id")
        store_name = store.get("store_name", "") or store.get("name", "")  # store_name ë˜ëŠ” name ì»¬ëŸ¼
        business_type = store.get("business_type", "restaurant")  # ê¸°ë³¸ê°’ì€ restaurant
        
        print(f"[DEBUG] place_id={place_id}, store_name='{store_name}', business_type='{business_type}'", flush=True)
        logger.info(f"ğŸ“‹ ë§¤ì¥ ì •ë³´: id={store_id}, name='{store_name}', place_id={place_id}, business_type='{business_type}'")
        
        if not place_id:
            print(f"[DEBUG] place_idê°€ ì—†ì–´ì„œ 400 ì—ëŸ¬", flush=True)
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ IDê°€ ë“±ë¡ë˜ì§€ ì•Šì€ ë§¤ì¥ì…ë‹ˆë‹¤"
            )
        
        # ë„¤ì´ë²„ APIì—ì„œ ë§¤ì¥ ì •ë³´ ì¡°íšŒ (ë§¤ì¥ëª…, ì¢Œí‘œ ì „ë‹¬)
        review_service = NaverReviewService()
        
        # ë§¤ì¥ëª…ì´ ì—†ìœ¼ë©´ ë¦¬ë·°ì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        if not store_name:
            print(f"[DEBUG] ë§¤ì¥ëª… ì—†ìŒ. ë¦¬ë·°ì—ì„œ ì¶”ì¶œ ì‹œë„", flush=True)
            logger.warning(f"âš ï¸ ë§¤ì¥ëª… ì—†ìŒ. ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì‹œë„: place_id={place_id}")
            try:
                visitor_result = await review_service.get_visitor_reviews(place_id, size=1, business_type=business_type)
                if visitor_result and visitor_result.get("items"):
                    store_name = visitor_result["items"][0].get("businessName", "")
                    print(f"[DEBUG] ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ: '{store_name}'", flush=True)
                    logger.info(f"âœ… ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ: '{store_name}'")
            except Exception as e:
                print(f"[DEBUG] ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}", flush=True)
                logger.error(f"ë¦¬ë·°ì—ì„œ ë§¤ì¥ëª… ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        
        x = store.get("x")
        y = store.get("y")
        print(f"[DEBUG] get_place_info í˜¸ì¶œ ì „: place_id={place_id}, store_name='{store_name}', x={x}, y={y}", flush=True)
        place_info = await review_service.get_place_info(place_id, store_name, x, y)
        print(f"[DEBUG] get_place_info í˜¸ì¶œ í›„: place_info={place_info}", flush=True)
        
        if not place_info:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )
        
        # ë§¤ì¥ëª…ì´ ì—†ìœ¼ë©´ stores í…Œì´ë¸”ì˜ name ì‚¬ìš©
        if not place_info.get("name"):
            place_info["name"] = store.get("name", "")
        
        # ì¸ë„¤ì¼ ë””ë²„ê¹…
        print(f"[DEBUG] ì¸ë„¤ì¼ ì²´í¬:", flush=True)
        print(f"  - place_info.get('image_url'): {place_info.get('image_url')}", flush=True)
        print(f"  - place_info.get('thumbnail'): {place_info.get('thumbnail')}", flush=True)
        print(f"  - store.get('thumbnail'): {store.get('thumbnail')}", flush=True)
        print(f"  - store keys: {list(store.keys())}", flush=True)
        
        # ì¸ë„¤ì¼ì´ ì—†ìœ¼ë©´ stores í…Œì´ë¸”ì˜ thumbnail ì‚¬ìš© (fallback)
        if not place_info.get("image_url") and not place_info.get("thumbnail"):
            fallback_thumbnail = store.get("thumbnail")
            print(f"[DEBUG] Fallback ì¡°ê±´ ì¶©ì¡±! fallback_thumbnail: {fallback_thumbnail}", flush=True)
            if fallback_thumbnail:
                place_info["image_url"] = fallback_thumbnail
                place_info["thumbnail"] = fallback_thumbnail
                print(f"[DEBUG] âœ… Supabase thumbnail ì ìš© ì™„ë£Œ!", flush=True)
                logger.info(f"ğŸ“¸ Supabase thumbnail fallback ì ìš©: {fallback_thumbnail}")
            else:
                print(f"[DEBUG] âŒ Supabaseì—ë„ thumbnail ì—†ìŒ!", flush=True)
        else:
            print(f"[DEBUG] â„¹ï¸ ë„¤ì´ë²„ APIì—ì„œ ì¸ë„¤ì¼ ë°›ìŒ (fallback ë¶ˆí•„ìš”)", flush=True)
        
        print(f"[DEBUG] ìµœì¢… place_info: {place_info}", flush=True)
        logger.info(f"ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {place_info}")
        return {
            "status": "success",
            "store_id": store_id,
            "place_info": place_info
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="ë§¤ì¥ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )
